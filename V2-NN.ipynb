{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erc2A41DMzku"
   },
   "source": [
    "# DSBA 22/23 HSE & University of London\n",
    "\n",
    "# Practical assignment 1. DL in classification.\n",
    "\n",
    "## General info\n",
    "Release data: 26.09.2022\n",
    "\n",
    "Soft deadline: 10.10.2022 23:59 MSK\n",
    "\n",
    "Hard deadline: 13.10.2021 23:59 MSK\n",
    "\n",
    "In this task, you are to build a NN for a binary classification task. We suggest using Google Colab for access to GPU. Competition invite link: https://www.kaggle.com/t/1917e22edb71437ca24d790ab1d57695\n",
    "\n",
    "## Evaluation and fines\n",
    "\n",
    "Each section has a defined \"value\" (in brackets near the section). Maximum grade for the task - 10 points, other points can be assigned to your tests.\n",
    "\n",
    "**Your notebook with the best solution must be reproducible should be sent to the dropbox!** If the assessor cannot reproduce your results, you may be assigned score = 0, so make all your computations fixed!\n",
    "\n",
    "**You can only use neural networks / linear / nearest neighbors models for this task - tree-based models are forbidden!**\n",
    "\n",
    "All the parts must be done independently.\n",
    "\n",
    "After the hard deadline is passed, the hometask is not accepted. If you send the hometask after the soft deadline, you will be excluded from competition among your mates and the homework will only be scored by the \"Beating the baseline\" part.\n",
    "\n",
    "Feel free to ask questions both the teacher and your mates, but __do not copy the code or do it together__. \"Similar\" solutions are considered a plagiarism and all the involved students (the ones who gave & the ones who did) cannot get more than 0.01 points for the task. If you found a solution in some open source, you __must__ reference it in a special block at the end of your work (to exclude the suspicions in plagiarism).\n",
    "\n",
    "\n",
    "## Format of handing over\n",
    "\n",
    "The tasks are sent to the dropbox: https://www.dropbox.com/request/Y6TJouxNbm3r0RgcBL35. Don't forget to attach your name, surname & your group.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwMZDBm4S9o3"
   },
   "source": [
    "## 1. Model training\n",
    "\n",
    "**Important!** Public Leaderboard contains only 33% of the test data. Your points will be measured wrt to the whole test set, therefore your position on the LB after the end of the competition may change.\n",
    "\n",
    "* test_accuracy > weak baseline (public LB): 3 points\n",
    "\n",
    "* test_accuracy > medium baseline (public LB): + 3 points\n",
    "\n",
    "* test_accuracy > strong baseline (public LB): + 2 points\n",
    "\n",
    "* You are among 25% most successful students (private LB): + 2 point\n",
    "\n",
    "* You are among top-3 most successful students (private LB): + 1 point\n",
    "\n",
    "* You are among top-2 most successful students (private LB): + 1 point\n",
    "\n",
    "* You are among top-1 most successful students (private LB): + 1 point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VnJmCRD8S9o3"
   },
   "outputs": [],
   "source": [
    "# Your code here ╰( ͡° ͜ʖ ͡° )つ──☆*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WLBmP2zTFmnB"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c6tn9gN7ohs9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "3Tj4gkZnWENb"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LudqW7xct2rH"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9PHJ1f-gPTm"
   },
   "source": [
    "# **Preprocessing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "P3YAH9EgS9o3"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "target_df = pd.read_csv('train_target.csv')\n",
    "train_expected_target1 = pd.read_csv('train_expected_target_agent_1.csv')\n",
    "train_expected_target2 = pd.read_csv('train_expected_target_agent_2.csv')\n",
    "train_target_agent_1 = pd.read_csv('train_target_agent_1.csv')\n",
    "train_target_agent_2 = pd.read_csv('train_target_agent_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_agent_1 = train_target_agent_1.rename(columns={\"0\": \"expected_target1\"})\n",
    "train_target_agent_2 = train_target_agent_2.rename(columns={\"0\": \"expected_target2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, train_target_agent_1, train_target_agent_2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "XtxGrhyjVEee",
    "outputId": "d81db566-83f9-47d1-c8e8-4c9333b09d31"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_1_feat_Possession%</th>\n",
       "      <th>agent_1_feat_Pass%</th>\n",
       "      <th>agent_1_feat_AerialsWon</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "      <th>agent_1_feat_XGrealiz</th>\n",
       "      <th>agent_1_feat_XGArealiz</th>\n",
       "      <th>agent_1_feat_PPDA</th>\n",
       "      <th>agent_1_feat_OPPDA</th>\n",
       "      <th>agent_1_feat_DC</th>\n",
       "      <th>agent_1_feat_ODC</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_2_feattotal_xg_1</th>\n",
       "      <th>agent_2_feattotal_xg_mean_3</th>\n",
       "      <th>agent_2_feattotal_xg_mean</th>\n",
       "      <th>agent_2_featboth_scored_3</th>\n",
       "      <th>agent_2_featboth_scored_2</th>\n",
       "      <th>agent_2_featboth_scored_1</th>\n",
       "      <th>agent_2_featboth_scored_mean_3</th>\n",
       "      <th>agent_2_featboth_scored_mean</th>\n",
       "      <th>expected_target1</th>\n",
       "      <th>expected_target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58.8</td>\n",
       "      <td>85.1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>6.99</td>\n",
       "      <td>1.143700</td>\n",
       "      <td>0.928715</td>\n",
       "      <td>7.13</td>\n",
       "      <td>14.16</td>\n",
       "      <td>267.0</td>\n",
       "      <td>194.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.739439</td>\n",
       "      <td>2.739439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.8</td>\n",
       "      <td>71.1</td>\n",
       "      <td>23.4</td>\n",
       "      <td>6.84</td>\n",
       "      <td>0.954159</td>\n",
       "      <td>0.975350</td>\n",
       "      <td>9.99</td>\n",
       "      <td>7.66</td>\n",
       "      <td>191.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.336756</td>\n",
       "      <td>2.336756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46.3</td>\n",
       "      <td>70.8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.918434</td>\n",
       "      <td>1.118603</td>\n",
       "      <td>9.56</td>\n",
       "      <td>7.34</td>\n",
       "      <td>179.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.120322</td>\n",
       "      <td>2.120322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50.2</td>\n",
       "      <td>77.5</td>\n",
       "      <td>24.4</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1.037613</td>\n",
       "      <td>0.956836</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.53</td>\n",
       "      <td>195.0</td>\n",
       "      <td>239.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.216415</td>\n",
       "      <td>2.216415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44.9</td>\n",
       "      <td>75.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.983691</td>\n",
       "      <td>0.948837</td>\n",
       "      <td>12.24</td>\n",
       "      <td>8.76</td>\n",
       "      <td>161.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.604025</td>\n",
       "      <td>2.604025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_1_feat_Possession%  agent_1_feat_Pass%  agent_1_feat_AerialsWon  \\\n",
       "0                      58.8                85.1                     15.8   \n",
       "1                      44.8                71.1                     23.4   \n",
       "2                      46.3                70.8                     21.7   \n",
       "3                      50.2                77.5                     24.4   \n",
       "4                      44.9                75.0                     17.2   \n",
       "\n",
       "   agent_1_feat_Rating  agent_1_feat_XGrealiz  agent_1_feat_XGArealiz  \\\n",
       "0                 6.99               1.143700                0.928715   \n",
       "1                 6.84               0.954159                0.975350   \n",
       "2                 6.77               0.918434                1.118603   \n",
       "3                 6.87               1.037613                0.956836   \n",
       "4                 6.77               0.983691                0.948837   \n",
       "\n",
       "   agent_1_feat_PPDA  agent_1_feat_OPPDA  agent_1_feat_DC  agent_1_feat_ODC  \\\n",
       "0               7.13               14.16            267.0             194.0   \n",
       "1               9.99                7.66            191.0             287.0   \n",
       "2               9.56                7.34            179.0             298.0   \n",
       "3               9.60                9.53            195.0             239.0   \n",
       "4              12.24                8.76            161.0             283.0   \n",
       "\n",
       "   ...  agent_2_feattotal_xg_1  agent_2_feattotal_xg_mean_3  \\\n",
       "0  ...                2.739439                     2.739439   \n",
       "1  ...                2.336756                     2.336756   \n",
       "2  ...                2.120322                     2.120322   \n",
       "3  ...                2.216415                     2.216415   \n",
       "4  ...                2.604025                     2.604025   \n",
       "\n",
       "   agent_2_feattotal_xg_mean  agent_2_featboth_scored_3  \\\n",
       "0                        NaN                   0.473684   \n",
       "1                        NaN                   0.578947   \n",
       "2                        NaN                   0.368421   \n",
       "3                        NaN                   0.210526   \n",
       "4                        NaN                   0.421053   \n",
       "\n",
       "   agent_2_featboth_scored_2  agent_2_featboth_scored_1  \\\n",
       "0                   0.473684                   0.473684   \n",
       "1                   0.578947                   0.578947   \n",
       "2                   0.368421                   0.368421   \n",
       "3                   0.210526                   0.210526   \n",
       "4                   0.421053                   0.421053   \n",
       "\n",
       "   agent_2_featboth_scored_mean_3  agent_2_featboth_scored_mean  \\\n",
       "0                        0.473684                           NaN   \n",
       "1                        0.578947                           NaN   \n",
       "2                        0.368421                           NaN   \n",
       "3                        0.210526                           NaN   \n",
       "4                        0.421053                           NaN   \n",
       "\n",
       "   expected_target1  expected_target2  \n",
       "0                 1                 2  \n",
       "1                 2                 2  \n",
       "2                 0                 1  \n",
       "3                 0                 1  \n",
       "4                 2                 2  \n",
       "\n",
       "[5 rows x 236 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "fi9w8WcKVID2",
    "outputId": "3acc7dd6-6503-4c25-ccee-9bbc933f764b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_1_feat_Possession%</th>\n",
       "      <th>agent_1_feat_Pass%</th>\n",
       "      <th>agent_1_feat_AerialsWon</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "      <th>agent_1_feat_XGrealiz</th>\n",
       "      <th>agent_1_feat_XGArealiz</th>\n",
       "      <th>agent_1_feat_PPDA</th>\n",
       "      <th>agent_1_feat_OPPDA</th>\n",
       "      <th>agent_1_feat_DC</th>\n",
       "      <th>agent_1_feat_ODC</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_2_feattotal_xg_3</th>\n",
       "      <th>agent_2_feattotal_xg_2</th>\n",
       "      <th>agent_2_feattotal_xg_1</th>\n",
       "      <th>agent_2_feattotal_xg_mean_3</th>\n",
       "      <th>agent_2_feattotal_xg_mean</th>\n",
       "      <th>agent_2_featboth_scored_3</th>\n",
       "      <th>agent_2_featboth_scored_2</th>\n",
       "      <th>agent_2_featboth_scored_1</th>\n",
       "      <th>agent_2_featboth_scored_mean_3</th>\n",
       "      <th>agent_2_featboth_scored_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58.6</td>\n",
       "      <td>87.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0.844742</td>\n",
       "      <td>1.165049</td>\n",
       "      <td>9.19</td>\n",
       "      <td>16.50</td>\n",
       "      <td>337.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.661870</td>\n",
       "      <td>1.893116</td>\n",
       "      <td>4.241360</td>\n",
       "      <td>2.932115</td>\n",
       "      <td>2.690442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.7</td>\n",
       "      <td>81.3</td>\n",
       "      <td>14.2</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.743218</td>\n",
       "      <td>1.152593</td>\n",
       "      <td>10.31</td>\n",
       "      <td>13.63</td>\n",
       "      <td>311.0</td>\n",
       "      <td>208.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.550724</td>\n",
       "      <td>2.373700</td>\n",
       "      <td>4.197010</td>\n",
       "      <td>3.373811</td>\n",
       "      <td>3.075302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.3</td>\n",
       "      <td>81.4</td>\n",
       "      <td>17.7</td>\n",
       "      <td>6.73</td>\n",
       "      <td>0.954509</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>14.21</td>\n",
       "      <td>11.82</td>\n",
       "      <td>207.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.693652</td>\n",
       "      <td>2.042668</td>\n",
       "      <td>0.966665</td>\n",
       "      <td>1.900995</td>\n",
       "      <td>3.007033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.5</td>\n",
       "      <td>84.8</td>\n",
       "      <td>14.5</td>\n",
       "      <td>6.85</td>\n",
       "      <td>1.155612</td>\n",
       "      <td>1.049618</td>\n",
       "      <td>10.95</td>\n",
       "      <td>12.46</td>\n",
       "      <td>339.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.938100</td>\n",
       "      <td>1.466409</td>\n",
       "      <td>0.922046</td>\n",
       "      <td>2.108852</td>\n",
       "      <td>2.643923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.3</td>\n",
       "      <td>81.8</td>\n",
       "      <td>16.4</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1.199718</td>\n",
       "      <td>0.856327</td>\n",
       "      <td>11.27</td>\n",
       "      <td>11.52</td>\n",
       "      <td>193.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.358338</td>\n",
       "      <td>2.138405</td>\n",
       "      <td>1.872476</td>\n",
       "      <td>2.456406</td>\n",
       "      <td>3.113815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 234 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   agent_1_feat_Possession%  agent_1_feat_Pass%  agent_1_feat_AerialsWon  \\\n",
       "0                      58.6                87.0                     15.2   \n",
       "1                      50.7                81.3                     14.2   \n",
       "2                      47.3                81.4                     17.7   \n",
       "3                      54.5                84.8                     14.5   \n",
       "4                      51.3                81.8                     16.4   \n",
       "\n",
       "   agent_1_feat_Rating  agent_1_feat_XGrealiz  agent_1_feat_XGArealiz  \\\n",
       "0                 6.83               0.844742                1.165049   \n",
       "1                 6.65               0.743218                1.152593   \n",
       "2                 6.73               0.954509                0.956938   \n",
       "3                 6.85               1.155612                1.049618   \n",
       "4                 6.81               1.199718                0.856327   \n",
       "\n",
       "   agent_1_feat_PPDA  agent_1_feat_OPPDA  agent_1_feat_DC  agent_1_feat_ODC  \\\n",
       "0               9.19               16.50            337.0             179.0   \n",
       "1              10.31               13.63            311.0             208.0   \n",
       "2              14.21               11.82            207.0             270.0   \n",
       "3              10.95               12.46            339.0             186.0   \n",
       "4              11.27               11.52            193.0             293.0   \n",
       "\n",
       "   ...  agent_2_feattotal_xg_3  agent_2_feattotal_xg_2  \\\n",
       "0  ...                2.661870                1.893116   \n",
       "1  ...                3.550724                2.373700   \n",
       "2  ...                2.693652                2.042668   \n",
       "3  ...                3.938100                1.466409   \n",
       "4  ...                3.358338                2.138405   \n",
       "\n",
       "   agent_2_feattotal_xg_1  agent_2_feattotal_xg_mean_3  \\\n",
       "0                4.241360                     2.932115   \n",
       "1                4.197010                     3.373811   \n",
       "2                0.966665                     1.900995   \n",
       "3                0.922046                     2.108852   \n",
       "4                1.872476                     2.456406   \n",
       "\n",
       "   agent_2_feattotal_xg_mean  agent_2_featboth_scored_3  \\\n",
       "0                   2.690442                        1.0   \n",
       "1                   3.075302                        0.0   \n",
       "2                   3.007033                        0.0   \n",
       "3                   2.643923                        1.0   \n",
       "4                   3.113815                        0.0   \n",
       "\n",
       "   agent_2_featboth_scored_2  agent_2_featboth_scored_1  \\\n",
       "0                        0.0                        1.0   \n",
       "1                        1.0                        1.0   \n",
       "2                        1.0                        1.0   \n",
       "3                        0.0                        0.0   \n",
       "4                        0.0                        0.0   \n",
       "\n",
       "   agent_2_featboth_scored_mean_3  agent_2_featboth_scored_mean  \n",
       "0                        0.666667                      0.333333  \n",
       "1                        0.666667                      0.625000  \n",
       "2                        0.666667                      0.555556  \n",
       "3                        0.333333                      0.444444  \n",
       "4                        0.000000                      0.555556  \n",
       "\n",
       "[5 rows x 234 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WlvCdFqVUSf",
    "outputId": "95181e8f-d145-49d6-8c1f-7520814b665c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2470, 236)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjIbnhiPVYT-",
    "outputId": "8c96af8c-bf60-401c-b737-9294f66f3cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2470 entries, 0 to 2469\n",
      "Columns: 236 entries, agent_1_feat_Possession% to expected_target2\n",
      "dtypes: float64(212), int64(24)\n",
      "memory usage: 4.4 MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ceY_OeBgqdO8"
   },
   "outputs": [],
   "source": [
    "target_df.drop('id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p0N_mUFAqMIj"
   },
   "outputs": [],
   "source": [
    "train_df = pd.concat([target_df, train_df], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "1TZbR6awq_C9",
    "outputId": "12ad58c1-8a7b-4bb0-dff6-5aac2487450b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>agent_1_feat_Possession%</th>\n",
       "      <th>agent_1_feat_Pass%</th>\n",
       "      <th>agent_1_feat_AerialsWon</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "      <th>agent_1_feat_XGrealiz</th>\n",
       "      <th>agent_1_feat_XGArealiz</th>\n",
       "      <th>agent_1_feat_PPDA</th>\n",
       "      <th>agent_1_feat_OPPDA</th>\n",
       "      <th>agent_1_feat_DC</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_2_feattotal_xg_1</th>\n",
       "      <th>agent_2_feattotal_xg_mean_3</th>\n",
       "      <th>agent_2_feattotal_xg_mean</th>\n",
       "      <th>agent_2_featboth_scored_3</th>\n",
       "      <th>agent_2_featboth_scored_2</th>\n",
       "      <th>agent_2_featboth_scored_1</th>\n",
       "      <th>agent_2_featboth_scored_mean_3</th>\n",
       "      <th>agent_2_featboth_scored_mean</th>\n",
       "      <th>expected_target1</th>\n",
       "      <th>expected_target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>58.8</td>\n",
       "      <td>85.1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>6.99</td>\n",
       "      <td>1.143700</td>\n",
       "      <td>0.928715</td>\n",
       "      <td>7.13</td>\n",
       "      <td>14.16</td>\n",
       "      <td>267.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.739439</td>\n",
       "      <td>2.739439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>44.8</td>\n",
       "      <td>71.1</td>\n",
       "      <td>23.4</td>\n",
       "      <td>6.84</td>\n",
       "      <td>0.954159</td>\n",
       "      <td>0.975350</td>\n",
       "      <td>9.99</td>\n",
       "      <td>7.66</td>\n",
       "      <td>191.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.336756</td>\n",
       "      <td>2.336756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>70.8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.918434</td>\n",
       "      <td>1.118603</td>\n",
       "      <td>9.56</td>\n",
       "      <td>7.34</td>\n",
       "      <td>179.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.120322</td>\n",
       "      <td>2.120322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>50.2</td>\n",
       "      <td>77.5</td>\n",
       "      <td>24.4</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1.037613</td>\n",
       "      <td>0.956836</td>\n",
       "      <td>9.60</td>\n",
       "      <td>9.53</td>\n",
       "      <td>195.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.216415</td>\n",
       "      <td>2.216415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>44.9</td>\n",
       "      <td>75.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.983691</td>\n",
       "      <td>0.948837</td>\n",
       "      <td>12.24</td>\n",
       "      <td>8.76</td>\n",
       "      <td>161.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.604025</td>\n",
       "      <td>2.604025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>76.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>6.62</td>\n",
       "      <td>1.046406</td>\n",
       "      <td>1.032989</td>\n",
       "      <td>18.00</td>\n",
       "      <td>8.27</td>\n",
       "      <td>138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.684860</td>\n",
       "      <td>4.024907</td>\n",
       "      <td>3.872622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>1</td>\n",
       "      <td>42.9</td>\n",
       "      <td>76.1</td>\n",
       "      <td>18.3</td>\n",
       "      <td>6.61</td>\n",
       "      <td>1.161802</td>\n",
       "      <td>1.066236</td>\n",
       "      <td>16.14</td>\n",
       "      <td>7.60</td>\n",
       "      <td>201.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.568175</td>\n",
       "      <td>2.000313</td>\n",
       "      <td>2.572016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>19.1</td>\n",
       "      <td>6.51</td>\n",
       "      <td>1.000858</td>\n",
       "      <td>1.026472</td>\n",
       "      <td>15.99</td>\n",
       "      <td>7.99</td>\n",
       "      <td>164.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.871643</td>\n",
       "      <td>2.496854</td>\n",
       "      <td>2.555157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1</td>\n",
       "      <td>51.4</td>\n",
       "      <td>79.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>6.62</td>\n",
       "      <td>1.037986</td>\n",
       "      <td>1.161401</td>\n",
       "      <td>9.73</td>\n",
       "      <td>10.47</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.904164</td>\n",
       "      <td>2.977092</td>\n",
       "      <td>2.495116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>1</td>\n",
       "      <td>43.5</td>\n",
       "      <td>71.6</td>\n",
       "      <td>23.4</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.865460</td>\n",
       "      <td>0.931256</td>\n",
       "      <td>13.16</td>\n",
       "      <td>9.72</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.945618</td>\n",
       "      <td>3.414186</td>\n",
       "      <td>2.861741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2470 rows × 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category  agent_1_feat_Possession%  agent_1_feat_Pass%  \\\n",
       "0            1                      58.8                85.1   \n",
       "1            1                      44.8                71.1   \n",
       "2            0                      46.3                70.8   \n",
       "3            0                      50.2                77.5   \n",
       "4            1                      44.9                75.0   \n",
       "...        ...                       ...                 ...   \n",
       "2465         1                      41.6                76.0   \n",
       "2466         1                      42.9                76.1   \n",
       "2467         0                      41.0                72.2   \n",
       "2468         1                      51.4                79.3   \n",
       "2469         1                      43.5                71.6   \n",
       "\n",
       "      agent_1_feat_AerialsWon  agent_1_feat_Rating  agent_1_feat_XGrealiz  \\\n",
       "0                        15.8                 6.99               1.143700   \n",
       "1                        23.4                 6.84               0.954159   \n",
       "2                        21.7                 6.77               0.918434   \n",
       "3                        24.4                 6.87               1.037613   \n",
       "4                        17.2                 6.77               0.983691   \n",
       "...                       ...                  ...                    ...   \n",
       "2465                     17.1                 6.62               1.046406   \n",
       "2466                     18.3                 6.61               1.161802   \n",
       "2467                     19.1                 6.51               1.000858   \n",
       "2468                     14.1                 6.62               1.037986   \n",
       "2469                     23.4                 6.64               0.865460   \n",
       "\n",
       "      agent_1_feat_XGArealiz  agent_1_feat_PPDA  agent_1_feat_OPPDA  \\\n",
       "0                   0.928715               7.13               14.16   \n",
       "1                   0.975350               9.99                7.66   \n",
       "2                   1.118603               9.56                7.34   \n",
       "3                   0.956836               9.60                9.53   \n",
       "4                   0.948837              12.24                8.76   \n",
       "...                      ...                ...                 ...   \n",
       "2465                1.032989              18.00                8.27   \n",
       "2466                1.066236              16.14                7.60   \n",
       "2467                1.026472              15.99                7.99   \n",
       "2468                1.161401               9.73               10.47   \n",
       "2469                0.931256              13.16                9.72   \n",
       "\n",
       "      agent_1_feat_DC  ...  agent_2_feattotal_xg_1  \\\n",
       "0               267.0  ...                2.739439   \n",
       "1               191.0  ...                2.336756   \n",
       "2               179.0  ...                2.120322   \n",
       "3               195.0  ...                2.216415   \n",
       "4               161.0  ...                2.604025   \n",
       "...               ...  ...                     ...   \n",
       "2465            138.0  ...                3.684860   \n",
       "2466            201.0  ...                1.568175   \n",
       "2467            164.0  ...                3.871643   \n",
       "2468            222.0  ...                4.904164   \n",
       "2469            233.0  ...                3.945618   \n",
       "\n",
       "      agent_2_feattotal_xg_mean_3  agent_2_feattotal_xg_mean  \\\n",
       "0                        2.739439                        NaN   \n",
       "1                        2.336756                        NaN   \n",
       "2                        2.120322                        NaN   \n",
       "3                        2.216415                        NaN   \n",
       "4                        2.604025                        NaN   \n",
       "...                           ...                        ...   \n",
       "2465                     4.024907                   3.872622   \n",
       "2466                     2.000313                   2.572016   \n",
       "2467                     2.496854                   2.555157   \n",
       "2468                     2.977092                   2.495116   \n",
       "2469                     3.414186                   2.861741   \n",
       "\n",
       "      agent_2_featboth_scored_3  agent_2_featboth_scored_2  \\\n",
       "0                      0.473684                   0.473684   \n",
       "1                      0.578947                   0.578947   \n",
       "2                      0.368421                   0.368421   \n",
       "3                      0.210526                   0.210526   \n",
       "4                      0.421053                   0.421053   \n",
       "...                         ...                        ...   \n",
       "2465                   1.000000                   0.000000   \n",
       "2466                   0.000000                   0.000000   \n",
       "2467                   0.000000                   0.000000   \n",
       "2468                   1.000000                   0.000000   \n",
       "2469                   1.000000                   1.000000   \n",
       "\n",
       "      agent_2_featboth_scored_1  agent_2_featboth_scored_mean_3  \\\n",
       "0                      0.473684                        0.473684   \n",
       "1                      0.578947                        0.578947   \n",
       "2                      0.368421                        0.368421   \n",
       "3                      0.210526                        0.210526   \n",
       "4                      0.421053                        0.421053   \n",
       "...                         ...                             ...   \n",
       "2465                   0.000000                        0.333333   \n",
       "2466                   0.000000                        0.000000   \n",
       "2467                   1.000000                        0.333333   \n",
       "2468                   0.000000                        0.333333   \n",
       "2469                   0.000000                        0.666667   \n",
       "\n",
       "      agent_2_featboth_scored_mean  expected_target1  expected_target2  \n",
       "0                              NaN                 1                 2  \n",
       "1                              NaN                 2                 2  \n",
       "2                              NaN                 0                 1  \n",
       "3                              NaN                 0                 1  \n",
       "4                              NaN                 2                 2  \n",
       "...                            ...               ...               ...  \n",
       "2465                      0.444444                 1                 2  \n",
       "2466                      0.444444                 2                 3  \n",
       "2467                      0.500000                 0                 5  \n",
       "2468                      0.222222                 1                 3  \n",
       "2469                      0.333333                 3                 2  \n",
       "\n",
       "[2470 rows x 237 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VaUeSE29_6w"
   },
   "source": [
    "## Delete outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "CkheOAfn_zOK",
    "outputId": "000d0384-64b4-4d66-f341-0a872abe165e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_expected_target2</th>\n",
       "      <th>train_expected_target1</th>\n",
       "      <th>category</th>\n",
       "      <th>agent_1_feat_Possession%</th>\n",
       "      <th>agent_1_feat_Pass%</th>\n",
       "      <th>agent_1_feat_AerialsWon</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "      <th>agent_1_feat_XGrealiz</th>\n",
       "      <th>agent_1_feat_XGArealiz</th>\n",
       "      <th>agent_1_feat_PPDA</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_2_feattotal_xg_1</th>\n",
       "      <th>agent_2_feattotal_xg_mean_3</th>\n",
       "      <th>agent_2_feattotal_xg_mean</th>\n",
       "      <th>agent_2_featboth_scored_3</th>\n",
       "      <th>agent_2_featboth_scored_2</th>\n",
       "      <th>agent_2_featboth_scored_1</th>\n",
       "      <th>agent_2_featboth_scored_mean_3</th>\n",
       "      <th>agent_2_featboth_scored_mean</th>\n",
       "      <th>expected_target1</th>\n",
       "      <th>expected_target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.278076</td>\n",
       "      <td>1.166350</td>\n",
       "      <td>1</td>\n",
       "      <td>58.8</td>\n",
       "      <td>85.1</td>\n",
       "      <td>15.8</td>\n",
       "      <td>6.99</td>\n",
       "      <td>1.143700</td>\n",
       "      <td>0.928715</td>\n",
       "      <td>7.13</td>\n",
       "      <td>...</td>\n",
       "      <td>2.739439</td>\n",
       "      <td>2.739439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.613273</td>\n",
       "      <td>1.278300</td>\n",
       "      <td>1</td>\n",
       "      <td>44.8</td>\n",
       "      <td>71.1</td>\n",
       "      <td>23.4</td>\n",
       "      <td>6.84</td>\n",
       "      <td>0.954159</td>\n",
       "      <td>0.975350</td>\n",
       "      <td>9.99</td>\n",
       "      <td>...</td>\n",
       "      <td>2.336756</td>\n",
       "      <td>2.336756</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.117570</td>\n",
       "      <td>1.900670</td>\n",
       "      <td>0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>70.8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.918434</td>\n",
       "      <td>1.118603</td>\n",
       "      <td>9.56</td>\n",
       "      <td>...</td>\n",
       "      <td>2.120322</td>\n",
       "      <td>2.120322</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.909774</td>\n",
       "      <td>0.423368</td>\n",
       "      <td>0</td>\n",
       "      <td>50.2</td>\n",
       "      <td>77.5</td>\n",
       "      <td>24.4</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1.037613</td>\n",
       "      <td>0.956836</td>\n",
       "      <td>9.60</td>\n",
       "      <td>...</td>\n",
       "      <td>2.216415</td>\n",
       "      <td>2.216415</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.991901</td>\n",
       "      <td>1.683430</td>\n",
       "      <td>1</td>\n",
       "      <td>44.9</td>\n",
       "      <td>75.0</td>\n",
       "      <td>17.2</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.983691</td>\n",
       "      <td>0.948837</td>\n",
       "      <td>12.24</td>\n",
       "      <td>...</td>\n",
       "      <td>2.604025</td>\n",
       "      <td>2.604025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 239 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_expected_target2  train_expected_target1  category  \\\n",
       "0                0.278076                1.166350         1   \n",
       "1                0.613273                1.278300         1   \n",
       "2                1.117570                1.900670         0   \n",
       "3                0.909774                0.423368         0   \n",
       "4                0.991901                1.683430         1   \n",
       "\n",
       "   agent_1_feat_Possession%  agent_1_feat_Pass%  agent_1_feat_AerialsWon  \\\n",
       "0                      58.8                85.1                     15.8   \n",
       "1                      44.8                71.1                     23.4   \n",
       "2                      46.3                70.8                     21.7   \n",
       "3                      50.2                77.5                     24.4   \n",
       "4                      44.9                75.0                     17.2   \n",
       "\n",
       "   agent_1_feat_Rating  agent_1_feat_XGrealiz  agent_1_feat_XGArealiz  \\\n",
       "0                 6.99               1.143700                0.928715   \n",
       "1                 6.84               0.954159                0.975350   \n",
       "2                 6.77               0.918434                1.118603   \n",
       "3                 6.87               1.037613                0.956836   \n",
       "4                 6.77               0.983691                0.948837   \n",
       "\n",
       "   agent_1_feat_PPDA  ...  agent_2_feattotal_xg_1  \\\n",
       "0               7.13  ...                2.739439   \n",
       "1               9.99  ...                2.336756   \n",
       "2               9.56  ...                2.120322   \n",
       "3               9.60  ...                2.216415   \n",
       "4              12.24  ...                2.604025   \n",
       "\n",
       "   agent_2_feattotal_xg_mean_3  agent_2_feattotal_xg_mean  \\\n",
       "0                     2.739439                        NaN   \n",
       "1                     2.336756                        NaN   \n",
       "2                     2.120322                        NaN   \n",
       "3                     2.216415                        NaN   \n",
       "4                     2.604025                        NaN   \n",
       "\n",
       "   agent_2_featboth_scored_3  agent_2_featboth_scored_2  \\\n",
       "0                   0.473684                   0.473684   \n",
       "1                   0.578947                   0.578947   \n",
       "2                   0.368421                   0.368421   \n",
       "3                   0.210526                   0.210526   \n",
       "4                   0.421053                   0.421053   \n",
       "\n",
       "   agent_2_featboth_scored_1  agent_2_featboth_scored_mean_3  \\\n",
       "0                   0.473684                        0.473684   \n",
       "1                   0.578947                        0.578947   \n",
       "2                   0.368421                        0.368421   \n",
       "3                   0.210526                        0.210526   \n",
       "4                   0.421053                        0.421053   \n",
       "\n",
       "   agent_2_featboth_scored_mean  expected_target1  expected_target2  \n",
       "0                           NaN                 1                 2  \n",
       "1                           NaN                 2                 2  \n",
       "2                           NaN                 0                 1  \n",
       "3                           NaN                 0                 1  \n",
       "4                           NaN                 2                 2  \n",
       "\n",
       "[5 rows x 239 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_expected_target1 = train_expected_target1.rename(columns={\"0\": \"train_expected_target1\"})\n",
    "train_expected_target2 = train_expected_target2.rename(columns={\"0\": \"train_expected_target2\"})\n",
    "train_df = pd.concat([train_expected_target1, train_df], axis = 1)\n",
    "train_df = pd.concat([train_expected_target2, train_df], axis = 1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJHCElVu-906",
    "outputId": "df35fdc8-6e8d-4af7-9ecf-e96d0c76a9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before deleting:  2470\n",
      "Rows after deleting:  2295\n"
     ]
    }
   ],
   "source": [
    "print('Rows before deleting: ', train_df.shape[0])\n",
    "train_df = train_df.drop(train_df[(train_df.train_expected_target2 > 1) &\n",
    "                                  (train_df.train_expected_target1 > 1) &\n",
    "                                  (train_df.category == 0)].index)\n",
    "train_df.drop(['train_expected_target1', 'train_expected_target2'], axis = 1, inplace = True)\n",
    "print('Rows after deleting: ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ruuBsKl75SR"
   },
   "source": [
    "## Work with missing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kc2PbEGDRIjw",
    "outputId": "3042e755-cbf8-4661-ac14-71b4a75e601c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before deleting:  2295\n",
      "Rows after deleting:  2163\n"
     ]
    }
   ],
   "source": [
    "print('Rows before deleting: ', train_df.shape[0])\n",
    "train_df = train_df.dropna()  \n",
    "print('Rows after deleting: ', train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_2_featboth_scored_mean        20\n",
       "agent_2_feattotal_xg_mean           20\n",
       "agent_1_feat_total_scored_mean.1    20\n",
       "agent_1_feat_both_scored_mean.1     20\n",
       "agent_2_feattotal_scored_mean       20\n",
       "                                    ..\n",
       "agent_1_feat_scheme_10               0\n",
       "agent_1_feat_scheme_9                0\n",
       "agent_1_feat_scheme_8                0\n",
       "agent_1_feat_scheme_7                0\n",
       "agent_1_feat_Possession%             0\n",
       "Length: 234, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_2_featboth_scored_mean        0\n",
       "agent_2_feat_PPDA                   0\n",
       "agent_2_feat_Dispossessed           0\n",
       "agent_2_feat_UnsuccessfulTouches    0\n",
       "agent_2_feat_DribbledPast           0\n",
       "                                   ..\n",
       "agent_1_feat_scheme_17              0\n",
       "agent_1_feat_scheme_16              0\n",
       "agent_1_feat_scheme_15              0\n",
       "agent_1_feat_scheme_14              0\n",
       "agent_1_feat_Possession%            0\n",
       "Length: 234, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.apply(lambda x: x.fillna(x.mean()), axis=0) \n",
    "test_df.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>agent_1_feat_Possession%</th>\n",
       "      <th>agent_1_feat_Pass%</th>\n",
       "      <th>agent_1_feat_AerialsWon</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "      <th>agent_1_feat_XGrealiz</th>\n",
       "      <th>agent_1_feat_XGArealiz</th>\n",
       "      <th>agent_1_feat_PPDA</th>\n",
       "      <th>agent_1_feat_OPPDA</th>\n",
       "      <th>agent_1_feat_DC</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_2_feattotal_xg_1</th>\n",
       "      <th>agent_2_feattotal_xg_mean_3</th>\n",
       "      <th>agent_2_feattotal_xg_mean</th>\n",
       "      <th>agent_2_featboth_scored_3</th>\n",
       "      <th>agent_2_featboth_scored_2</th>\n",
       "      <th>agent_2_featboth_scored_1</th>\n",
       "      <th>agent_2_featboth_scored_mean_3</th>\n",
       "      <th>agent_2_featboth_scored_mean</th>\n",
       "      <th>expected_target1</th>\n",
       "      <th>expected_target2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>70.3</td>\n",
       "      <td>25.1</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.711201</td>\n",
       "      <td>0.915529</td>\n",
       "      <td>10.74</td>\n",
       "      <td>9.43</td>\n",
       "      <td>218.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.608046</td>\n",
       "      <td>2.112304</td>\n",
       "      <td>1.608046</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>84.6</td>\n",
       "      <td>15.9</td>\n",
       "      <td>7.07</td>\n",
       "      <td>1.094698</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>7.57</td>\n",
       "      <td>13.92</td>\n",
       "      <td>575.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479335</td>\n",
       "      <td>2.214160</td>\n",
       "      <td>2.479335</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>48.1</td>\n",
       "      <td>76.9</td>\n",
       "      <td>17.7</td>\n",
       "      <td>6.74</td>\n",
       "      <td>0.994530</td>\n",
       "      <td>1.235052</td>\n",
       "      <td>9.77</td>\n",
       "      <td>8.24</td>\n",
       "      <td>175.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.712261</td>\n",
       "      <td>2.183093</td>\n",
       "      <td>1.712261</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>46.3</td>\n",
       "      <td>70.8</td>\n",
       "      <td>21.7</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.918434</td>\n",
       "      <td>1.118603</td>\n",
       "      <td>9.56</td>\n",
       "      <td>7.34</td>\n",
       "      <td>179.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.675331</td>\n",
       "      <td>2.627794</td>\n",
       "      <td>2.675331</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>50.7</td>\n",
       "      <td>82.1</td>\n",
       "      <td>14.4</td>\n",
       "      <td>6.86</td>\n",
       "      <td>1.124694</td>\n",
       "      <td>0.875939</td>\n",
       "      <td>11.79</td>\n",
       "      <td>10.66</td>\n",
       "      <td>156.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.331644</td>\n",
       "      <td>2.260683</td>\n",
       "      <td>1.331644</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>1</td>\n",
       "      <td>41.6</td>\n",
       "      <td>76.0</td>\n",
       "      <td>17.1</td>\n",
       "      <td>6.62</td>\n",
       "      <td>1.046406</td>\n",
       "      <td>1.032989</td>\n",
       "      <td>18.00</td>\n",
       "      <td>8.27</td>\n",
       "      <td>138.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.684860</td>\n",
       "      <td>4.024907</td>\n",
       "      <td>3.872622</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>1</td>\n",
       "      <td>42.9</td>\n",
       "      <td>76.1</td>\n",
       "      <td>18.3</td>\n",
       "      <td>6.61</td>\n",
       "      <td>1.161802</td>\n",
       "      <td>1.066236</td>\n",
       "      <td>16.14</td>\n",
       "      <td>7.60</td>\n",
       "      <td>201.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.568175</td>\n",
       "      <td>2.000313</td>\n",
       "      <td>2.572016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>72.2</td>\n",
       "      <td>19.1</td>\n",
       "      <td>6.51</td>\n",
       "      <td>1.000858</td>\n",
       "      <td>1.026472</td>\n",
       "      <td>15.99</td>\n",
       "      <td>7.99</td>\n",
       "      <td>164.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.871643</td>\n",
       "      <td>2.496854</td>\n",
       "      <td>2.555157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1</td>\n",
       "      <td>51.4</td>\n",
       "      <td>79.3</td>\n",
       "      <td>14.1</td>\n",
       "      <td>6.62</td>\n",
       "      <td>1.037986</td>\n",
       "      <td>1.161401</td>\n",
       "      <td>9.73</td>\n",
       "      <td>10.47</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.904164</td>\n",
       "      <td>2.977092</td>\n",
       "      <td>2.495116</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>1</td>\n",
       "      <td>43.5</td>\n",
       "      <td>71.6</td>\n",
       "      <td>23.4</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.865460</td>\n",
       "      <td>0.931256</td>\n",
       "      <td>13.16</td>\n",
       "      <td>9.72</td>\n",
       "      <td>233.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.945618</td>\n",
       "      <td>3.414186</td>\n",
       "      <td>2.861741</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2163 rows × 237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      category  agent_1_feat_Possession%  agent_1_feat_Pass%  \\\n",
       "20           0                      44.0                70.3   \n",
       "21           0                      57.0                84.6   \n",
       "22           1                      48.1                76.9   \n",
       "23           0                      46.3                70.8   \n",
       "24           0                      50.7                82.1   \n",
       "...        ...                       ...                 ...   \n",
       "2465         1                      41.6                76.0   \n",
       "2466         1                      42.9                76.1   \n",
       "2467         0                      41.0                72.2   \n",
       "2468         1                      51.4                79.3   \n",
       "2469         1                      43.5                71.6   \n",
       "\n",
       "      agent_1_feat_AerialsWon  agent_1_feat_Rating  agent_1_feat_XGrealiz  \\\n",
       "20                       25.1                 6.79               0.711201   \n",
       "21                       15.9                 7.07               1.094698   \n",
       "22                       17.7                 6.74               0.994530   \n",
       "23                       21.7                 6.77               0.918434   \n",
       "24                       14.4                 6.86               1.124694   \n",
       "...                       ...                  ...                    ...   \n",
       "2465                     17.1                 6.62               1.046406   \n",
       "2466                     18.3                 6.61               1.161802   \n",
       "2467                     19.1                 6.51               1.000858   \n",
       "2468                     14.1                 6.62               1.037986   \n",
       "2469                     23.4                 6.64               0.865460   \n",
       "\n",
       "      agent_1_feat_XGArealiz  agent_1_feat_PPDA  agent_1_feat_OPPDA  \\\n",
       "20                  0.915529              10.74                9.43   \n",
       "21                  0.938272               7.57               13.92   \n",
       "22                  1.235052               9.77                8.24   \n",
       "23                  1.118603               9.56                7.34   \n",
       "24                  0.875939              11.79               10.66   \n",
       "...                      ...                ...                 ...   \n",
       "2465                1.032989              18.00                8.27   \n",
       "2466                1.066236              16.14                7.60   \n",
       "2467                1.026472              15.99                7.99   \n",
       "2468                1.161401               9.73               10.47   \n",
       "2469                0.931256              13.16                9.72   \n",
       "\n",
       "      agent_1_feat_DC  ...  agent_2_feattotal_xg_1  \\\n",
       "20              218.0  ...                1.608046   \n",
       "21              575.0  ...                2.479335   \n",
       "22              175.0  ...                1.712261   \n",
       "23              179.0  ...                2.675331   \n",
       "24              156.0  ...                1.331644   \n",
       "...               ...  ...                     ...   \n",
       "2465            138.0  ...                3.684860   \n",
       "2466            201.0  ...                1.568175   \n",
       "2467            164.0  ...                3.871643   \n",
       "2468            222.0  ...                4.904164   \n",
       "2469            233.0  ...                3.945618   \n",
       "\n",
       "      agent_2_feattotal_xg_mean_3  agent_2_feattotal_xg_mean  \\\n",
       "20                       2.112304                   1.608046   \n",
       "21                       2.214160                   2.479335   \n",
       "22                       2.183093                   1.712261   \n",
       "23                       2.627794                   2.675331   \n",
       "24                       2.260683                   1.331644   \n",
       "...                           ...                        ...   \n",
       "2465                     4.024907                   3.872622   \n",
       "2466                     2.000313                   2.572016   \n",
       "2467                     2.496854                   2.555157   \n",
       "2468                     2.977092                   2.495116   \n",
       "2469                     3.414186                   2.861741   \n",
       "\n",
       "      agent_2_featboth_scored_3  agent_2_featboth_scored_2  \\\n",
       "20                     0.578947                   0.578947   \n",
       "21                     0.526316                   0.526316   \n",
       "22                     0.526316                   0.526316   \n",
       "23                     0.421053                   0.421053   \n",
       "24                     0.368421                   0.368421   \n",
       "...                         ...                        ...   \n",
       "2465                   1.000000                   0.000000   \n",
       "2466                   0.000000                   0.000000   \n",
       "2467                   0.000000                   0.000000   \n",
       "2468                   1.000000                   0.000000   \n",
       "2469                   1.000000                   1.000000   \n",
       "\n",
       "      agent_2_featboth_scored_1  agent_2_featboth_scored_mean_3  \\\n",
       "20                          1.0                        0.719298   \n",
       "21                          1.0                        0.684211   \n",
       "22                          1.0                        0.684211   \n",
       "23                          1.0                        0.614035   \n",
       "24                          0.0                        0.245614   \n",
       "...                         ...                             ...   \n",
       "2465                        0.0                        0.333333   \n",
       "2466                        0.0                        0.000000   \n",
       "2467                        1.0                        0.333333   \n",
       "2468                        0.0                        0.333333   \n",
       "2469                        0.0                        0.666667   \n",
       "\n",
       "      agent_2_featboth_scored_mean  expected_target1  expected_target2  \n",
       "20                        1.000000                 0                 0  \n",
       "21                        1.000000                 0                 1  \n",
       "22                        1.000000                 3                 3  \n",
       "23                        1.000000                 1                 0  \n",
       "24                        0.000000                 3                 0  \n",
       "...                            ...               ...               ...  \n",
       "2465                      0.444444                 1                 2  \n",
       "2466                      0.444444                 2                 3  \n",
       "2467                      0.500000                 0                 5  \n",
       "2468                      0.222222                 1                 3  \n",
       "2469                      0.333333                 3                 2  \n",
       "\n",
       "[2163 rows x 237 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_target1          1.000000\n",
      "agent_1_feat_ScoredAv     0.385431\n",
      "agent_1_feat_XgAv         0.360820\n",
      "agent_1_feat_DC           0.341588\n",
      "agent_1_feat_pl_median    0.328869\n",
      "                            ...   \n",
      "agent_1_feat_PPDA        -0.213783\n",
      "agent_2_feat_Rating      -0.215384\n",
      "agent_1_feat_MissedAv    -0.260551\n",
      "agent_1_feat_XgaAv       -0.267009\n",
      "agent_1_feat_ODC         -0.290223\n",
      "Name: expected_target1, Length: 237, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr_matrix1 = train_df.corr()\n",
    "corr1 = corr_matrix1[\"expected_target1\"].sort_values(ascending=False)\n",
    "print(corr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected_target2         1.000000\n",
      "category                 0.471964\n",
      "agent_2_feat_ScoredAv    0.335948\n",
      "agent_2_feat_XgAv        0.324758\n",
      "agent_2_feat_pl_mean     0.294398\n",
      "                           ...   \n",
      "agent_2_feat_MissedAv   -0.225085\n",
      "agent_1_feat_pl_mean    -0.225340\n",
      "agent_2_feat_XgaAv      -0.241378\n",
      "agent_1_feat_Rating     -0.250496\n",
      "agent_2_feat_ODC        -0.263211\n",
      "Name: expected_target2, Length: 237, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "corr_matrix2 = train_df.corr()\n",
    "corr2 = corr_matrix2[\"expected_target2\"].sort_values(ascending=False)\n",
    "print(corr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr1_df = pd.DataFrame({'feature': corr1.index, 'correlation':corr1.values})\n",
    "# corr1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>expected_target2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>category</td>\n",
       "      <td>0.471964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>agent_2_feat_ScoredAv</td>\n",
       "      <td>0.335948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>agent_2_feat_XgAv</td>\n",
       "      <td>0.324758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>agent_2_feat_pl_mean</td>\n",
       "      <td>0.294398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>agent_2_feat_MissedAv</td>\n",
       "      <td>-0.225085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>agent_1_feat_pl_mean</td>\n",
       "      <td>-0.225340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>agent_2_feat_XgaAv</td>\n",
       "      <td>-0.241378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>agent_1_feat_Rating</td>\n",
       "      <td>-0.250496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>agent_2_feat_ODC</td>\n",
       "      <td>-0.263211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   feature  correlation\n",
       "0         expected_target2     1.000000\n",
       "1                 category     0.471964\n",
       "2    agent_2_feat_ScoredAv     0.335948\n",
       "3        agent_2_feat_XgAv     0.324758\n",
       "4     agent_2_feat_pl_mean     0.294398\n",
       "..                     ...          ...\n",
       "232  agent_2_feat_MissedAv    -0.225085\n",
       "233   agent_1_feat_pl_mean    -0.225340\n",
       "234     agent_2_feat_XgaAv    -0.241378\n",
       "235    agent_1_feat_Rating    -0.250496\n",
       "236       agent_2_feat_ODC    -0.263211\n",
       "\n",
       "[237 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2_df = pd.DataFrame({'feature' : corr2.index, 'correlation' : corr2.values})\n",
    "corr2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corr1_df = corr1_df.drop(corr1_df[corr1_df.feature == 'category'].index)\n",
    "# corr1_df = corr1_df.drop(corr1_df[corr1_df.feature == 'expected_target1'].index)\n",
    "# corr1_df = corr1_df.drop(corr1_df[corr1_df.feature == 'expected_target2'].index)\n",
    "# corr1_df = corr1_df.loc[(corr1_df['correlation'] > 0.1) | (corr1_df['correlation'] < -0.1)]\n",
    "# len(corr1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2_df = corr2_df.drop(corr2_df[corr2_df.feature == 'category'].index)\n",
    "corr2_df = corr2_df.drop(corr2_df[corr2_df.feature == 'expected_target1'].index)\n",
    "corr2_df = corr2_df.drop(corr2_df[corr2_df.feature == 'expected_target2'].index)\n",
    "corr2_df = corr2_df.loc[(corr2_df['correlation'] > 0.15) | (corr2_df['correlation'] < -0.15)]\n",
    "len(corr2_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6FEvgOH7tUX"
   },
   "source": [
    "## Split dataset on train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "A3QmeYdOEhwI"
   },
   "outputs": [],
   "source": [
    "X = train_df.drop(['expected_target1', 'category', 'expected_target2'], axis=1)\n",
    "\n",
    "# X1 = X[list(corr1_df.feature)]\n",
    "X2 = X[list(corr2_df.feature)]\n",
    "\n",
    "X1 = X\n",
    "# X2 = X\n",
    "\n",
    "Y_category = train_df['category']\n",
    "Y_expected1 = train_df['expected_target1']\n",
    "Y_expected2 = train_df['expected_target2']\n",
    "\n",
    "# test_df1 = test_df[list(corr1_df.feature)]\n",
    "test_df2 = test_df[list(corr2_df.feature)]\n",
    "\n",
    "test_df1 = test_df\n",
    "# test_df2 = test_df\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = (X1.iloc[0:int(len(X1)*0.8)], \n",
    "                                    X1.iloc[int(len(X1)*0.8):len(X1)], \n",
    "                                    Y_expected1.iloc[0:int(len(Y_expected1)*0.8)], \n",
    "                                    Y_expected1.iloc[int(len(Y_expected1)*0.8):len(Y_expected1)],)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = (X2.iloc[0:int(len(X2)*0.8)], \n",
    "                                    X2.iloc[int(len(X2)*0.8):len(X2)], \n",
    "                                    Y_expected2.iloc[0:int(len(Y_expected2)*0.8)], \n",
    "                                    Y_expected2.iloc[int(len(Y_expected2)*0.8):len(Y_expected2)],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1730, 234), (433, 234), (1730,), (433,))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train1.shape, X_test1.shape, y_train1.shape, y_test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1730, 60), (433, 60), (1730,), (433,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train2.shape, X_test2.shape, y_train2.shape, y_test2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCXvCiWHPn3F"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(batch_size, epochs, learning_rate, num_classes, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation=\"relu\", input_shape=(input_shape, ))) # Hidden Layer 1\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(128, activation=\"relu\")) # Hidden Layer 2\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(64, activation=\"relu\")) # Hidden Layer 3\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(64, activation=\"relu\")) # Hidden Layer 4\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(32, activation=\"relu\")) # Hidden Layer 5\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "        \n",
    "    model.add(Dense(num_classes, activation=\"softmax\")) # Outout Layer\n",
    "    \n",
    "    sgd = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "    model.compile(optimizer=sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_equal_dummies(y_train, y_test):\n",
    "    dif = len(y_train.columns) - len(y_test.columns)\n",
    "    if dif == 0:\n",
    "        pass\n",
    "    elif dif > 0:\n",
    "        while dif != 0:\n",
    "            y_test[int(y_test.columns[-1])+1] = 0\n",
    "            dif -= 1\n",
    "    else:\n",
    "        while dif != 0:\n",
    "            y_train[int(y_train.columns[-1])+1] = 0\n",
    "            dif += 1\n",
    "            \n",
    "    print(y_train.shape, y_test.shape)\n",
    "    return y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_graphs(history):\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "    plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "    plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_acc(model, expected_target, Y_expected, X_test):\n",
    "    tmp_real_value = train_df[expected_target][int(len(Y_expected)*0.8):len(Y_expected)]\n",
    "    y_pred = model.predict(X_test)\n",
    "    tmp_pred_value = []\n",
    "    for i in y_pred:\n",
    "        if i.argmax() != 0:\n",
    "            tmp_pred_value.append(1)\n",
    "        else:\n",
    "            tmp_pred_value.append(0)\n",
    "    \n",
    "#     print(len(tmp_real_value), len(tmp_pred_value))\n",
    "    num_matching = 0\n",
    "    if len(tmp_real_value) == len(tmp_pred_value):\n",
    "        for i, j in zip(tmp_real_value, tmp_pred_value):\n",
    "            if i != 0:\n",
    "                i = 1\n",
    "            if i == j:\n",
    "                num_matching += 1\n",
    "        print(\"ACC: \", np.round((num_matching / len(tmp_real_value)), 3))\n",
    "    else:\n",
    "        print('Not equal siae of predictions')\n",
    "        \n",
    "    return tmp_pred_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_data(X_train, X_test, test_df):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    test_df = scaler.transform(test_df)\n",
    "    \n",
    "    return X_train, X_test, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_model(X_train, y_train, X_test, y_test):\n",
    "    error_rates = []\n",
    "\n",
    "    for i in np.arange(1, 100):\n",
    "        new_model = KNeighborsClassifier(n_neighbors = i)\n",
    "        new_model.fit(X_train, y_train)\n",
    "        model = KNeighborsClassifier(n_neighbors = 5)\n",
    "        model.fit(X_train, y_train)\n",
    "        new_predictions = new_model.predict(X_test)\n",
    "        error_rates.append(np.mean(new_predictions != y_test))\n",
    "\n",
    "    plt.plot(error_rates);\n",
    "    \n",
    "    return error_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(X_train, y_train, X_test, y_test):\n",
    "    feature_selection_model = KNeighborsClassifier(n_neighbors = np.argmin(KNN_model(X_train, y_train, X_test, y_test)))\n",
    "    \n",
    "    sfs = SFS(feature_selection_model,\n",
    "          k_features=X_train.shape[1],\n",
    "          forward=True, \n",
    "          floating=False, \n",
    "          scoring='accuracy', \n",
    "          verbose=1,\n",
    "          cv=StratifiedKFold(n_splits=5),\n",
    "          n_jobs=-1\n",
    "          )\n",
    "\n",
    "    sfs = sfs.fit(X_train, y_train)\n",
    "    \n",
    "    sfs_data = pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "\n",
    "    sfs_data['avg_score'] = pd.to_numeric(sfs_data['avg_score'])\n",
    "    fearues_names = (sfs_data.loc[sfs_data['avg_score'].idxmax(), 'feature_names'])\n",
    "    \n",
    "    return fearues_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train, expected_target):\n",
    "    batch_size = [16, 32, 64, 128]\n",
    "    epochs = [50, 75, 125]\n",
    "    learning_rate = [0.001, 0.0001]\n",
    "    num_classes = [train_df[expected_target].nunique()]\n",
    "    input_shape = [X_train.shape[1]]\n",
    "    param_opt = dict(batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     learning_rate=learning_rate,\n",
    "                     num_classes=num_classes,\n",
    "                     input_shape=input_shape)\n",
    "\n",
    "\n",
    "    model_GridSearch = KerasClassifier(build_fn=create_model, \n",
    "                                       verbose=0)\n",
    "    grid = GridSearchCV(estimator=model_GridSearch, \n",
    "                        param_grid=param_opt, \n",
    "                        n_jobs=-1, \n",
    "                        cv=5, \n",
    "                        verbose = 0)\n",
    "    grid_result = grid.fit(X_train, y_train)\n",
    "    \n",
    "    print('Best parameters are: ')\n",
    "    print('batch_size: ' + str(grid_result.best_params_['batch_size']))\n",
    "    print('epochs: ' + str(grid_result.best_params_['epochs']))\n",
    "    print('learning_rate: ' + str(grid_result.best_params_['learning_rate']))\n",
    "    \n",
    "    return grid_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for Agent1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 234 out of 234 | elapsed:    4.2s finished\n",
      "Features: 1/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 222 out of 233 | elapsed:    3.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 233 out of 233 | elapsed:    3.3s finished\n",
      "Features: 2/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 221 out of 232 | elapsed:    3.4s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 232 out of 232 | elapsed:    3.5s finished\n",
      "Features: 3/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 220 out of 231 | elapsed:    3.8s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 231 out of 231 | elapsed:    3.9s finished\n",
      "Features: 4/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 230 out of 230 | elapsed:    4.4s finished\n",
      "Features: 5/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 218 out of 229 | elapsed:    4.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 229 out of 229 | elapsed:    4.5s finished\n",
      "Features: 6/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 228 out of 228 | elapsed:    4.4s finished\n",
      "Features: 7/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 227 | elapsed:    3.8s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 227 out of 227 | elapsed:    3.9s finished\n",
      "Features: 8/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done 215 out of 226 | elapsed:    3.8s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 226 out of 226 | elapsed:    4.0s finished\n",
      "Features: 9/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 214 out of 225 | elapsed:    3.8s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:    4.0s finished\n",
      "Features: 10/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 224 out of 224 | elapsed:    4.4s finished\n",
      "Features: 11/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done 212 out of 223 | elapsed:    4.1s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 223 out of 223 | elapsed:    4.2s finished\n",
      "Features: 12/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 211 out of 222 | elapsed:    4.3s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 222 out of 222 | elapsed:    4.5s finished\n",
      "Features: 13/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 221 | elapsed:    5.4s remaining:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 221 out of 221 | elapsed:    5.7s finished\n",
      "Features: 14/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 220 out of 220 | elapsed:    5.2s finished\n",
      "Features: 15/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    8.3s\n",
      "[Parallel(n_jobs=-1)]: Done 219 out of 219 | elapsed:    9.7s finished\n",
      "Features: 16/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 218 out of 218 | elapsed:    8.2s finished\n",
      "Features: 17/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done 217 out of 217 | elapsed:    8.0s finished\n",
      "Features: 18/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 216 out of 216 | elapsed:    8.8s finished\n",
      "Features: 19/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 215 out of 215 | elapsed:    8.8s finished\n",
      "Features: 20/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 214 out of 214 | elapsed:    9.7s finished\n",
      "Features: 21/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 213 out of 213 | elapsed:    8.6s finished\n",
      "Features: 22/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 212 out of 212 | elapsed:    8.4s finished\n",
      "Features: 23/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 211 out of 211 | elapsed:    8.6s finished\n",
      "Features: 24/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.5s\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 210 | elapsed:    8.3s finished\n",
      "Features: 25/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 209 out of 209 | elapsed:    7.8s finished\n",
      "Features: 26/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 208 out of 208 | elapsed:    8.0s finished\n",
      "Features: 27/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 207 out of 207 | elapsed:    8.0s finished\n",
      "Features: 28/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 206 out of 206 | elapsed:    7.9s finished\n",
      "Features: 29/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.2s\n",
      "[Parallel(n_jobs=-1)]: Done 205 out of 205 | elapsed:    7.8s finished\n",
      "Features: 30/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.7s\n",
      "[Parallel(n_jobs=-1)]: Done 204 out of 204 | elapsed:    8.3s finished\n",
      "Features: 31/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done 203 out of 203 | elapsed:    9.2s finished\n",
      "Features: 32/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    8.5s\n",
      "[Parallel(n_jobs=-1)]: Done 202 out of 202 | elapsed:    9.0s finished\n",
      "Features: 33/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 201 out of 201 | elapsed:    8.6s finished\n",
      "Features: 34/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    7.9s finished\n",
      "Features: 35/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=-1)]: Done 199 out of 199 | elapsed:    7.8s finished\n",
      "Features: 36/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 198 out of 198 | elapsed:    7.6s finished\n",
      "Features: 37/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 197 out of 197 | elapsed:    7.5s finished\n",
      "Features: 38/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 196 out of 196 | elapsed:    7.7s finished\n",
      "Features: 39/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 195 out of 195 | elapsed:    7.5s finished\n",
      "Features: 40/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 194 out of 194 | elapsed:    7.5s finished\n",
      "Features: 41/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 193 out of 193 | elapsed:    7.5s finished\n",
      "Features: 42/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 192 out of 192 | elapsed:    7.4s finished\n",
      "Features: 43/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 191 out of 191 | elapsed:    7.4s finished\n",
      "Features: 44/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 190 out of 190 | elapsed:    7.4s finished\n",
      "Features: 45/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 189 out of 189 | elapsed:    7.3s finished\n",
      "Features: 46/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 188 out of 188 | elapsed:    7.3s finished\n",
      "Features: 47/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 187 out of 187 | elapsed:    7.5s finished\n",
      "Features: 48/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 186 out of 186 | elapsed:    7.3s finished\n",
      "Features: 49/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 185 out of 185 | elapsed:    7.4s finished\n",
      "Features: 50/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 out of 184 | elapsed:    7.3s finished\n",
      "Features: 51/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 183 out of 183 | elapsed:    7.2s finished\n",
      "Features: 52/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 182 out of 182 | elapsed:    7.0s finished\n",
      "Features: 53/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 181 out of 181 | elapsed:    7.2s finished\n",
      "Features: 54/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:    7.3s finished\n",
      "Features: 55/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 179 out of 179 | elapsed:    7.2s finished\n",
      "Features: 56/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 178 out of 178 | elapsed:    7.6s finished\n",
      "Features: 57/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 177 out of 177 | elapsed:    6.9s finished\n",
      "Features: 58/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 176 out of 176 | elapsed:    7.3s finished\n",
      "Features: 59/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 175 out of 175 | elapsed:    7.4s finished\n",
      "Features: 60/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 174 out of 174 | elapsed:    7.1s finished\n",
      "Features: 61/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 173 out of 173 | elapsed:    7.4s finished\n",
      "Features: 62/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 172 out of 172 | elapsed:    6.8s finished\n",
      "Features: 63/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 171 out of 171 | elapsed:    6.4s finished\n",
      "Features: 64/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 170 out of 170 | elapsed:    6.4s finished\n",
      "Features: 65/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 169 out of 169 | elapsed:    6.7s finished\n",
      "Features: 66/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 out of 168 | elapsed:    6.2s finished\n",
      "Features: 67/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 167 out of 167 | elapsed:    6.1s finished\n",
      "Features: 68/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 166 out of 166 | elapsed:    6.1s finished\n",
      "Features: 69/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 165 out of 165 | elapsed:    6.0s finished\n",
      "Features: 70/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 164 out of 164 | elapsed:    5.9s finished\n",
      "Features: 71/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 163 out of 163 | elapsed:    6.0s finished\n",
      "Features: 72/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 162 out of 162 | elapsed:    5.9s finished\n",
      "Features: 73/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 161 out of 161 | elapsed:    5.8s finished\n",
      "Features: 74/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:    5.8s finished\n",
      "Features: 75/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 159 out of 159 | elapsed:    5.9s finished\n",
      "Features: 76/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 158 out of 158 | elapsed:    5.9s finished\n",
      "Features: 77/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 157 out of 157 | elapsed:    5.7s finished\n",
      "Features: 78/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 156 out of 156 | elapsed:    5.7s finished\n",
      "Features: 79/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 155 out of 155 | elapsed:    5.7s finished\n",
      "Features: 80/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 out of 154 | elapsed:    5.6s finished\n",
      "Features: 81/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 153 out of 153 | elapsed:    5.6s finished\n",
      "Features: 82/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 152 out of 152 | elapsed:    5.6s finished\n",
      "Features: 83/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 151 out of 151 | elapsed:    5.5s finished\n",
      "Features: 84/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed:    5.4s finished\n",
      "Features: 85/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 149 out of 149 | elapsed:    5.4s finished\n",
      "Features: 86/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 148 out of 148 | elapsed:    5.5s finished\n",
      "Features: 87/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 147 out of 147 | elapsed:    5.6s finished\n",
      "Features: 88/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 146 out of 146 | elapsed:    5.3s finished\n",
      "Features: 89/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 145 out of 145 | elapsed:    5.4s finished\n",
      "Features: 90/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed:    5.4s finished\n",
      "Features: 91/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 143 out of 143 | elapsed:    5.2s finished\n",
      "Features: 92/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 142 out of 142 | elapsed:    5.3s finished\n",
      "Features: 93/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 141 out of 141 | elapsed:    5.2s finished\n",
      "Features: 94/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 140 out of 140 | elapsed:    5.2s finished\n",
      "Features: 95/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 139 out of 139 | elapsed:    5.2s finished\n",
      "Features: 96/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 138 out of 138 | elapsed:    5.1s finished\n",
      "Features: 97/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 137 out of 137 | elapsed:    5.1s finished\n",
      "Features: 98/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 136 out of 136 | elapsed:    5.1s finished\n",
      "Features: 99/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:    5.3s finished\n",
      "Features: 100/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 134 out of 134 | elapsed:    5.0s finished\n",
      "Features: 101/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 133 out of 133 | elapsed:    5.0s finished\n",
      "Features: 102/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 132 out of 132 | elapsed:    5.0s finished\n",
      "Features: 103/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 131 out of 131 | elapsed:    4.9s finished\n",
      "Features: 104/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 130 out of 130 | elapsed:    4.9s finished\n",
      "Features: 105/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 129 out of 129 | elapsed:    4.8s finished\n",
      "Features: 106/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 128 out of 128 | elapsed:    4.9s finished\n",
      "Features: 107/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 127 out of 127 | elapsed:    5.1s finished\n",
      "Features: 108/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 126 out of 126 | elapsed:    4.6s finished\n",
      "Features: 109/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 125 out of 125 | elapsed:    4.8s finished\n",
      "Features: 110/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 124 out of 124 | elapsed:    5.5s finished\n",
      "Features: 111/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done 123 out of 123 | elapsed:    4.9s finished\n",
      "Features: 112/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 122 out of 122 | elapsed:    4.8s finished\n",
      "Features: 113/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done 121 out of 121 | elapsed:    4.6s finished\n",
      "Features: 114/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:    4.5s finished\n",
      "Features: 115/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 119 out of 119 | elapsed:    4.5s finished\n",
      "Features: 116/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 118 out of 118 | elapsed:    4.6s finished\n",
      "Features: 117/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 117 out of 117 | elapsed:    4.4s finished\n",
      "Features: 118/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 116 out of 116 | elapsed:    4.4s finished\n",
      "Features: 119/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 115 out of 115 | elapsed:    4.5s finished\n",
      "Features: 120/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 114 out of 114 | elapsed:    4.3s finished\n",
      "Features: 121/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 113 out of 113 | elapsed:    4.3s finished\n",
      "Features: 122/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:    4.3s finished\n",
      "Features: 123/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 111 out of 111 | elapsed:    4.3s finished\n",
      "Features: 124/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed:    4.2s finished\n",
      "Features: 125/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 109 out of 109 | elapsed:    4.2s finished\n",
      "Features: 126/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 108 out of 108 | elapsed:    4.4s finished\n",
      "Features: 127/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 107 out of 107 | elapsed:    4.1s finished\n",
      "Features: 128/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 106 out of 106 | elapsed:    4.1s finished\n",
      "Features: 129/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 105 out of 105 | elapsed:    4.1s finished\n",
      "Features: 130/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 104 out of 104 | elapsed:    4.1s finished\n",
      "Features: 131/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 103 out of 103 | elapsed:    4.1s finished\n",
      "Features: 132/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 102 out of 102 | elapsed:    3.8s finished\n",
      "Features: 133/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 101 out of 101 | elapsed:    3.9s finished\n",
      "Features: 134/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:    3.9s finished\n",
      "Features: 135/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  99 out of  99 | elapsed:    3.9s finished\n",
      "Features: 136/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  98 out of  98 | elapsed:    3.8s finished\n",
      "Features: 137/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  97 out of  97 | elapsed:    3.8s finished\n",
      "Features: 138/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  96 out of  96 | elapsed:    3.7s finished\n",
      "Features: 139/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  95 out of  95 | elapsed:    3.7s finished\n",
      "Features: 140/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  94 out of  94 | elapsed:    3.7s finished\n",
      "Features: 141/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  93 out of  93 | elapsed:    3.7s finished\n",
      "Features: 142/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of  92 | elapsed:    3.8s finished\n",
      "Features: 143/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  91 out of  91 | elapsed:    3.6s finished\n",
      "Features: 144/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:    3.5s finished\n",
      "Features: 145/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  89 out of  89 | elapsed:    3.5s finished\n",
      "Features: 146/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  88 out of  88 | elapsed:    3.5s finished\n",
      "Features: 147/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  87 out of  87 | elapsed:    3.5s finished\n",
      "Features: 148/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  86 out of  86 | elapsed:    3.5s finished\n",
      "Features: 149/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  85 | elapsed:    3.4s finished\n",
      "Features: 150/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  84 out of  84 | elapsed:    3.3s finished\n",
      "Features: 151/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  83 out of  83 | elapsed:    3.3s finished\n",
      "Features: 152/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  82 out of  82 | elapsed:    3.2s finished\n",
      "Features: 153/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  81 out of  81 | elapsed:    3.2s finished\n",
      "Features: 154/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:    3.1s finished\n",
      "Features: 155/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  79 out of  79 | elapsed:    3.2s finished\n",
      "Features: 156/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  78 out of  78 | elapsed:    3.1s finished\n",
      "Features: 157/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  77 out of  77 | elapsed:    3.0s finished\n",
      "Features: 158/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  76 out of  76 | elapsed:    3.0s finished\n",
      "Features: 159/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  75 | elapsed:    3.0s finished\n",
      "Features: 160/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  74 out of  74 | elapsed:    3.1s finished\n",
      "Features: 161/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  73 out of  73 | elapsed:    3.1s finished\n",
      "Features: 162/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.5s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    2.8s finished\n",
      "Features: 163/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  71 out of  71 | elapsed:    2.8s finished\n",
      "Features: 164/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  70 out of  70 | elapsed:    2.8s finished\n",
      "Features: 165/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  69 out of  69 | elapsed:    2.7s finished\n",
      "Features: 166/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  68 out of  68 | elapsed:    2.8s finished\n",
      "Features: 167/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  67 out of  67 | elapsed:    2.8s finished\n",
      "Features: 168/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  66 | elapsed:    2.8s finished\n",
      "Features: 169/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  65 out of  65 | elapsed:    2.8s finished\n",
      "Features: 170/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  64 out of  64 | elapsed:    2.6s finished\n",
      "Features: 171/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  63 | elapsed:    2.6s finished\n",
      "Features: 172/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  62 out of  62 | elapsed:    2.6s finished\n",
      "Features: 173/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  61 out of  61 | elapsed:    2.5s finished\n",
      "Features: 174/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:    2.3s finished\n",
      "Features: 175/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  59 out of  59 | elapsed:    2.4s finished\n",
      "Features: 176/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  58 out of  58 | elapsed:    2.4s finished\n",
      "Features: 177/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  57 out of  57 | elapsed:    2.3s finished\n",
      "Features: 178/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  56 | elapsed:    2.3s finished\n",
      "Features: 179/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  55 out of  55 | elapsed:    2.3s finished\n",
      "Features: 180/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    2.1s finished\n",
      "Features: 181/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done  53 out of  53 | elapsed:    2.2s finished\n",
      "Features: 182/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  52 out of  52 | elapsed:    2.2s finished\n",
      "Features: 183/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  51 | elapsed:    2.2s finished\n",
      "Features: 184/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    2.2s finished\n",
      "Features: 185/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  49 | elapsed:    2.3s finished\n",
      "Features: 186/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 out of  48 | elapsed:    2.1s finished\n",
      "Features: 187/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  47 | elapsed:    1.9s finished\n",
      "Features: 188/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  46 out of  46 | elapsed:    1.9s finished\n",
      "Features: 189/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:    1.9s finished\n",
      "Features: 190/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  44 out of  44 | elapsed:    1.8s finished\n",
      "Features: 191/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  43 out of  43 | elapsed:    1.8s finished\n",
      "Features: 192/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  42 | elapsed:    1.6s finished\n",
      "Features: 193/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  41 | elapsed:    1.7s finished\n",
      "Features: 194/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    1.6s finished\n",
      "Features: 195/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  39 out of  39 | elapsed:    1.6s finished\n",
      "Features: 196/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  38 out of  38 | elapsed:    1.6s finished\n",
      "Features: 197/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  37 out of  37 | elapsed:    1.6s finished\n",
      "Features: 198/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:    1.4s finished\n",
      "Features: 199/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 out of  35 | elapsed:    1.4s finished\n",
      "Features: 200/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  34 | elapsed:    1.4s finished\n",
      "Features: 201/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  33 | elapsed:    1.4s finished\n",
      "Features: 202/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  32 | elapsed:    1.3s finished\n",
      "Features: 203/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  31 | elapsed:    1.3s finished\n",
      "Features: 204/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    1.1s finished\n",
      "Features: 205/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 out of  29 | elapsed:    1.2s finished\n",
      "Features: 206/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  28 out of  28 | elapsed:    1.2s finished\n",
      "Features: 207/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  27 out of  27 | elapsed:    1.2s finished\n",
      "Features: 208/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 out of  26 | elapsed:    1.1s finished\n",
      "Features: 209/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:    1.1s finished\n",
      "Features: 210/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:    0.9s finished\n",
      "Features: 211/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  23 out of  23 | elapsed:    0.9s finished\n",
      "Features: 212/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  22 | elapsed:    0.9s finished\n",
      "Features: 213/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  21 out of  21 | elapsed:    0.9s finished\n",
      "Features: 214/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    0.8s finished\n",
      "Features: 215/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  19 out of  19 | elapsed:    0.8s finished\n",
      "Features: 216/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:    0.7s finished\n",
      "Features: 217/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  17 out of  17 | elapsed:    0.7s finished\n",
      "Features: 218/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  16 | elapsed:    0.7s finished\n",
      "Features: 219/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    0.7s finished\n",
      "Features: 220/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  14 | elapsed:    0.6s finished\n",
      "Features: 221/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  13 | elapsed:    0.6s finished\n",
      "Features: 222/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.4s finished\n",
      "Features: 223/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  11 out of  11 | elapsed:    0.5s finished\n",
      "Features: 224/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.5s finished\n",
      "Features: 225/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:    0.4s finished\n",
      "Features: 226/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   8 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed:    0.4s finished\n",
      "Features: 227/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   7 | elapsed:    0.2s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:    0.4s finished\n",
      "Features: 228/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   6 | elapsed:    0.2s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:    0.2s finished\n",
      "Features: 229/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    0.2s finished\n",
      "Features: 230/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.2s finished\n",
      "Features: 231/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    0.2s finished\n",
      "Features: 232/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:    0.1s finished\n",
      "Features: 233/234[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.1s finished\n",
      "Features: 234/234"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fearues_names1 = feature_selection(X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_1_feat_AerialsWon</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "      <th>agent_1_feat_XGrealiz</th>\n",
       "      <th>agent_1_feat_XGArealiz</th>\n",
       "      <th>agent_1_feat_PPDA</th>\n",
       "      <th>agent_1_feat_OPPDA</th>\n",
       "      <th>agent_1_feat_Coach</th>\n",
       "      <th>agent_1_feat_ScoredAv</th>\n",
       "      <th>agent_1_feat_MissedAv</th>\n",
       "      <th>agent_1_feat_XgAv</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_2_feattotal_xg_3</th>\n",
       "      <th>agent_2_feattotal_xg_2</th>\n",
       "      <th>agent_2_feattotal_xg_1</th>\n",
       "      <th>agent_2_feattotal_xg_mean_3</th>\n",
       "      <th>agent_2_feattotal_xg_mean</th>\n",
       "      <th>agent_2_featboth_scored_3</th>\n",
       "      <th>agent_2_featboth_scored_2</th>\n",
       "      <th>agent_2_featboth_scored_1</th>\n",
       "      <th>agent_2_featboth_scored_mean_3</th>\n",
       "      <th>agent_2_featboth_scored_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25.1</td>\n",
       "      <td>6.79</td>\n",
       "      <td>0.711201</td>\n",
       "      <td>0.915529</td>\n",
       "      <td>10.74</td>\n",
       "      <td>9.43</td>\n",
       "      <td>135.1917</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>1.394737</td>\n",
       "      <td>1.036053</td>\n",
       "      <td>...</td>\n",
       "      <td>2.364433</td>\n",
       "      <td>2.364433</td>\n",
       "      <td>1.608046</td>\n",
       "      <td>2.112304</td>\n",
       "      <td>1.608046</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15.9</td>\n",
       "      <td>7.07</td>\n",
       "      <td>1.094698</td>\n",
       "      <td>0.938272</td>\n",
       "      <td>7.57</td>\n",
       "      <td>13.92</td>\n",
       "      <td>154.0908</td>\n",
       "      <td>2.184211</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.995263</td>\n",
       "      <td>...</td>\n",
       "      <td>2.081572</td>\n",
       "      <td>2.081572</td>\n",
       "      <td>2.479335</td>\n",
       "      <td>2.214160</td>\n",
       "      <td>2.479335</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17.7</td>\n",
       "      <td>6.74</td>\n",
       "      <td>0.994530</td>\n",
       "      <td>1.235052</td>\n",
       "      <td>9.77</td>\n",
       "      <td>8.24</td>\n",
       "      <td>133.6599</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>1.657895</td>\n",
       "      <td>1.058421</td>\n",
       "      <td>...</td>\n",
       "      <td>2.418509</td>\n",
       "      <td>2.418509</td>\n",
       "      <td>1.712261</td>\n",
       "      <td>2.183093</td>\n",
       "      <td>1.712261</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>21.7</td>\n",
       "      <td>6.77</td>\n",
       "      <td>0.918434</td>\n",
       "      <td>1.118603</td>\n",
       "      <td>9.56</td>\n",
       "      <td>7.34</td>\n",
       "      <td>121.3641</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>1.921053</td>\n",
       "      <td>1.203421</td>\n",
       "      <td>...</td>\n",
       "      <td>2.604025</td>\n",
       "      <td>2.604025</td>\n",
       "      <td>2.675331</td>\n",
       "      <td>2.627794</td>\n",
       "      <td>2.675331</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>14.4</td>\n",
       "      <td>6.86</td>\n",
       "      <td>1.124694</td>\n",
       "      <td>0.875939</td>\n",
       "      <td>11.79</td>\n",
       "      <td>10.66</td>\n",
       "      <td>122.7303</td>\n",
       "      <td>1.210526</td>\n",
       "      <td>1.289474</td>\n",
       "      <td>1.076316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.725202</td>\n",
       "      <td>2.725202</td>\n",
       "      <td>1.331644</td>\n",
       "      <td>2.260683</td>\n",
       "      <td>1.331644</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>14.1</td>\n",
       "      <td>6.52</td>\n",
       "      <td>0.698362</td>\n",
       "      <td>1.047340</td>\n",
       "      <td>12.59</td>\n",
       "      <td>9.65</td>\n",
       "      <td>136.2900</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.973684</td>\n",
       "      <td>0.979737</td>\n",
       "      <td>...</td>\n",
       "      <td>3.212219</td>\n",
       "      <td>1.717480</td>\n",
       "      <td>4.247550</td>\n",
       "      <td>3.059083</td>\n",
       "      <td>3.059083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>21.9</td>\n",
       "      <td>6.59</td>\n",
       "      <td>0.741351</td>\n",
       "      <td>1.075088</td>\n",
       "      <td>12.20</td>\n",
       "      <td>9.64</td>\n",
       "      <td>135.4500</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.684211</td>\n",
       "      <td>1.277895</td>\n",
       "      <td>...</td>\n",
       "      <td>2.939630</td>\n",
       "      <td>3.352280</td>\n",
       "      <td>1.768421</td>\n",
       "      <td>2.686777</td>\n",
       "      <td>2.686777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>22.9</td>\n",
       "      <td>6.72</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>0.907494</td>\n",
       "      <td>13.22</td>\n",
       "      <td>9.50</td>\n",
       "      <td>146.5800</td>\n",
       "      <td>1.289474</td>\n",
       "      <td>1.631579</td>\n",
       "      <td>1.291316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.089692</td>\n",
       "      <td>2.912603</td>\n",
       "      <td>1.626763</td>\n",
       "      <td>2.209686</td>\n",
       "      <td>2.209686</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>21.7</td>\n",
       "      <td>6.71</td>\n",
       "      <td>0.902655</td>\n",
       "      <td>1.060258</td>\n",
       "      <td>8.26</td>\n",
       "      <td>9.97</td>\n",
       "      <td>138.1800</td>\n",
       "      <td>1.342105</td>\n",
       "      <td>1.578947</td>\n",
       "      <td>1.486842</td>\n",
       "      <td>...</td>\n",
       "      <td>3.469760</td>\n",
       "      <td>2.973320</td>\n",
       "      <td>4.561580</td>\n",
       "      <td>3.668220</td>\n",
       "      <td>3.668220</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>13.5</td>\n",
       "      <td>6.71</td>\n",
       "      <td>1.101928</td>\n",
       "      <td>0.838428</td>\n",
       "      <td>11.02</td>\n",
       "      <td>11.45</td>\n",
       "      <td>150.9900</td>\n",
       "      <td>1.473684</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>1.337368</td>\n",
       "      <td>...</td>\n",
       "      <td>3.278460</td>\n",
       "      <td>2.792386</td>\n",
       "      <td>3.477940</td>\n",
       "      <td>3.182929</td>\n",
       "      <td>3.182929</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      agent_1_feat_AerialsWon  agent_1_feat_Rating  agent_1_feat_XGrealiz  \\\n",
       "20                       25.1                 6.79               0.711201   \n",
       "21                       15.9                 7.07               1.094698   \n",
       "22                       17.7                 6.74               0.994530   \n",
       "23                       21.7                 6.77               0.918434   \n",
       "24                       14.4                 6.86               1.124694   \n",
       "...                       ...                  ...                    ...   \n",
       "1973                     14.1                 6.52               0.698362   \n",
       "1974                     21.9                 6.59               0.741351   \n",
       "1975                     22.9                 6.72               0.998573   \n",
       "1977                     21.7                 6.71               0.902655   \n",
       "1978                     13.5                 6.71               1.101928   \n",
       "\n",
       "      agent_1_feat_XGArealiz  agent_1_feat_PPDA  agent_1_feat_OPPDA  \\\n",
       "20                  0.915529              10.74                9.43   \n",
       "21                  0.938272               7.57               13.92   \n",
       "22                  1.235052               9.77                8.24   \n",
       "23                  1.118603               9.56                7.34   \n",
       "24                  0.875939              11.79               10.66   \n",
       "...                      ...                ...                 ...   \n",
       "1973                1.047340              12.59                9.65   \n",
       "1974                1.075088              12.20                9.64   \n",
       "1975                0.907494              13.22                9.50   \n",
       "1977                1.060258               8.26                9.97   \n",
       "1978                0.838428              11.02               11.45   \n",
       "\n",
       "      agent_1_feat_Coach  agent_1_feat_ScoredAv  agent_1_feat_MissedAv  \\\n",
       "20              135.1917               0.736842               1.394737   \n",
       "21              154.0908               2.184211               1.000000   \n",
       "22              133.6599               1.052632               1.657895   \n",
       "23              121.3641               1.105263               1.921053   \n",
       "24              122.7303               1.210526               1.289474   \n",
       "...                  ...                    ...                    ...   \n",
       "1973            136.2900               0.684211               1.973684   \n",
       "1974            135.4500               0.947368               1.684211   \n",
       "1975            146.5800               1.289474               1.631579   \n",
       "1977            138.1800               1.342105               1.578947   \n",
       "1978            150.9900               1.473684               1.263158   \n",
       "\n",
       "      agent_1_feat_XgAv  ...  agent_2_feattotal_xg_3  agent_2_feattotal_xg_2  \\\n",
       "20             1.036053  ...                2.364433                2.364433   \n",
       "21             1.995263  ...                2.081572                2.081572   \n",
       "22             1.058421  ...                2.418509                2.418509   \n",
       "23             1.203421  ...                2.604025                2.604025   \n",
       "24             1.076316  ...                2.725202                2.725202   \n",
       "...                 ...  ...                     ...                     ...   \n",
       "1973           0.979737  ...                3.212219                1.717480   \n",
       "1974           1.277895  ...                2.939630                3.352280   \n",
       "1975           1.291316  ...                2.089692                2.912603   \n",
       "1977           1.486842  ...                3.469760                2.973320   \n",
       "1978           1.337368  ...                3.278460                2.792386   \n",
       "\n",
       "      agent_2_feattotal_xg_1  agent_2_feattotal_xg_mean_3  \\\n",
       "20                  1.608046                     2.112304   \n",
       "21                  2.479335                     2.214160   \n",
       "22                  1.712261                     2.183093   \n",
       "23                  2.675331                     2.627794   \n",
       "24                  1.331644                     2.260683   \n",
       "...                      ...                          ...   \n",
       "1973                4.247550                     3.059083   \n",
       "1974                1.768421                     2.686777   \n",
       "1975                1.626763                     2.209686   \n",
       "1977                4.561580                     3.668220   \n",
       "1978                3.477940                     3.182929   \n",
       "\n",
       "      agent_2_feattotal_xg_mean  agent_2_featboth_scored_3  \\\n",
       "20                     1.608046                   0.578947   \n",
       "21                     2.479335                   0.526316   \n",
       "22                     1.712261                   0.526316   \n",
       "23                     2.675331                   0.421053   \n",
       "24                     1.331644                   0.368421   \n",
       "...                         ...                        ...   \n",
       "1973                   3.059083                   1.000000   \n",
       "1974                   2.686777                   1.000000   \n",
       "1975                   2.209686                   0.000000   \n",
       "1977                   3.668220                   0.000000   \n",
       "1978                   3.182929                   1.000000   \n",
       "\n",
       "      agent_2_featboth_scored_2  agent_2_featboth_scored_1  \\\n",
       "20                     0.578947                        1.0   \n",
       "21                     0.526316                        1.0   \n",
       "22                     0.526316                        1.0   \n",
       "23                     0.421053                        1.0   \n",
       "24                     0.368421                        0.0   \n",
       "...                         ...                        ...   \n",
       "1973                   0.000000                        1.0   \n",
       "1974                   1.000000                        0.0   \n",
       "1975                   1.000000                        0.0   \n",
       "1977                   1.000000                        1.0   \n",
       "1978                   1.000000                        1.0   \n",
       "\n",
       "      agent_2_featboth_scored_mean_3  agent_2_featboth_scored_mean  \n",
       "20                          0.719298                      1.000000  \n",
       "21                          0.684211                      1.000000  \n",
       "22                          0.684211                      1.000000  \n",
       "23                          0.614035                      1.000000  \n",
       "24                          0.245614                      0.000000  \n",
       "...                              ...                           ...  \n",
       "1973                        0.666667                      0.666667  \n",
       "1974                        0.666667                      0.666667  \n",
       "1975                        0.333333                      0.333333  \n",
       "1977                        0.666667                      0.666667  \n",
       "1978                        1.000000                      1.000000  \n",
       "\n",
       "[1730 rows x 185 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train1 = X_train1[list(fearues_names1)]\n",
    "new_X_train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_1_feat_AerialsWon</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "      <th>agent_1_feat_XGrealiz</th>\n",
       "      <th>agent_1_feat_XGArealiz</th>\n",
       "      <th>agent_1_feat_PPDA</th>\n",
       "      <th>agent_1_feat_OPPDA</th>\n",
       "      <th>agent_1_feat_Coach</th>\n",
       "      <th>agent_1_feat_ScoredAv</th>\n",
       "      <th>agent_1_feat_MissedAv</th>\n",
       "      <th>agent_1_feat_XgAv</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_2_feattotal_xg_3</th>\n",
       "      <th>agent_2_feattotal_xg_2</th>\n",
       "      <th>agent_2_feattotal_xg_1</th>\n",
       "      <th>agent_2_feattotal_xg_mean_3</th>\n",
       "      <th>agent_2_feattotal_xg_mean</th>\n",
       "      <th>agent_2_featboth_scored_3</th>\n",
       "      <th>agent_2_featboth_scored_2</th>\n",
       "      <th>agent_2_featboth_scored_1</th>\n",
       "      <th>agent_2_featboth_scored_mean_3</th>\n",
       "      <th>agent_2_featboth_scored_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>19.8</td>\n",
       "      <td>6.65</td>\n",
       "      <td>1.041381</td>\n",
       "      <td>0.865284</td>\n",
       "      <td>19.00</td>\n",
       "      <td>7.55</td>\n",
       "      <td>129.36</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.526316</td>\n",
       "      <td>0.960263</td>\n",
       "      <td>...</td>\n",
       "      <td>1.450676</td>\n",
       "      <td>2.673176</td>\n",
       "      <td>1.792020</td>\n",
       "      <td>1.971957</td>\n",
       "      <td>1.971957</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>23.1</td>\n",
       "      <td>6.68</td>\n",
       "      <td>0.819214</td>\n",
       "      <td>1.137980</td>\n",
       "      <td>10.17</td>\n",
       "      <td>10.38</td>\n",
       "      <td>129.57</td>\n",
       "      <td>1.157895</td>\n",
       "      <td>1.473684</td>\n",
       "      <td>1.413421</td>\n",
       "      <td>...</td>\n",
       "      <td>5.751160</td>\n",
       "      <td>1.940299</td>\n",
       "      <td>3.615030</td>\n",
       "      <td>3.768830</td>\n",
       "      <td>3.679915</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>19.0</td>\n",
       "      <td>6.62</td>\n",
       "      <td>0.909293</td>\n",
       "      <td>0.935754</td>\n",
       "      <td>12.34</td>\n",
       "      <td>7.89</td>\n",
       "      <td>130.41</td>\n",
       "      <td>1.078947</td>\n",
       "      <td>1.763158</td>\n",
       "      <td>1.186579</td>\n",
       "      <td>...</td>\n",
       "      <td>3.169215</td>\n",
       "      <td>2.313990</td>\n",
       "      <td>4.880693</td>\n",
       "      <td>3.454633</td>\n",
       "      <td>2.972359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>21.1</td>\n",
       "      <td>6.60</td>\n",
       "      <td>0.895456</td>\n",
       "      <td>1.027018</td>\n",
       "      <td>13.38</td>\n",
       "      <td>9.15</td>\n",
       "      <td>136.71</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>1.710526</td>\n",
       "      <td>1.175526</td>\n",
       "      <td>...</td>\n",
       "      <td>3.373970</td>\n",
       "      <td>3.194548</td>\n",
       "      <td>2.517099</td>\n",
       "      <td>3.028539</td>\n",
       "      <td>3.040646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>18.7</td>\n",
       "      <td>6.86</td>\n",
       "      <td>0.905155</td>\n",
       "      <td>1.314188</td>\n",
       "      <td>9.01</td>\n",
       "      <td>14.66</td>\n",
       "      <td>135.03</td>\n",
       "      <td>1.815789</td>\n",
       "      <td>1.421053</td>\n",
       "      <td>2.006053</td>\n",
       "      <td>...</td>\n",
       "      <td>1.773550</td>\n",
       "      <td>3.223883</td>\n",
       "      <td>1.878514</td>\n",
       "      <td>2.291982</td>\n",
       "      <td>2.473024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>17.1</td>\n",
       "      <td>6.62</td>\n",
       "      <td>1.046406</td>\n",
       "      <td>1.032989</td>\n",
       "      <td>18.00</td>\n",
       "      <td>8.27</td>\n",
       "      <td>130.40</td>\n",
       "      <td>1.210526</td>\n",
       "      <td>1.631579</td>\n",
       "      <td>1.156842</td>\n",
       "      <td>...</td>\n",
       "      <td>6.412100</td>\n",
       "      <td>1.977760</td>\n",
       "      <td>3.684860</td>\n",
       "      <td>4.024907</td>\n",
       "      <td>3.872622</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>18.3</td>\n",
       "      <td>6.61</td>\n",
       "      <td>1.161802</td>\n",
       "      <td>1.066236</td>\n",
       "      <td>16.14</td>\n",
       "      <td>7.60</td>\n",
       "      <td>139.00</td>\n",
       "      <td>1.078947</td>\n",
       "      <td>1.736842</td>\n",
       "      <td>0.928684</td>\n",
       "      <td>...</td>\n",
       "      <td>2.689307</td>\n",
       "      <td>1.743456</td>\n",
       "      <td>1.568175</td>\n",
       "      <td>2.000313</td>\n",
       "      <td>2.572016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>19.1</td>\n",
       "      <td>6.51</td>\n",
       "      <td>1.000858</td>\n",
       "      <td>1.026472</td>\n",
       "      <td>15.99</td>\n",
       "      <td>7.99</td>\n",
       "      <td>135.00</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.920263</td>\n",
       "      <td>...</td>\n",
       "      <td>1.875957</td>\n",
       "      <td>1.742962</td>\n",
       "      <td>3.871643</td>\n",
       "      <td>2.496854</td>\n",
       "      <td>2.555157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>14.1</td>\n",
       "      <td>6.62</td>\n",
       "      <td>1.037986</td>\n",
       "      <td>1.161401</td>\n",
       "      <td>9.73</td>\n",
       "      <td>10.47</td>\n",
       "      <td>151.00</td>\n",
       "      <td>1.236842</td>\n",
       "      <td>1.789474</td>\n",
       "      <td>1.191579</td>\n",
       "      <td>...</td>\n",
       "      <td>1.913804</td>\n",
       "      <td>2.113308</td>\n",
       "      <td>4.904164</td>\n",
       "      <td>2.977092</td>\n",
       "      <td>2.495116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>23.4</td>\n",
       "      <td>6.64</td>\n",
       "      <td>0.865460</td>\n",
       "      <td>0.931256</td>\n",
       "      <td>13.16</td>\n",
       "      <td>9.72</td>\n",
       "      <td>140.80</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>1.447368</td>\n",
       "      <td>1.003421</td>\n",
       "      <td>...</td>\n",
       "      <td>2.267869</td>\n",
       "      <td>4.029070</td>\n",
       "      <td>3.945618</td>\n",
       "      <td>3.414186</td>\n",
       "      <td>2.861741</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      agent_1_feat_AerialsWon  agent_1_feat_Rating  agent_1_feat_XGrealiz  \\\n",
       "1979                     19.8                 6.65               1.041381   \n",
       "1980                     23.1                 6.68               0.819214   \n",
       "1981                     19.0                 6.62               0.909293   \n",
       "1982                     21.1                 6.60               0.895456   \n",
       "1983                     18.7                 6.86               0.905155   \n",
       "...                       ...                  ...                    ...   \n",
       "2465                     17.1                 6.62               1.046406   \n",
       "2466                     18.3                 6.61               1.161802   \n",
       "2467                     19.1                 6.51               1.000858   \n",
       "2468                     14.1                 6.62               1.037986   \n",
       "2469                     23.4                 6.64               0.865460   \n",
       "\n",
       "      agent_1_feat_XGArealiz  agent_1_feat_PPDA  agent_1_feat_OPPDA  \\\n",
       "1979                0.865284              19.00                7.55   \n",
       "1980                1.137980              10.17               10.38   \n",
       "1981                0.935754              12.34                7.89   \n",
       "1982                1.027018              13.38                9.15   \n",
       "1983                1.314188               9.01               14.66   \n",
       "...                      ...                ...                 ...   \n",
       "2465                1.032989              18.00                8.27   \n",
       "2466                1.066236              16.14                7.60   \n",
       "2467                1.026472              15.99                7.99   \n",
       "2468                1.161401               9.73               10.47   \n",
       "2469                0.931256              13.16                9.72   \n",
       "\n",
       "      agent_1_feat_Coach  agent_1_feat_ScoredAv  agent_1_feat_MissedAv  \\\n",
       "1979              129.36               1.000000               1.526316   \n",
       "1980              129.57               1.157895               1.473684   \n",
       "1981              130.41               1.078947               1.763158   \n",
       "1982              136.71               1.052632               1.710526   \n",
       "1983              135.03               1.815789               1.421053   \n",
       "...                  ...                    ...                    ...   \n",
       "2465              130.40               1.210526               1.631579   \n",
       "2466              139.00               1.078947               1.736842   \n",
       "2467              135.00               0.921053               2.000000   \n",
       "2468              151.00               1.236842               1.789474   \n",
       "2469              140.80               0.868421               1.447368   \n",
       "\n",
       "      agent_1_feat_XgAv  ...  agent_2_feattotal_xg_3  agent_2_feattotal_xg_2  \\\n",
       "1979           0.960263  ...                1.450676                2.673176   \n",
       "1980           1.413421  ...                5.751160                1.940299   \n",
       "1981           1.186579  ...                3.169215                2.313990   \n",
       "1982           1.175526  ...                3.373970                3.194548   \n",
       "1983           2.006053  ...                1.773550                3.223883   \n",
       "...                 ...  ...                     ...                     ...   \n",
       "2465           1.156842  ...                6.412100                1.977760   \n",
       "2466           0.928684  ...                2.689307                1.743456   \n",
       "2467           0.920263  ...                1.875957                1.742962   \n",
       "2468           1.191579  ...                1.913804                2.113308   \n",
       "2469           1.003421  ...                2.267869                4.029070   \n",
       "\n",
       "      agent_2_feattotal_xg_1  agent_2_feattotal_xg_mean_3  \\\n",
       "1979                1.792020                     1.971957   \n",
       "1980                3.615030                     3.768830   \n",
       "1981                4.880693                     3.454633   \n",
       "1982                2.517099                     3.028539   \n",
       "1983                1.878514                     2.291982   \n",
       "...                      ...                          ...   \n",
       "2465                3.684860                     4.024907   \n",
       "2466                1.568175                     2.000313   \n",
       "2467                3.871643                     2.496854   \n",
       "2468                4.904164                     2.977092   \n",
       "2469                3.945618                     3.414186   \n",
       "\n",
       "      agent_2_feattotal_xg_mean  agent_2_featboth_scored_3  \\\n",
       "1979                   1.971957                        1.0   \n",
       "1980                   3.679915                        1.0   \n",
       "1981                   2.972359                        0.0   \n",
       "1982                   3.040646                        0.0   \n",
       "1983                   2.473024                        0.0   \n",
       "...                         ...                        ...   \n",
       "2465                   3.872622                        1.0   \n",
       "2466                   2.572016                        0.0   \n",
       "2467                   2.555157                        0.0   \n",
       "2468                   2.495116                        1.0   \n",
       "2469                   2.861741                        1.0   \n",
       "\n",
       "      agent_2_featboth_scored_2  agent_2_featboth_scored_1  \\\n",
       "1979                        1.0                        0.0   \n",
       "1980                        0.0                        1.0   \n",
       "1981                        0.0                        0.0   \n",
       "1982                        0.0                        0.0   \n",
       "1983                        1.0                        0.0   \n",
       "...                         ...                        ...   \n",
       "2465                        0.0                        0.0   \n",
       "2466                        0.0                        0.0   \n",
       "2467                        0.0                        1.0   \n",
       "2468                        0.0                        0.0   \n",
       "2469                        1.0                        0.0   \n",
       "\n",
       "      agent_2_featboth_scored_mean_3  agent_2_featboth_scored_mean  \n",
       "1979                        0.666667                      0.666667  \n",
       "1980                        0.666667                      0.750000  \n",
       "1981                        0.000000                      0.000000  \n",
       "1982                        0.000000                      0.250000  \n",
       "1983                        0.333333                      0.500000  \n",
       "...                              ...                           ...  \n",
       "2465                        0.333333                      0.444444  \n",
       "2466                        0.000000                      0.444444  \n",
       "2467                        0.333333                      0.500000  \n",
       "2468                        0.333333                      0.222222  \n",
       "2469                        0.666667                      0.333333  \n",
       "\n",
       "[433 rows x 185 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_test1 = X_test1[list(fearues_names1)]\n",
    "new_X_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_1_feat_AerialsWon</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "      <th>agent_1_feat_XGrealiz</th>\n",
       "      <th>agent_1_feat_XGArealiz</th>\n",
       "      <th>agent_1_feat_PPDA</th>\n",
       "      <th>agent_1_feat_OPPDA</th>\n",
       "      <th>agent_1_feat_Coach</th>\n",
       "      <th>agent_1_feat_ScoredAv</th>\n",
       "      <th>agent_1_feat_MissedAv</th>\n",
       "      <th>agent_1_feat_XgAv</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_2_feattotal_xg_3</th>\n",
       "      <th>agent_2_feattotal_xg_2</th>\n",
       "      <th>agent_2_feattotal_xg_1</th>\n",
       "      <th>agent_2_feattotal_xg_mean_3</th>\n",
       "      <th>agent_2_feattotal_xg_mean</th>\n",
       "      <th>agent_2_featboth_scored_3</th>\n",
       "      <th>agent_2_featboth_scored_2</th>\n",
       "      <th>agent_2_featboth_scored_1</th>\n",
       "      <th>agent_2_featboth_scored_mean_3</th>\n",
       "      <th>agent_2_featboth_scored_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.2</td>\n",
       "      <td>6.83</td>\n",
       "      <td>0.844742</td>\n",
       "      <td>1.165049</td>\n",
       "      <td>9.19</td>\n",
       "      <td>16.50</td>\n",
       "      <td>144.0</td>\n",
       "      <td>1.526316</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.806842</td>\n",
       "      <td>...</td>\n",
       "      <td>2.661870</td>\n",
       "      <td>1.893116</td>\n",
       "      <td>4.241360</td>\n",
       "      <td>2.932115</td>\n",
       "      <td>2.690442</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.2</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.743218</td>\n",
       "      <td>1.152593</td>\n",
       "      <td>10.31</td>\n",
       "      <td>13.63</td>\n",
       "      <td>146.2</td>\n",
       "      <td>1.052632</td>\n",
       "      <td>1.210526</td>\n",
       "      <td>1.416316</td>\n",
       "      <td>...</td>\n",
       "      <td>3.550724</td>\n",
       "      <td>2.373700</td>\n",
       "      <td>4.197010</td>\n",
       "      <td>3.373811</td>\n",
       "      <td>3.075302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.7</td>\n",
       "      <td>6.73</td>\n",
       "      <td>0.954509</td>\n",
       "      <td>0.956938</td>\n",
       "      <td>14.21</td>\n",
       "      <td>11.82</td>\n",
       "      <td>169.0</td>\n",
       "      <td>1.236842</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>1.295789</td>\n",
       "      <td>...</td>\n",
       "      <td>2.693652</td>\n",
       "      <td>2.042668</td>\n",
       "      <td>0.966665</td>\n",
       "      <td>1.900995</td>\n",
       "      <td>3.007033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.5</td>\n",
       "      <td>6.85</td>\n",
       "      <td>1.155612</td>\n",
       "      <td>1.049618</td>\n",
       "      <td>10.95</td>\n",
       "      <td>12.46</td>\n",
       "      <td>138.2</td>\n",
       "      <td>1.921053</td>\n",
       "      <td>1.157895</td>\n",
       "      <td>1.662368</td>\n",
       "      <td>...</td>\n",
       "      <td>3.938100</td>\n",
       "      <td>1.466409</td>\n",
       "      <td>0.922046</td>\n",
       "      <td>2.108852</td>\n",
       "      <td>2.643923</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.4</td>\n",
       "      <td>6.81</td>\n",
       "      <td>1.199718</td>\n",
       "      <td>0.856327</td>\n",
       "      <td>11.27</td>\n",
       "      <td>11.52</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.789474</td>\n",
       "      <td>1.184211</td>\n",
       "      <td>1.491579</td>\n",
       "      <td>...</td>\n",
       "      <td>3.358338</td>\n",
       "      <td>2.138405</td>\n",
       "      <td>1.872476</td>\n",
       "      <td>2.456406</td>\n",
       "      <td>3.113815</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>16.1</td>\n",
       "      <td>6.72</td>\n",
       "      <td>0.814332</td>\n",
       "      <td>1.292407</td>\n",
       "      <td>13.35</td>\n",
       "      <td>8.32</td>\n",
       "      <td>134.8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1.228000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.369897</td>\n",
       "      <td>4.584170</td>\n",
       "      <td>3.757075</td>\n",
       "      <td>3.570381</td>\n",
       "      <td>3.240205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>14.0</td>\n",
       "      <td>6.69</td>\n",
       "      <td>1.186944</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>14.87</td>\n",
       "      <td>14.67</td>\n",
       "      <td>150.6</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>...</td>\n",
       "      <td>3.465800</td>\n",
       "      <td>2.800194</td>\n",
       "      <td>4.670804</td>\n",
       "      <td>3.645599</td>\n",
       "      <td>3.315186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>15.1</td>\n",
       "      <td>7.05</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.257069</td>\n",
       "      <td>9.05</td>\n",
       "      <td>22.23</td>\n",
       "      <td>190.0</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.736000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.745647</td>\n",
       "      <td>1.865882</td>\n",
       "      <td>3.505180</td>\n",
       "      <td>2.372236</td>\n",
       "      <td>2.343328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>12.7</td>\n",
       "      <td>7.11</td>\n",
       "      <td>0.846805</td>\n",
       "      <td>0.401606</td>\n",
       "      <td>7.86</td>\n",
       "      <td>26.84</td>\n",
       "      <td>189.4</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.598000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.534070</td>\n",
       "      <td>0.828837</td>\n",
       "      <td>3.122840</td>\n",
       "      <td>2.495249</td>\n",
       "      <td>2.364201</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>13.0</td>\n",
       "      <td>6.42</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>1.319510</td>\n",
       "      <td>15.45</td>\n",
       "      <td>9.95</td>\n",
       "      <td>138.6</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.470721</td>\n",
       "      <td>1.833055</td>\n",
       "      <td>2.030735</td>\n",
       "      <td>2.111504</td>\n",
       "      <td>2.811747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.388889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>570 rows × 185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     agent_1_feat_AerialsWon  agent_1_feat_Rating  agent_1_feat_XGrealiz  \\\n",
       "0                       15.2                 6.83               0.844742   \n",
       "1                       14.2                 6.65               0.743218   \n",
       "2                       17.7                 6.73               0.954509   \n",
       "3                       14.5                 6.85               1.155612   \n",
       "4                       16.4                 6.81               1.199718   \n",
       "..                       ...                  ...                    ...   \n",
       "565                     16.1                 6.72               0.814332   \n",
       "566                     14.0                 6.69               1.186944   \n",
       "567                     15.1                 7.05               0.877193   \n",
       "568                     12.7                 7.11               0.846805   \n",
       "569                     13.0                 6.42               0.480769   \n",
       "\n",
       "     agent_1_feat_XGArealiz  agent_1_feat_PPDA  agent_1_feat_OPPDA  \\\n",
       "0                  1.165049               9.19               16.50   \n",
       "1                  1.152593              10.31               13.63   \n",
       "2                  0.956938              14.21               11.82   \n",
       "3                  1.049618              10.95               12.46   \n",
       "4                  0.856327              11.27               11.52   \n",
       "..                      ...                ...                 ...   \n",
       "565                1.292407              13.35                8.32   \n",
       "566                0.769231              14.87               14.67   \n",
       "567                0.257069               9.05               22.23   \n",
       "568                0.401606               7.86               26.84   \n",
       "569                1.319510              15.45                9.95   \n",
       "\n",
       "     agent_1_feat_Coach  agent_1_feat_ScoredAv  agent_1_feat_MissedAv  \\\n",
       "0                 144.0               1.526316               0.947368   \n",
       "1                 146.2               1.052632               1.210526   \n",
       "2                 169.0               1.236842               1.263158   \n",
       "3                 138.2               1.921053               1.157895   \n",
       "4                 170.0               1.789474               1.184211   \n",
       "..                  ...                    ...                    ...   \n",
       "565               134.8               1.000000               1.600000   \n",
       "566               150.6               1.000000               1.500000   \n",
       "567               190.0               2.400000               0.200000   \n",
       "568               189.4               2.200000               0.200000   \n",
       "569               138.6               0.400000               2.800000   \n",
       "\n",
       "     agent_1_feat_XgAv  ...  agent_2_feattotal_xg_3  agent_2_feattotal_xg_2  \\\n",
       "0             1.806842  ...                2.661870                1.893116   \n",
       "1             1.416316  ...                3.550724                2.373700   \n",
       "2             1.295789  ...                2.693652                2.042668   \n",
       "3             1.662368  ...                3.938100                1.466409   \n",
       "4             1.491579  ...                3.358338                2.138405   \n",
       "..                 ...  ...                     ...                     ...   \n",
       "565           1.228000  ...                2.369897                4.584170   \n",
       "566           0.842500  ...                3.465800                2.800194   \n",
       "567           2.736000  ...                1.745647                1.865882   \n",
       "568           2.598000  ...                3.534070                0.828837   \n",
       "569           0.832000  ...                2.470721                1.833055   \n",
       "\n",
       "     agent_2_feattotal_xg_1  agent_2_feattotal_xg_mean_3  \\\n",
       "0                  4.241360                     2.932115   \n",
       "1                  4.197010                     3.373811   \n",
       "2                  0.966665                     1.900995   \n",
       "3                  0.922046                     2.108852   \n",
       "4                  1.872476                     2.456406   \n",
       "..                      ...                          ...   \n",
       "565                3.757075                     3.570381   \n",
       "566                4.670804                     3.645599   \n",
       "567                3.505180                     2.372236   \n",
       "568                3.122840                     2.495249   \n",
       "569                2.030735                     2.111504   \n",
       "\n",
       "     agent_2_feattotal_xg_mean  agent_2_featboth_scored_3  \\\n",
       "0                     2.690442                        1.0   \n",
       "1                     3.075302                        0.0   \n",
       "2                     3.007033                        0.0   \n",
       "3                     2.643923                        1.0   \n",
       "4                     3.113815                        0.0   \n",
       "..                         ...                        ...   \n",
       "565                   3.240205                        0.0   \n",
       "566                   3.315186                        0.0   \n",
       "567                   2.343328                        0.0   \n",
       "568                   2.364201                        1.0   \n",
       "569                   2.811747                        0.0   \n",
       "\n",
       "     agent_2_featboth_scored_2  agent_2_featboth_scored_1  \\\n",
       "0                          0.0                        1.0   \n",
       "1                          1.0                        1.0   \n",
       "2                          1.0                        1.0   \n",
       "3                          0.0                        0.0   \n",
       "4                          0.0                        0.0   \n",
       "..                         ...                        ...   \n",
       "565                        1.0                        0.0   \n",
       "566                        1.0                        0.0   \n",
       "567                        0.0                        1.0   \n",
       "568                        0.0                        1.0   \n",
       "569                        0.0                        1.0   \n",
       "\n",
       "     agent_2_featboth_scored_mean_3  agent_2_featboth_scored_mean  \n",
       "0                          0.666667                      0.333333  \n",
       "1                          0.666667                      0.625000  \n",
       "2                          0.666667                      0.555556  \n",
       "3                          0.333333                      0.444444  \n",
       "4                          0.000000                      0.555556  \n",
       "..                              ...                           ...  \n",
       "565                        0.333333                      0.666667  \n",
       "566                        0.333333                      0.611111  \n",
       "567                        0.333333                      0.277778  \n",
       "568                        0.666667                      0.444444  \n",
       "569                        0.333333                      0.388889  \n",
       "\n",
       "[570 rows x 185 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_df1 = test_df1[list(fearues_names1)]\n",
    "new_test_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train1, new_X_test1, new_test_df1 = scale_data(new_X_train1, new_X_test1, new_test_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1730, 9) (433, 9)\n"
     ]
    }
   ],
   "source": [
    "y_train1 = pd.get_dummies(y_train1)\n",
    "y_test1 = pd.get_dummies(y_test1)\n",
    "y_train1, y_test1 = make_equal_dummies(y_train1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_results1 = grid_search(new_X_train1, y_train1, 'expected_target1')\n",
    "# batch_size1 = grid_results1.best_params_['batch_size']\n",
    "# epochs1 = grid_results1.best_params_['epochs']\n",
    "# learning_rate1 = grid_results1.best_params_['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size1 = 32\n",
    "epochs1 = 100\n",
    "learning_rate1 = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "55/55 [==============================] - 1s 6ms/step - loss: 2.7821 - accuracy: 0.1225 - val_loss: 2.1124 - val_accuracy: 0.1801\n",
      "Epoch 2/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 2.4715 - accuracy: 0.1642 - val_loss: 1.9955 - val_accuracy: 0.2356\n",
      "Epoch 3/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 2.2353 - accuracy: 0.2127 - val_loss: 1.9565 - val_accuracy: 0.2425\n",
      "Epoch 4/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 2.1135 - accuracy: 0.2370 - val_loss: 1.9652 - val_accuracy: 0.2725\n",
      "Epoch 5/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.9694 - accuracy: 0.2717 - val_loss: 1.8392 - val_accuracy: 0.2910\n",
      "Epoch 6/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.9134 - accuracy: 0.2884 - val_loss: 1.7831 - val_accuracy: 0.2933\n",
      "Epoch 7/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.8532 - accuracy: 0.2954 - val_loss: 1.7354 - val_accuracy: 0.2933\n",
      "Epoch 8/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.8195 - accuracy: 0.3052 - val_loss: 1.7104 - val_accuracy: 0.3187\n",
      "Epoch 9/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.7340 - accuracy: 0.3243 - val_loss: 1.6876 - val_accuracy: 0.3326\n",
      "Epoch 10/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.7305 - accuracy: 0.3179 - val_loss: 1.6844 - val_accuracy: 0.3072\n",
      "Epoch 11/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.7102 - accuracy: 0.3254 - val_loss: 1.6607 - val_accuracy: 0.3303\n",
      "Epoch 12/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.6957 - accuracy: 0.3225 - val_loss: 1.7375 - val_accuracy: 0.2656\n",
      "Epoch 13/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.7348 - accuracy: 0.2960 - val_loss: 1.7081 - val_accuracy: 0.2656\n",
      "Epoch 14/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.6960 - accuracy: 0.3075 - val_loss: 1.6574 - val_accuracy: 0.2794\n",
      "Epoch 15/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.6755 - accuracy: 0.3121 - val_loss: 1.6310 - val_accuracy: 0.3233\n",
      "Epoch 16/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.6385 - accuracy: 0.3474 - val_loss: 1.6077 - val_accuracy: 0.3164\n",
      "Epoch 17/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.6123 - accuracy: 0.3254 - val_loss: 1.5895 - val_accuracy: 0.3256\n",
      "Epoch 18/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5881 - accuracy: 0.3584 - val_loss: 1.5797 - val_accuracy: 0.3279\n",
      "Epoch 19/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5954 - accuracy: 0.3445 - val_loss: 1.5659 - val_accuracy: 0.3233\n",
      "Epoch 20/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5830 - accuracy: 0.3376 - val_loss: 1.5547 - val_accuracy: 0.3580\n",
      "Epoch 21/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5791 - accuracy: 0.3422 - val_loss: 1.5561 - val_accuracy: 0.3626\n",
      "Epoch 22/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5749 - accuracy: 0.3503 - val_loss: 1.5482 - val_accuracy: 0.3718\n",
      "Epoch 23/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5628 - accuracy: 0.3434 - val_loss: 1.5428 - val_accuracy: 0.3464\n",
      "Epoch 24/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5599 - accuracy: 0.3584 - val_loss: 1.5698 - val_accuracy: 0.3395\n",
      "Epoch 25/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5781 - accuracy: 0.3318 - val_loss: 1.5502 - val_accuracy: 0.3303\n",
      "Epoch 26/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5695 - accuracy: 0.3376 - val_loss: 1.5249 - val_accuracy: 0.3580\n",
      "Epoch 27/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5727 - accuracy: 0.3509 - val_loss: 1.5161 - val_accuracy: 0.3580\n",
      "Epoch 28/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5752 - accuracy: 0.3277 - val_loss: 1.5163 - val_accuracy: 0.3372\n",
      "Epoch 29/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5438 - accuracy: 0.3584 - val_loss: 1.5196 - val_accuracy: 0.3464\n",
      "Epoch 30/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5533 - accuracy: 0.3520 - val_loss: 1.5226 - val_accuracy: 0.3557\n",
      "Epoch 31/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5416 - accuracy: 0.3451 - val_loss: 1.5207 - val_accuracy: 0.3441\n",
      "Epoch 32/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5320 - accuracy: 0.3555 - val_loss: 1.5147 - val_accuracy: 0.3533\n",
      "Epoch 33/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5387 - accuracy: 0.3382 - val_loss: 1.5105 - val_accuracy: 0.3395\n",
      "Epoch 34/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5324 - accuracy: 0.3434 - val_loss: 1.5068 - val_accuracy: 0.3487\n",
      "Epoch 35/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5197 - accuracy: 0.3434 - val_loss: 1.5007 - val_accuracy: 0.3626\n",
      "Epoch 36/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5241 - accuracy: 0.3561 - val_loss: 1.5020 - val_accuracy: 0.3557\n",
      "Epoch 37/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5254 - accuracy: 0.3578 - val_loss: 1.4995 - val_accuracy: 0.3649\n",
      "Epoch 38/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5102 - accuracy: 0.3642 - val_loss: 1.5009 - val_accuracy: 0.3649\n",
      "Epoch 39/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5188 - accuracy: 0.3595 - val_loss: 1.5060 - val_accuracy: 0.3533\n",
      "Epoch 40/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5067 - accuracy: 0.3503 - val_loss: 1.5092 - val_accuracy: 0.3510\n",
      "Epoch 41/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5054 - accuracy: 0.3520 - val_loss: 1.5038 - val_accuracy: 0.3510\n",
      "Epoch 42/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4976 - accuracy: 0.3607 - val_loss: 1.4980 - val_accuracy: 0.3349\n",
      "Epoch 43/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4952 - accuracy: 0.3422 - val_loss: 1.4980 - val_accuracy: 0.3372\n",
      "Epoch 44/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4934 - accuracy: 0.3601 - val_loss: 1.4976 - val_accuracy: 0.3487\n",
      "Epoch 45/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4954 - accuracy: 0.3613 - val_loss: 1.4969 - val_accuracy: 0.3487\n",
      "Epoch 46/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4775 - accuracy: 0.3751 - val_loss: 1.4991 - val_accuracy: 0.3418\n",
      "Epoch 47/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4833 - accuracy: 0.3514 - val_loss: 1.5008 - val_accuracy: 0.3303\n",
      "Epoch 48/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4798 - accuracy: 0.3671 - val_loss: 1.4977 - val_accuracy: 0.3441\n",
      "Epoch 49/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4694 - accuracy: 0.3682 - val_loss: 1.4949 - val_accuracy: 0.3395\n",
      "Epoch 50/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4814 - accuracy: 0.3671 - val_loss: 1.5094 - val_accuracy: 0.3464\n",
      "Epoch 51/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4760 - accuracy: 0.3699 - val_loss: 1.5093 - val_accuracy: 0.3372\n",
      "Epoch 52/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4668 - accuracy: 0.3636 - val_loss: 1.5076 - val_accuracy: 0.3395\n",
      "Epoch 53/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4711 - accuracy: 0.3665 - val_loss: 1.5010 - val_accuracy: 0.3603\n",
      "Epoch 54/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4532 - accuracy: 0.3740 - val_loss: 1.5027 - val_accuracy: 0.3557\n",
      "Epoch 55/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4691 - accuracy: 0.3584 - val_loss: 1.5042 - val_accuracy: 0.3533\n",
      "Epoch 56/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4732 - accuracy: 0.3607 - val_loss: 1.5018 - val_accuracy: 0.3649\n",
      "Epoch 57/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4743 - accuracy: 0.3653 - val_loss: 1.4933 - val_accuracy: 0.3626\n",
      "Epoch 58/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4705 - accuracy: 0.3671 - val_loss: 1.4944 - val_accuracy: 0.3580\n",
      "Epoch 59/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4530 - accuracy: 0.3676 - val_loss: 1.4944 - val_accuracy: 0.3557\n",
      "Epoch 60/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4717 - accuracy: 0.3803 - val_loss: 1.4917 - val_accuracy: 0.3533\n",
      "Epoch 61/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4789 - accuracy: 0.3520 - val_loss: 1.4880 - val_accuracy: 0.3395\n",
      "Epoch 62/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4700 - accuracy: 0.3879 - val_loss: 1.4880 - val_accuracy: 0.3395\n",
      "Epoch 63/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4633 - accuracy: 0.3763 - val_loss: 1.4931 - val_accuracy: 0.3487\n",
      "Epoch 64/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4587 - accuracy: 0.3850 - val_loss: 1.5051 - val_accuracy: 0.3464\n",
      "Epoch 65/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4792 - accuracy: 0.3740 - val_loss: 1.5152 - val_accuracy: 0.3233\n",
      "Epoch 66/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5004 - accuracy: 0.3590 - val_loss: 1.5010 - val_accuracy: 0.3418\n",
      "Epoch 67/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4955 - accuracy: 0.3613 - val_loss: 1.5004 - val_accuracy: 0.3418\n",
      "Epoch 68/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5127 - accuracy: 0.3543 - val_loss: 1.5393 - val_accuracy: 0.3210\n",
      "Epoch 69/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5010 - accuracy: 0.3497 - val_loss: 1.5132 - val_accuracy: 0.3441\n",
      "Epoch 70/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4826 - accuracy: 0.3653 - val_loss: 1.5053 - val_accuracy: 0.3557\n",
      "Epoch 71/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5014 - accuracy: 0.3526 - val_loss: 1.4988 - val_accuracy: 0.3510\n",
      "Epoch 72/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4791 - accuracy: 0.3566 - val_loss: 1.5016 - val_accuracy: 0.3464\n",
      "Epoch 73/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.5003 - accuracy: 0.3613 - val_loss: 1.5075 - val_accuracy: 0.3626\n",
      "Epoch 74/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4826 - accuracy: 0.3613 - val_loss: 1.5079 - val_accuracy: 0.3580\n",
      "Epoch 75/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4806 - accuracy: 0.3520 - val_loss: 1.5128 - val_accuracy: 0.3510\n",
      "Epoch 76/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4951 - accuracy: 0.3520 - val_loss: 1.5061 - val_accuracy: 0.3557\n",
      "Epoch 77/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4932 - accuracy: 0.3636 - val_loss: 1.5001 - val_accuracy: 0.3649\n",
      "Epoch 78/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4739 - accuracy: 0.3642 - val_loss: 1.5022 - val_accuracy: 0.3695\n",
      "Epoch 79/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4848 - accuracy: 0.3671 - val_loss: 1.4960 - val_accuracy: 0.3695\n",
      "Epoch 80/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4924 - accuracy: 0.3532 - val_loss: 1.4962 - val_accuracy: 0.3788\n",
      "Epoch 81/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4914 - accuracy: 0.3491 - val_loss: 1.4949 - val_accuracy: 0.3880\n",
      "Epoch 82/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4845 - accuracy: 0.3653 - val_loss: 1.4873 - val_accuracy: 0.3764\n",
      "Epoch 83/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4840 - accuracy: 0.3399 - val_loss: 1.4874 - val_accuracy: 0.3811\n",
      "Epoch 84/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4795 - accuracy: 0.3630 - val_loss: 1.4895 - val_accuracy: 0.3741\n",
      "Epoch 85/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4758 - accuracy: 0.3572 - val_loss: 1.4872 - val_accuracy: 0.3788\n",
      "Epoch 86/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4892 - accuracy: 0.3572 - val_loss: 1.4871 - val_accuracy: 0.3695\n",
      "Epoch 87/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4789 - accuracy: 0.3445 - val_loss: 1.4860 - val_accuracy: 0.3788\n",
      "Epoch 88/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4825 - accuracy: 0.3566 - val_loss: 1.4847 - val_accuracy: 0.3972\n",
      "Epoch 89/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4805 - accuracy: 0.3451 - val_loss: 1.4829 - val_accuracy: 0.3857\n",
      "Epoch 90/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4673 - accuracy: 0.3561 - val_loss: 1.4845 - val_accuracy: 0.3903\n",
      "Epoch 91/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4542 - accuracy: 0.3665 - val_loss: 1.4834 - val_accuracy: 0.3903\n",
      "Epoch 92/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4721 - accuracy: 0.3665 - val_loss: 1.4825 - val_accuracy: 0.3811\n",
      "Epoch 93/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4599 - accuracy: 0.3792 - val_loss: 1.4851 - val_accuracy: 0.3834\n",
      "Epoch 94/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4655 - accuracy: 0.3763 - val_loss: 1.4859 - val_accuracy: 0.3903\n",
      "Epoch 95/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4595 - accuracy: 0.3775 - val_loss: 1.4866 - val_accuracy: 0.3718\n",
      "Epoch 96/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4670 - accuracy: 0.3624 - val_loss: 1.4872 - val_accuracy: 0.3695\n",
      "Epoch 97/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4523 - accuracy: 0.3775 - val_loss: 1.4883 - val_accuracy: 0.3764\n",
      "Epoch 98/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4511 - accuracy: 0.3821 - val_loss: 1.4893 - val_accuracy: 0.3695\n",
      "Epoch 99/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4566 - accuracy: 0.3584 - val_loss: 1.4909 - val_accuracy: 0.3626\n",
      "Epoch 100/100\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 1.4608 - accuracy: 0.3578 - val_loss: 1.4911 - val_accuracy: 0.3626\n"
     ]
    }
   ],
   "source": [
    "num_classes = train_df['expected_target1'].nunique()\n",
    "model1 = create_model(batch_size1, epochs1, learning_rate1, num_classes, new_X_train1.shape[1])\n",
    "history1 = model1.fit(new_X_train1, y_train1, \n",
    "                      batch_size = batch_size1, \n",
    "                      epochs = epochs1,\n",
    "                      validation_data = (new_X_test1, y_test1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABabklEQVR4nO2dd3xUVfbAvyeTXggJIZSE3ovSUcEuumABu2JD110bdteybkN33bWt+lvL7rprQd1dRNcFVOyCBSxEpNfQCQHSe5vM/f1x3ySTZEISyCQhOd/PZz4z77573ztvJnnnnXLPFWMMiqIoilKboNYWQFEURWmbqIJQFEVR/KIKQlEURfGLKghFURTFL6ogFEVRFL+oglAURVH8ogpCaTQi8oGIzGruvq2JiOwUkSkBOK4RkYHO57+JyG8a0/cwznOliHx8uHIqyqEQnQfRvhGRQp/NSKAMqHS2bzTG/KvlpWo7iMhO4GfGmE+b+bgGGGSMSW2uviLSF9gBhBhj3M0iqKIcguDWFkAJLMaYaO/nQ90MRSRYbzpKW0H/HtsG6mLqoIjIqSKyV0TuF5H9wCsiEici74lIhojkOJ+TfcYsFZGfOZ+vFZGvReRJp+8OEZl2mH37iciXIlIgIp+KyPMi8kY9cjdGxt+LyDLneB+LSILP/qtFZJeIZInIrw7x/RwnIvtFxOXTdoGIrHE+TxSRb0QkV0TSReQ5EQmt51ivisgffLbvdcbsE5Gf1up7joj8KCL5IrJHROb47P7Sec8VkUIROcH73fqMnyQiK0Qkz3mf1Njvponfc7yIvOJcQ46ILPDZN0NEVjnXsE1EpjrtNdx5IjLH+zuLSF/H1Xa9iOwGPnfa33J+hzznb2SEz/gIEfmz83vmOX9jESLyvojcVut61ojIBf6uVakfVRAdm+5APNAHuAH79/CKs90bKAGeO8T444DNQALwOPCSiMhh9P038D3QBZgDXH2IczZGxiuA64BEIBT4BYCIDAf+6hy/p3O+ZPxgjPkOKAJOr3XcfzufK4G7nOs5ATgDuOUQcuPIMNWR50xgEFA7/lEEXAN0Bs4BbhaR8519JzvvnY0x0caYb2odOx54H/iLc21PAe+LSJda11Dnu/FDQ9/z61iX5QjnWE87MkwEXgPuda7hZGBnPefwxynAMOAnzvYH2O8pEVgJ+LpEnwTGAZOwf8f3AR5gLnCVt5OIjAKSsN+N0hSMMfrqIC/sP+oU5/OpQDkQfoj+o4Ecn+2lWBcVwLVAqs++SMAA3ZvSF3vzcQORPvvfAN5o5DX5k/HXPtu3AB86n38LzPPZF+V8B1PqOfYfgJedzzHYm3efevreCfzPZ9sAA53PrwJ/cD6/DDzq02+wb18/x30GeNr53NfpG+yz/1rga+fz1cD3tcZ/A1zb0HfTlO8Z6IG9Ecf56fd3r7yH+vtztud4f2efa+t/CBk6O31isQqsBBjlp184kION64BVJC8E4n+qvb/UgujYZBhjSr0bIhIpIn93TPZ8rEujs6+bpRb7vR+MMcXOx+gm9u0JZPu0AeypT+BGyrjf53Oxj0w9fY9tjCkCsuo7F9ZauFBEwoALgZXGmF2OHIMdt8t+R44/Yq2JhqghA7Cr1vUdJyJLHNdOHnBTI4/rPfauWm27sE/PXur7bmrQwPfcC/ub5fgZ2gvY1kh5/VH13YiIS0QeddxU+VRbIgnOK9zfuZy/6TeBq0QkCJiJtXiUJqIKomNTO4XtHmAIcJwxphPVLo363EbNQToQLyKRPm29DtH/SGRM9z22c84u9XU2xmzA3mCnUdO9BNZVtQn7lNoJePBwZMBaUL78G1gE9DLGxAJ/8zluQymH+7AuIV96A2mNkKs2h/qe92B/s85+xu0BBtRzzCKs9eilu58+vtd4BTAD64aLxVoZXhkygdJDnGsucCXW9VdsarnjlMahCkLxJQZrtuc6/uzfBfqEzhN5CjBHREJF5ATgvADJ+DZwroic6ASUH6bh/4F/A3dgb5Bv1ZIjHygUkaHAzY2UYT5wrYgMdxRUbfljsE/npY4//wqffRlY107/eo69GBgsIleISLCIXAYMB95rpGy15fD7PRtj0rGxgRecYHaIiHgVyEvAdSJyhogEiUiS8/0ArAIud/qPBy5uhAxlWCsvEmuleWXwYN11T4lIT8faOMGx9nAUggf4M2o9HDaqIBRfngEisE9n3wIfttB5r8QGerOwfv83sTcGfzzDYcpojFkPzMbe9NOxfuq9DQz7DzZw+rkxJtOn/RfYm3cB8A9H5sbI8IFzDZ8Dqc67L7cAD4tIATZmMt9nbDHwCLBMbPbU8bWOnQWci336z8IGbc+tJXdjeYZDf89XAxVYK+ogNgaDMeZ7bBD8aSAP+IJqq+Y32Cf+HOAhalpk/ngNa8GlARscOXz5BbAWWAFkA49R8572GnAMNqalHAY6UU5pc4jIm8AmY0zALRil/SIi1wA3GGNObG1ZjlbUglBaHRGZICIDHJfEVKzfeUEri6UcxTjuu1uAF1tblqMZVRBKW6A7NgWzEJvDf7Mx5sdWlUg5ahGRn2DjNQdo2I2lHAJ1MSmKoih+UQtCURRF8Uu7KdaXkJBg+vbt29piKIqiHFX88MMPmcaYrv72tRsF0bdvX1JSUlpbDEVRlKMKEak9+74KdTEpiqIoflEFoSiKovhFFYSiKIril4AqCBGZKiKbRSRVRB44RL+LnMVCxvu0/dIZt9nJa1YURVFakIAFqZ2ywM9jF0bZC6wQkUVOhUzffjHYYmjf+bQNBy7HLkbSE/hURAYbYypRFEVRWoRAWhATsYvEbDfGlAPzsCUUavN7bJGtUp+2GdiFXcqMMTuwRc0mBlBWRVEUpRaBVBBJ1FwYZS81Fy5BRMZi697XXgqwwbHO+BtEJEVEUjIyMppHakVRFAVoxSC1s9LTU9jSxIeFMeZFY8x4Y8z4rl39zvNQFEVpPjZuhA8+aG0pWoxATpRLo+bKWcnUXNkqBhgJLHXWru8OLBKR6Y0YqyiK0vL88pdWQezfD3FxrS1NwAmkBbECGCQi/ZzVuy7HLqUIgDEmzxiTYIzpa4zpi10MZLoxJsXpd7mIhIlIP2AQ8H0AZVUURTk0xsDXX0N5Ocyf33D/dkDAFIQxxg3cCnwEbATmG2PWi8jDjpVwqLHrsStpbcCuZDVbM5gURWlVNm+GrCz7ee7c1pWlhQhoLSZjzGLsOrm+bb+tp++ptbYfwS6vqCiK0vosW2bff/pTePll2LIFBg9uXZkCjM6kVhRFaQzLlkGXLvDwwxAUBK+/Xr1vxw743e/gV7+yr2eeAY+n/mNVVMALL0B+fsDFPhLaTTVXRVGUgLJsGUyeDElJcOaZ8Npr8NBDkJYGp54Ku3dDcLCNVVRWQt++cP75/o/11FPwwAO27+zZLXgRTUMtCEXpQOTkfEZq6l20+5Uks7PhtNPghx+a53gZGdalNHmy3Z41yyqEd96Bs86CnBx7rooKKC2Ffv3gscesAqjNzp1WsUC126qNogpCUToQBw78m717n6GwsJ0v+f3cc7B0KTz9dPMcz3sj9yqI88+HTp3gssvsDf/dd2HsWLsvOBjuuQe+/dZmPfliDNx6q3VRnXiiKghFUdoOpaV2bZj9+2tm4RQXb2Hr1jvweMpbQ6zmpbgYnn0WROwTfnP4+Zctg9BQGDfObkdEwMyZ9hzz58Mpp9Tsf911kJAAjz9es33BAnj/fWtBXHKJtUL27j1y+QKEKghF6UCUle0G4ODBf+PxVFS1b9t2D2lpfyEzc1F9Q48eXnkFMjOti6ekBN5++8iPuWwZjB8P4eHVbU8/bWdWn3de3f6RkXDbbfDee7B+vW3btQtuvx2OPda+e62RNmxFqIJQlA6CMR5KS3cTGTmciopMsrNtyYiCgpVkZb0HQHr6P1tTxCPH7YY//xmOPx5+8QubhnqkcxZKS218wXtD9xIRAYMG1T9u9myrKObMsS6nwYPtPIq//x1CQmDUKIiKquuG8oMxhn37/k5ubsN9mxNVEIrSDikt3Utx8dYabRUVGRhTRo8ePyMkpCv7978GwK5dvyc4uDNJSXeQk/MxpaW7W0Pk5uHtt23K6f33W/fPNdfAl1/aNrCWxbRpMG9e44+ZkmJnT594YtNk6dIFfvYzK9Mzz8BVV9lA9/HH2/3BwXDccY2yIHbvfowtW25i9+4/Nk2GI0QVhKK0QzZsuIwNGy6t0eaNP0REDKBbtyvJynqXnJwlZGYuIDn5TpKT7wRg//5XWlrc5sEY6/MfPBimO8Uarr7aKorXX4eCAjj7bPjwQ7jxRltPyZf8fOuSqo33CX/SpKbL9Otfw333wZo1eP7xV9JkEWVlPuedPBlWr7ay1cO+ff9gx45fIhJGUdHapstwBKiCUJR2RmnpbvLzl1NUtBHfCjVeyyA8vA/dus3CmHLWr78Il6sTSUm3ExHRl7i4M0hPf5mjsrLNZ5/Bjz/CvffaLCGA3r1tuuvcuXDBBbBypZ2DUFoKd99dPXbTJhgwwAaOa7N4MQwdaoPOTaVrVxsLGTGCAwf+zdats1mxYiQHDzq1nE48ETweij5/rUZMyMvBg2+zZctNxMefQ9++v6WsbC8VFdlNl+MwUQWhKO2MjAwblDWmrMpqACgrs5/DwvoQHT2KqKhjcLtzSE6+nZAQW5m0R4+fUVa2m5ycz1pe8CPlscege3drNfhyzTWwfbtVIC+/DHfdZauy/uc/8OmnNpPorLNsfOD9962S8fL99/DVV/Dznx+xeBkZ8wkLSyYioj8bNlzG2rXTWRv1GCYIDi64lX37XqjR3+MpI/uFq+mafSwjRswnJsZmUBUWrjliWRqLKghFaSRZWYvJzHy3tcVokIMH5xMUFAnY9FUvpaW7cLliCA6ORURISrqNkJBuVa4lgISE8wkOjj/6gtUrV9qb/Z13QlhYzX0XXWT9/s89Z5UF2FnMAwfCzTdb5ZCXB0uWQEwMPPFE9dgnnoDY2CNWEBUV2eTkfEJi4hWMGbOcfv0eISfnU0pC9lM+OIG4DeF1/rYKF/4fQx8qZeh9GbjcLqKijgWgqEgVhKK0KYwxbNlyE9u23Ruwc1RU5LB+/SUUFW067GOUlOykoOA7eva8ydneXLWvtHQ34eF9cNZfoWfPnzNpUjohIV2q+gQFhdGt29VkZi6goiLrsOVocZ54wt7cb7yx7r7oaPjmm5olLcLDbS2k1FSbfvree3Yuw403wptv2qD21q3w3//CLbfYYx8BmZkLMMZNYuKlBAUF06fPg5x0UhETJ64n7PRL6bS+krysL3G7C+2AsjLC73mU8lhwbUuDxx8nNLQ7ISEJakEoLUxFBezb19pStGkKC3+krGwPJSVbqaz0E8hsBnbtepiMjLfZv//Vwz6G172UlDSb4ODONSyIsrJdhIX1rtHfqyx86dbtSoypICvrKFk5bccOO1ntppugc+fGjzvzTJty+vHHcNJJtu2OO8DlsnGKP//ZTo67/fYjFvHgwfmEh/cnOnpsVVvVdz95MkFFFURtryA3d4lte/xxQnfksOdPY+1s7UceQbZtIyrqWLUglBbmnntgxAibQ674pXoCmYeiovXNfvzi4s2kpT0HQE7Ox4d9nIyM+cTEjCcioj8REYMpLq5rQTRETMw4QkK6kp394WHL0aL8+c/2pn7HHU0fe8MN1coBIDkZrrwSXnoJXn3V1lzq3v2IxKuoyCIn51MSEy/1q5A5+WSMy8WIh4Tyfz0HW7diHvkDB06H0HOvssoqLAxmzyY66liKita1WBKBKoiOzsGD8I9/QG4ubNvW2tK0WbKyFhIWZlfBbeoTnDEe9u79Cz/+eDJlZf4ttW3b7iUoKIKkpDsoLPyR8vKDTZaxpGQHBQUr6NrVprdGRg6pcjG53YW43dl1LAh/iAQRH/8TcnI+wphDlKxuLdLSbAzhzjutUnj5ZTvHICmpeY5/77023bW83D48HSEZGf8DKqt+lzokJyOLFyMR0fS882PM6NGY0CC23QLx8VOhZ0945BH4+GO6fFaKx1NCSUnL/K+qgujoPPusTfmD6pIASg1KS3dRWLiKpKTZBAVF+vUBG+OhsHA1u3c/SWrq3WRkLMDtzqOkZAerVp1Oauod5OV9xebNN9SppJqd/QlZWe/Sp8+v6N7dZuDk5HzaJBk9nrIq11TXrjZVMzJyCGVle6msLKoqsdEYCwIgPn4aFRWZFBSkNEmOgJOZCVOmwJNP2if8uXMhPt4qjOZi+HAbi7jllmZZECgjYz4REQOJjh5df6ezziLrsz+x8X4wfZJIu28Y0rMPkZFD7f6bb4Zx44h96G1chS2XyaTrQXRkCgvh+edtFscnn1gFceGFrS1Vm8PrXkpIuICMjP/VsSAyM99j8+brqaiwT/0ioezd+zTgIigoBJEQhgx5icrKAlJT72T//rn06HEtAB5POdu23U14eD+Sku4gKCiU4OAuZGd/TLduV9QrkzEesrM/Jj39HxQUrKCsbC9giIk5joiIvgBERNibW3HxVsrL0wEID2/YggCIizsLELKzP6BTp4mN+6ICTUGBnQW9c6fNODrpJLKy3ic9/RUG940ntDnP9be/HfbQysoiMjMX4PFUYEwFOTmf07v3/f7dSz7Edz2brVMh6pafsWvX7+nW5arqMS4X/O1vyMSJ9HsZikasAS4+bBkbiyqIjsw//2nr2D/0kM3mONosCLcbVqyojp3ExtpCaM1MVtYiIiOHEhk5mOjoY8nI+C/GmKp/3j17HsPliqR//1eIi5tCaGgi+fnfkJ39CRUVB+nT51eEh/fBGA8ZGe+QmnoHcXFTcLuz2LhxFkVF6xgx4h1cLlsILj7+THJyPq5xDvbutU/KkZGkpf2VPXueoLR0ByEhXYmLO4uIiIFERAygc+fTquSOjBwC2EwmtzsXsHMgGkNoaAIxMRPIyvqAvn1/16Tv68CBeURHjyYqamiTxh2S0lKYMcPOUViwAPcJx5K66adVs747dz6V5ORb/Q41xrB//yvEx08jLKxH88lUD7t3P8GuXQ/5tASRmDizwXEREf2IiBjCnj1PUFlZSHz8tJodxo9HZs8m6YXn2HbpF9CveeX2h7qYOioVFTb4dfLJNkd8xIijT0E88YQtf3DyyfY1ahQVy5o386aiIpfc3KV06TIDgKioY3G7sykvt7EEtzufvLxvSEy8kh49riU8PJmgoFA6dz6F/v3/wJAhL1a5dUSCGDr0FYypZPXqKfzwwwTKy/czcuRCuna9oOqccXFnUV6eTlHROtuQnW1n8g4YQPafLiR14y2EhvZk+PB5nHDCXoYPf4N+/ebQvfvVhIcnVx0nImIgYOdClJbuQiS4STfI+PhpFBR836R014qKbDZuvILt25vR5QN4/vgHWLKEzCcvZvOgd1mxYiT798+ld+8HiYgYTFbWwnrHFhdvYvPm69m37/CtgkbL6akgPf1F4uLO5LjjdnDccTuYNGk/0dEjGzW+Sxfr2hMJqaHsq/jDH3DHh9P9oW/tqnUBRhVER+WNN2DPHlsnBqyC2LzZKo62xtq1NpDuS0mJLYB26ql4Pv6Irc8OpjIUcp6+Grc7r9lOnZ39Aca4SUiwCiI62looXh+wTUusJD7+rEYdLyKiPwMGPEFJyWYSEi5kwoR1JCRMr9EnLu5MwCeb6c03oaiIii4hxD/4P47/eQxjiv5IYuJlBAXV71hxuSIJC+tNcfFmSkt3ExaWjIir0dfepcs0wJCd3fisKjsD25Cd/QFu9yHWYfjySzv3oDEUFWGefYrMSbBu9JtkZv6PiIiBjB27nP79HyEh4Xxyc5dSUZFbj0yfOoep+wC0YcNV7NnzTOPkaARZWYsoL08nKek2IiL6EhHRl9DQro0e77UaYmNPIjjYz9yL2Fhyf3c+0ZvK8Vw0wy4+dOutNiYTAFRBdES+/db+UU2YYIuXgVUQFRV2clBb49e/tumIH/qkXb72ms3A+t3v2DN4JWkjt1B81mDiPs5i3Q/nNGquQllZut/6NwCVlSVkZ39CWtqzhIR0o1On4wCIijoGqM5kys7+GJcrmk6djm/05SQl3czxx+9hxIh5hIbWre8THp5MZOTw6hvz3Lm4h/Vm2bN72fl/Ywl1dUXOPse61xrAZjJt8TsHoiFiYsY78RBrlRUVbWDbtnvrzcQCr1ILwpjyQ64t4b5+Jpx3HmmPn8TGjbPYt+/v9fY1L72EK7eE7J+P4cQT85g8+SCjRy+p+k0SEmZgjLvetFyvgigurqkg3O5CDh78N9u23UNOztJ6z98U0tL+SlhYb7p0OfuwxsfGnkx4eD+6dbuq3j5Bl19J+jTgq69tVdp582wMMQCoguhorFtnlUKPHrBoka10CVZBQNtzMxUXV//xz55tLYfKSvvENGECReMT2Lnzd3TtejHRs/+PkAII+XAZGzZcisdT/7yOkpKdfPddf9LSnq2zb9u2e/n66zjWrDmLgoIUeve+DxH7rxISEkdYWK8qCyIn52M6dz7tkE/y/qhyBaWl2bjJxzWf0uPjzyIv70uKf1wM333HrlN20yn2BHrN/gr56mtbBG7aNLtgzSHwzoUoLd3V6AwmLyIu4uN/Qnb2h2zadD0rVhzDnj1PsmXLzX77G2OtjYSE6YSF9SIjY77ffgdX/oXg1H24o4SeD36NeW8BW7bMrlE3qoqKCsyTfyJvJHSaehfBwZ3qdOnU6ThCQrr6dTN5PG5yc5cCQRQXb8XjKavaZyujGoKCwti48QrKyzMa87XUS3HxZnJzP6NnzxubZKn54nKFc/zx2+nR47p6+0RFj2LzfZC+5k82qyszEz766HDFPiSqINoh2dkfkZf3bd0dO3bYjKXwcHvT9Z0ANHSorYDpoyAqKrLZu/e5ugvcG2Nr5C9ZYl/Ll/v1h6anv1xnTYIm88knVin86le24Nof/wj/+x+kpuK59x42brqW4OBYBg16ATnzTOjZk/7LRpKV9R4HDrxW72F3734Uj6e0Tjqp253Pnj1P0bnzqRxzzGJOPDGHXr3urtEnOnoURUVrKCnZTklJqpPxc5h8+ql1oV1wgbXsHOLizsLjKSXjz+dggiB41q0ce+yHuFyRVrl/8oldT+DMM21WTz1ERg6hsjKfsrI9TbYgwJvumsGBA2+QnHwHffr8mqysRWRm1r0ZW0tlN/HxU+na9RKysz+q4/bJzHyPrPl3AiDvf4KMHsew35bReS3s3ftMXQHeeougPfvZc0UYXbv6z7ATcdGly3lkZS2us2RqQUEKlZX5jhuvssbM8sLCVQAMH/4mFRXZbNo064jmfezb9zdEQujR4/rDPkZjCAtLJji4MwcPzuPAgX+Rl/ct5eWZATmXKoh2yJYtt7B1q5+nvF/9CoqK7NNqv1opEBER0L9/DQWRlvYcqam3UVi4umbflBTrnjr9dPuaPBneeqtGF7c7n82br2fduvNrPLU1mYULbXbS735nJ0M99hjmwQeo6JvAiqTfUFj4A4MGvWD9vC4XXHUV4Us20bl8BHv2POH3H760dA/797+MSDD5+d/U6JOf/w3goVeve+jSZRouV1Sd8VFRx1JcvKlqFbbGxh/8kpJiVxXr0cNadutsYLpz51OIjhxLz8+j8Jx5Gn2Of7amT3rAAPs7FhbCMcfYTDQ/awp4M5mg8XMgfElMvISBA/+PiRM3M3DgU/Tp81uiokaydett1XWDHLKz7VNsXNxZJCZeijHlZGVVu5lyc79iw4ZLSFwTh+magOuk0+CDD5A+fRk5J5iDm16sWcraGMzjj1LUNwjX9Mv8/hZeEhJmUFmZT27uFzXavQ8ASUm3ATXjEIWFqwkO7kyXLucycOBTZGd/wI8/nsjq1VNZvXoq+/bVinth62WVlGyv015ZWcz+/a+SkHAhoaHd6pWzORAREhIuIC/vSzZuvIoffzyBNWuO4G/wEKiCaGd4PO6qiV11TPbNm+3NfGQ9GRW1Mpm8Pt3i4lpujNWOwpg/31oQ0dF1VsXyzvQsLt7Azp1zbOmCH36oc8r8/O8pKFjlX57KShvIPPtsu0Tjk0/iiQxFtm5j+wWZhIQnMnz4myQm+uSDX3MN4nYz4LuxNW7ivuze/SgAffr8Drc7l6KiDVX7cnO/Alx06nSCf5mwgWpj3KSlPUtYWB8iIg6x7GRDrFgB48ZZiyA83Fp4336LyxXJ+MInCEkvwnXtDf7HHnusHT91ql3WcuBAW4DOJ9HAOxcCDk9BBAWFkZx8e9XciqCgEAYP/htlZXvYtevhGn2zsz+26bZvfkHM5iDCwnpXrXuQk7OENWumERbam/hVIcgZU6zF2rUrvPkmrjw3fV8sJi3Np+T1Bx8gq9ey5zIP3Xtee0g54+KmEBQUUceyyc39jOjo0cTGTgZcNeIQhYWriIoahYjQs+fN9Op1P8Z4nL+JtWzf/ss6MaotW27khx8m1nnoOXhwPm53LklJ/t1vzc3QoS9z0knFTJiwnpEjF9G378MNDzoMVEG0M+yEKevuqRMk3LkT+hziJjFihA1Sl5dTUZFNfv53gB8FsXmzrQ1z4YVw6qkwdqx9EvbBqyA6dTqBzC8es0sv1ipb4PGUs3bteWzeXI9J/s03kJFh898BunVj94N9yT0unKQHVzB27NckJtYqXzBiBIwbR/T/1hIW1ofdux+vsbusLI309H/Svft1JCZeDkB+frVyy8v7ipiYMQQHR9f7NXnLLpeUpBIff1aDE6DqpbwcVq2y1li/ftYiMAZOOAEuvtjWGIqNrb5+fwwaZK23b7+1bsLZs+1M4PnzwRjCw3sTFGTnVxyOi8kfsbGT6d79evbseYqCgpWAncmdm7uExMLj4LrrkDPOIPngKeTkfMyBA/9i7dqzCQ/vy5jwF5H0A3DGGdUHHDUKueMOer4L+R/92SYYrF8PV19NWe9Ics/uRefOpxxSJpcrkri4s8jKWlTlEq2sLCYvb7mjPMKIiBhYZUEYU0lR0dqq2c0iwoABjzJu3LeMG/ctgwe/gNudVV08D5vynJm5CLc7q05p7vT0fxARMYTY2JOP9OttNC5XBFFRw0lIOI+EhHMDco6AKggRmSoim0UkVUTqJEaLyE0islZEVonI1yIy3GnvKyIlTvsqEQl8AnM7obTUa/66apj3FBTYfPq+fesf7C3Yt2WLk0HjQSSkroLYtMnemFxOIG7CBHuj83lyLSlJdQ75Fn3fjrCNX3xRvTYwkJn5P/o9epDEp1b6T01duNBaDtNs6l95+QF2TtxIzn/uJTphfP3XMWsW8uMqRrw1jKIDy8jLq1YAVmF46N37l0REDCAkJJG8PLukpMdTRn7+d8TGnlTPgS0REYMQsWsOHFH8Yf16KCuD8c61jBxpFfRDD9mg4+LFcOml1v3XEMcdB0uXWosrPNxWAL36akSCqiycxs6ibgwDBjxGaGh31q49h5KS7eTlfYPHU0y3j9w28aFTJ5J+9h7huyvYuPEqIiNHMHr0UkK/cmahT5lS84Bz5uDp2ZX+j+eS8c3jcNZZmLAQfny0hMTka6qSBA5FQsIMysr2kJNjkxry8r7GmHI6d7bKKCpqeJWCKCnZhsdTTHT0KL/Hiov7CS5XTPXKb9i/V2PKCAqKqBHfKiraSH7+cnr0+NnhPyy0UQKmIMSG8Z8HpgHDgZleBeDDv40xxxhjRgOPA0/57NtmjBntvG4KlJztjdJSewPu2vWimrnhuxx3U0MWBMD69WRnf0hwcDxxcWfVcMEA1oIYWj1LtnLMCDvT1fGfg1UQISHdCMswdP24jAzveu9vvFHVJ2vpE/R8D3ouhLyD1U9qgH2SXrjQLhfZyWauZGS8A3jqL3rm5ac/hYsuotPTH3LcVULRYzdT8u4/2Pvq+RQtfoFu8VcREdEXESE29sQqBVJQkIIxZQ0qiKCgYKKiRgBBxMWdfmhZDoU3TXXChOq26Gj47W9t4cQ//cl+biwicM45VlnfcINNf8zOJjJyCCEhXW2Au5kICenCqFEf4/GUs3r1mRw48BqCi4i3v7XWweefIxLM6PtC6Fp6PKNHf25Tej/91Ma6aj+oxMQgz/6N6O3Q9cw5VBTsI+WRDEp7GLp3v6ZRMnXteiEREYNZt+4CsrM/JSfnU2fCmf09o6JGUFKSisdTVhWgrk9BuFzhJCTMIDPznSo308GD/yY8fABJSbeRnf1BVUHF9PSXEAlutJxHE4G0ICYCqcaY7caYcmAeUMNWNsb4zqSJAmqlyyhNpaRkB+AiKWm2kxvuzCz2Kgg/FkRe3jK2bbsfM3gQBAVh1q0jO/tD4uPPIipqJCUlW6tTRsvLbTbREBv8LC5OJUWcRVp83EylpdvsTN5nnkEM5M+5lJwxgmfuP8EYiorWE/dPG5MILoayj/5VU6jNm+3TtI97JSNjPpGRw5yb8yGIioK334bly/H070XPP60lYvoNJF+3kNF3uhn4RfVzSmzsZEpLd1BWlu7EHyA29sT6jlxFYuKldOt2FSEh8Q32rZeUFIiLszfMuiewBeiSk+vuawiXyyrJykpYvJi+fR9i2LDXD1/OeoiKGs6xxy6mvPwA+/e/Qo9tI5Adu+yqbYMHIx9+SGhxBMPvyiU4t9xap0uX1rUeHOSCC3CffSoiIRx8+Wq6nHI/Q4b8k8jIxhXMCw6OZcyYL4mIGMDatedw4MDrdOo0qSq4HRk5AvBQXLyZwsLViAQTGVn7mbWarl0vxe3OISfnM8rK0snJ+Zxu3a6ge/drMMbNwYP/weMp58CB1+jSZTqhoYlN/QrbPIFUEEnAHp/tvU5bDURktohsw1oQvitz9BORH0XkCxHx+0gnIjeISIqIpGRkHFkOc3uhtHQH4eG9iI2dTEhIYnXQzpsKWcuC8Hgq2LhxFnv2PE56zuswcCDuNcupqDhAfPw0oqKGYUwFpaVOeeFt2+yNx7EgMjLmU9KjgsrY8BoTt0pKUokqT7YLslx2Gb1PfoGMaZEEbduNWfY1B1OeIPFzqPzZNVSGBxG8+MuaF7LQkXu6nWVcVraf3Nwv6Nq1npr6/jjhBFzLVpL61ukcePsWKj5/F/r0Ifiz5VVdbPDSKsm8vK+IjBxK6NwFdolJ7+vzz+scunfv+xk2bG7j5KiPFSuseykQbokJE2xm1MKFREUNJz7+J3X7bNlig9vp6Yd9mk6djmPkyP8hEkrPTyOscvYWfBw7Flm0CNm50yYaLF0K+fk14w++iBD8v48I2rGHpItfo3//PzY5ZTQ0tBujRy8lOnoM5eX7iYurPpf3waKoaD2FhauJjBxaVf/KH/HxZ+FydSIjYz4HD74JeEhMnElU1Aiio8exf/9csrLepaIiI+Cpra2GMSYgL2ypwX/6bF8NPHeI/lcAc53PYUAX5/M4rKLpdKjzjRs3znQ09ux5xuTnr6zR9sMPJ5gffzzNGGPMxo3Xmy+/7GQqK8uM+cUvjAkLM6ayskb/tLS/mSVLMMuX9zZffdXZVM4425T3TzBLlmDKyvabvLzvzJIlmIMH/2cHvPOOMWDM998bY4xZsWKcWbIEk398F2PGjDHGGON2F5slSzDZ906xfVetMsYYk771BeMOxxRccYLZc2mo8bjEmF27TOFZQ0xpAqaiPM84BzBmwABjTjihSs69e58zS5ZgCgvXHdmX9vOfGxMba0xFhTHGmMrKMvPFF+Fmy5bbzZdfxpqty64yJjjY9unZ05iYGPtKSzuy89amuNgYl8uYBx9s3uP6csMNxkRHG1Na6n//1Kn294mMNOY3vzEmL++wT1WRn248MTHGzJpVd+eiRfZao6Pt+TIyDvs8jZanIt/s3PmIKSurPldlZalZssRltm37lVm2LMmsX39lg8fZsOEa89VXnc2KFaPNihVjqtr37PmLWbIE8913Q82yZUnG43EH5DpaAiDF1HNfDaQFkQb08tlOdtrqYx5wPoAxpswYk+V8/gHYBhx5YfZ2RGVlKampd5KW9pca7daCsC6L6tzwpdbF1KePTS2sOkYJO3c+TKdOkzj22I+orCwiq+cegndl0tl9LKGh3arq0VcFqjc7K5QNGeKk0/6ASCh5g0vthK/SUkpLdxBUDp1edVIwR1k/b7cBN5J3RiLhC7+hx6JyKi46C3r3xkw/j7BMKPrKCfy98461VHyyng4enE9k5IiG3UsNMWWKXaDeSbkNCgolJmYiBw68QWVlHomfeqwrZNkyO8t55UrrVrv77gYO7Ie337ZpxXl+AvCrV1tLzDf+0NzMmGHnSSxZUnffmjW2dMns2XDeefD739sgeWFh3b6NIPi9JUhBgV2BrTbnnQevvGKPPXo0JNQtL9LcBAfH0KfPgzVKmQQFhREZOYi8vC8pL0879PoMDomJl+J251JYuIrExCt82mciEkxx8SZ69LjusGdOt3UCqSBWAINEpJ+IhAKXAzXyLkXEN4H8HGCr097VCXIjIv2BQUDd2SkdmLIy673zphmCTesrL99PRISdBGfT+yLJzFzgN8U1Le05ysv30b//n4iKGkrv3vezY+JaDND/H9btERzcidDQpJoKokcP6NTJWSkLevT4GbkDi+yNddUqSkpS6fYRuDLyqosBYquZRtz0R4KLwFUKIb+yKagRF9+KCQLP/+bZ4PRjj9ksqfPPd651H3l5X9VNaT0cTnMqZH72WVVTbOxk3G47QSv6ndV2XoI3YD9wIDz4oC2YV6scRhXFxbYcty+ZmXaN5OXL4cUX647xuuPGHyIb60g5/XTr8lm4sO6+J56w+37/exvM9hZvXLmybt/GMHcu9O4Np9STjnr11TbD6gjWWWgOIiNHkJdnY031Bah9iYs7E5crFpCqtGiw5dDj488BoHv3nwZE1rZAwBSEMcYN3Ap8BGwE5htj1ovIwyLiLV95q4isF5FVwN2A9/HjZGCN0/42cJMxJhulCu8kuKKi9VRWljptOwEID7cKwuWKICHhAvbvfxWza3uNALXbncfu3Y8SHz+Vzp1t7nbv3g/iGdqfvZdCp/mr4Wub/hkVNaxaQWzaVBWgzsz8L1FRx9K160UUeJOaUlIoKdxCr/ngGT/GzpPwIWLatVT27Y576kmIs3aDq1sfCkfFEPbRj/Zp94cfyL/hZFat/QmrVk1hzRpbVdS7UtoR0bWrfYr9tLrEhjcOEbe3G0Gr19d9Cr7/fruy2C232LIftbn9dqtIli6tOSYvz85yfvppm87qS0qKLXXSXMtk+iM83FpwixaBx2dG+a5d8J//2EynuDjbdrqTjeWdBNkUCgrsRL8rrqhhodbhnHNsOm4r4muBNkZBBAWFkpx8J927X1ujlDrAwIF/ZvjwN6seyNojAZ0HYYxZbIwZbIwZYIx5xGn7rTFmkfP5DmPMCGNTWU8zxqx32v/r0z7WGPPuoc7TESkr886SrnSKjlWnuHoVBED//n8iqFSQg1mY3jYP3hjD9u0P4nZn06/fH6v6ulwRDBv2GsX3XIHp08c+AVdUEBk5jOLiTRiPpyrFtaxsP3l5y+ja9UKio0dRlgCVXaNhxQpc735K5F4Iuv/BugFYlwvXirUEv7W45vX8ZDyRW4sxd9+JJzGOVce+QlnZLjyeUlyuGHr2vJmoqGHN8+WdcYZ1IRUXA9Cp0yQAkj+Ls/MuZtZa3CUszM5Q3rbNWje+uN22NlRZmQ2or1wJX31l10m+5x472S09Hf5VK0srkAFqX6ZPh337as5if/ppe9677qpu697dKs9Vq2qOLyiA226zWVE//alVkgdrrZe9Zo1VQJMnB+wymguvgggN7d7orKN+/eYwdOjLddojIgY0j1XbhtGZ1EcpvmU0CgutW8CmuNZUEOHhvRgQYlfaKuhiC3rt3v0o+/a9QHLyncTEjKlx3NjYyQwd9y/k2WftRK6nnyYychiVlYWU7V1tV6AbMsSpnGlISLiQkJAuhIYlUTw8FlasoPPfv6G0V5gtQOePhASb7++D68IrAZDVa9l9fgkRccMZN24lY8d+zdixXzN48Av+jnR4TJli4wpOeZCQkDgG9/8bcYsz7FOuPx/5GWfY63n22ZrWwNdf2wmIf/mLfRqfOtXOGu/TB37zG3uuMWPg8cern+ILCqwlFsj4g5dzzrFpr143U1aWXVvjiiugl0+IUMRaVrUtiEWL4Lnn7MS9Tz6Bv/61Tt2tqjGjGn4ib228CqIx8QdFFcRRS2npLqeqY1xVHKK0dAdBQRF1ioV1K7FZwruD/s2ePU+zY8eDJCZewYABf67/BOedZ2MAc+YQlW+ftMrXOumeQ4eSkfEOERGDiIqydZ2io0eRP6QCNm4kcn0+2T89pnqmdSOIGXMJRX3AHQHp50cwcuQi/wumNAcnnWQtBR83U8/1vQk6mGVz+OvjhhusMnj//eq2hQuthXHdddVlybdssTfVqCh7473vPmt5vfuuHf/AAzbW0hIKoksXOPFEO+kuKsq6tIqL4d576/YdNcpOdvQu4QpWicbEwO7d9pWYWHcdilWrrHI8nDkbLUxExGBcrk5NWr+jI6NrUh+lWAXRh6CgMAoLf3TadhAe3q/OPIGgPTZ5rCA+g8xtdxMffzZDh77acPmCJ5+ERYuIeXkpnAPudd8DUNE/kdz0z0lOvqfqXNHRo8nu/yFJQHkclF9WT657PQQHdyL1t0Mpz9zC0OPfCaxfNyrK1jvyURC8+qq9mZ5zTv3jpkyxrpi5c22uv3e295Qp1iIaPNiulPb993CuT22ciy+GX/4S7rwTcnNtbOL6622p7pbgqadszMHL0KH+CzaOGmWto82bq4P0y5bZ78qr7MePr1N3i9WrrfVxFJSZCAoKYcKEtYSEtL9JbYFAFcRRSlnZLjp1mkRYWE/27n0Wj6eCkpLtNdxLVezcCSEhdBv9C4pKNzB8+L8JCgpp+CQDBsDFFxP04muETYmDzZsw4eFsLXscYwzdul1Z1TU6ehTpQz2YsBD2XFJBVFzT4wU9L56L251LXNypTR7bZKZMsSXEv/jCZvS8/771yYceYuGf4GBbcvyZZ2wRwf37bW2pX/6yus/QoTXKkFSNe+ABG9M591y7psUxxwTksvwydqx9NcTo0fZ91SqrIPLybOryRRdV95kwwabHFhZapVhZafvceGMgJA8IzVmTqr2jLqajEGMqKSvbS3h4H6Kjx2JMGcXFGygt3eH/yXvnTujVi/6D/sgxxyxoWk2e++5D8vPp9UEsrq17cffrwsGsN+nX7yGio6tvctHRo6iIhS0fTGXP5RAePqDJ19Wp08QjW1uhKUyZYi2AU0+1cYTHHrM37oaYNcu6YP7zH2s9iFh3XEPccIN10bz7bssqh6YwZIhVkN6Ywrff2u/IN/g8YYKNpXjTYbdutZldR0H8QWk6akEchZSV7cMYt6MgbJA5J+czKivz/VsQu3YduorroRg3Ds44g27/+YbKoBJyh+YRFzeF3r1rFueNiBhoq1zyOXjsdptmwgR7Yx882M5ziG9kTaWRI+3T+Ny5NqXzuONqrsxXHyI1g8JtkZAQazl4FcTXX1vXkm9qqnfeRkoKnHxydV+v9aG0K9SCaAnmzbM3JD/Lch4O3gymsLA+REYOIigoioyM/wLU72I6XAUBcN99hBwsJny/obRPGEOHvl5n5qiIi6ioY/F4iggKigr4qlpHTHCwzdB58snGKwcvs2bZJ+iUlKpaUe2G0aOrU12XLbOWgW/GWbduVtF5A9WrV9vvclgzpSArbQpVEC3B3/9ubybba00G93jq5p03guoJcX0QcREdPZr8fFuALiKiVmXQsjKbh3+oMt8NceaZVB5jJ73HnXAbYWH+n5i9E48iIga2u7r4NZg5094U4dCL+RyNjBpl5zns2QPffWczoGozYUJ1oHrVKqscwsJaVEylZVAFEWiys+3EKaixnCdga/WMGVNXcTSAd5KcN9gWE1MdgKxjQezebd+PxIIQwfXbP2JEiD7ZT60dh2oF0fT4w1FF1652TsTIke3vydnrKnrtNZsO62/y2/jxkJpq58R4M5iUdokqiEDz/vvVrqXaCuJ7mzbKvn1NOmRp6S5CQhKq6tx74xDBwfEEFxlbX2e+sxJWYxYKagwXX4ykpx/yhuhrQbR75s61Pvr2Zik55U+qaib5UxDe+RsffGD/djVA3W7RIHWgWbjQFrcLCamrILzupZycJh3SOwfCS0zMWKQc+iyKhBn9rdXivXHlO2syHYkF4aXboeMK0dGjiYwcRufOpx35udo6ERGNWwr0aCMuzhbd273bPlT4qxU1bpx9f+kl+64Kot2iFkQgKS21OePTp1t3hK+CMAbPKlsfp3DP10087C7Cw/tWbUeGD2XMHUKvp/faf96vv7ZPflddZWsAuVyBLQrn4HJFMXHiBrp0mRrwcykBxOsyqq+2UlycLU7oXUhJFUS7RRVEIPn8cygqIvukaPbFLcds2lRVxsCk7SUoKxeA9I2Ps2XLLbjdDdfiNytXUp6/i/Dwagsi6L0P6LTJUPzk3bYk9eTJNt9++HBbYTQ5uTqoqigN4b3hH6r4ntfN1LOnjcko7RJVEIFk4UKIjiZ92FbyknOR8nJMaioAuV9UL/QTH3QC+/b9jZSU0VRUZNV/vMJCOO44Bj1RWq0gvOsn9OtH5B0+lUY7d7bWy8CB/ssqKEp9nHSStTrrWxoUqhWEWg/tGlUQgcLjgUWLMFOnklf6PWUDbS553vK/U1lZTMFXdhEZExpKF5nIiBH/pbR0Gzk5flb/8pKWhrjddP8EYlKc2MLXX9sZr/fcU9dK6N7dxjnefDMAF6i0W84805YRcdb98It3wpxmMLVrVEEEihUrYP9+KqZNorx8P11P/h0Aecv/xrZt9xK+OZ/KPj2Q7t0hJ6eqxETVwjz+2L8fAE8IRN/3kp3j8Pjjtjz1ddf5HxMVZV+K0hQaWhbUOxP94otbRh6lVVAFESjefx9cLnIn2Ztzpx5n4OmbTMSOcvbte4HYXTG4xhxnA345ObhcUYSF9Tm0gkhPB2D7z8G1dSf8/Od2GcfbboPIJtRXUpQjJTzczkRvTBFA5ahFI5eBYuVKGDaMPNdagoKiiIo6hqCRo4nb6iG80k3ozgyYNdqWf3bSXGss7ekPR0FknBPNgIxpyOuvW8Uwe3bgr0dRlA6HWhCBYv16GDGCvLxv6NRpIkFBwTBiBCHbMzgudD5ijA3wORYEQGTkcIqLN2OMx/8x09PxhAYRnNAHeeYZu37BbbfZd0VRlGZGFUQgKCyEnTvxDBtEYeEqOnU6wbaPGAEVFcg7C+x2HQUxDI+npMZyojVIT6eiSzBh4X1teuGuXXalMEVRlACgCiIQbLRuopL+oUAlsbGTbLt3la5586BTJzu72UdBREXZMhbFxRv8Hzc9nbIuldUprt4lLRVFUQKAKohA4MyYzk3KBahe/3boUHtD37/fWg8iVkGUlEBZGZGRVkEUFfmPQ7j3bKIsrpLIyEOkHyqKojQTqiACwfr1EBpKdtxmIiIGExLixAgiI6G/U47bO8EoLs6+5+QQEhJPSEii30B1VtaHmPQ06NGTHj1+3gIXoShKR0cVRCBYvx4zdCj5xd9Vu5e8eGc1eycYeRVEbi5g4xC1FURe3nI2rLyAkAKIH/lTXK52WCROUZQ2hyqIQLB+PZVDelNRkVkdoPbijUP4sSCgOtXVGANARUUWa9eeS3ShXaTHleRnxThFUZQAoAqiuSkogN27KelvV9iqoyAuuQQuvbS67n4tBREZORy3O5fy8gMAHDjwb9zuHAbHzLH9evQI9BUoiqIAqiCanw02Ayk36QDBwZ2Jihpec//o0bY2Umio3e7c2b77pLpCdcmNAwdeIzp6DFH5MbafKghFUVqIgCoIEZkqIptFJFVEHvCz/yYRWSsiq0TkaxEZ7rPvl864zSLyk0DK2aw4GUzp8d/Qvfu1iLgO3d+PiwlsqmtR0QYKClLo1u2aqlnUdPe/HrSiKEpzEzAFIfbO+DwwDRgOzPRVAA7/NsYcY4wZDTwOPOWMHQ5cDowApgIvSIN32jbC+vV4woIp7l5JUtJtDfevpSBCQ3vicsVQVLSR/fvnAi66dbvCKoigIK29ryhKixFIC2IikGqM2W6MKQfmATN8Oxhj8n02owDjfJ4BzDPGlBljdgCpzvHaPGbdWop7G7okziAion/DA0JC7IQ3R0GICJGRwygqWseBA2/Qpcs0QkMTrYLo1s3W6VcURWkBAqkgkoA9Ptt7nbYaiMhsEdmGtSBub+LYG0QkRURSMjIymk3wI8GzNoXCvpUkJ9/R+EE+s6nBxiHy8r6kvHwf3brNso3p6Rp/UBSlRWn1ILUx5nljzADgfuDXTRz7ojFmvDFmfNc24Hoxubm40nOoGNydzp1PbfzAWgrCxiEMwcGd6dLlXNuoCkJRlBYmkAoiDejls53stNXHPOD8wxzbJij47g0AoiZcgjSlRlIdC8KGahITL8flCreNqiAURWlhAqkgVgCDRKSfiIRig86LfDuIyCCfzXOArc7nRcDlIhImIv2AQcD3AZT1iDHGkLvsWQBiJ93ctMG1FERs7CRiY0+sDnK73XDwoCoIRVFalIAtGGSMcYvIrcBHgAt42RizXkQeBlKMMYuAW0VkClAB5ACznLHrRWQ+sAFwA7ONMZWBkvWwycmBJ5+EH36gvGwfXbdswRMRimtAE4vpxcVVldoACAnpwpgxX1XvP3gQjFEFoShKixLQFeWMMYuBxbXafuvzud5IrjHmEeCRwEl3BJSWwnPPwR//CLm5eMaMorx0IxIfhVx9q01HbQq1LIg6eOdAqIJQFKUF0SVHD4fLL4eFC2HqVHj0UVIj/sa+fWsYN+5rJGZ0048XF2cXGaqosGmvtdm/377rJDlFUVqQVs9iOuooK4MPP4RbboEPPiC/Xyn79v2d5OQ7iDkc5QB1KrrWQS0IRVFaAVUQTeWHH6ySmDIFgJ075xAa2p2+fR86/GPWqsdUBy2zoShKK9CgghCR80REFYmXZcvs+6RJeDxl5OZ+QdeulxAcHHP4x6xVbqMO6ekQHw9hYYd/DkVRlCbSmBv/ZcBWEXlcRIYGWqA2z7JlMHAgdOtGfv73eDwldO582pEdszEKQt1LiqK0MA0qCGPMVcAYYBvwqoh845S4OIJH5qMUY6yCmDwZgNzczwGhc+dTjuy4qiAURWmDNMp15BTVexs727kHcAGwUkQaUa60HbFlC2RmVimInJwlREePISQk7siOqwpCUZQ2SGNiENNF5H/AUiAEmGiMmQaMAu4JrHhtDG/84cQTqawsIT//myN3L8GhFYQxNs1VFYSiKC1MY+ZBXAQ8bYz50rfRGFMsItcHRqw2yrJlNlg8ZAj5eUsxppy4uGZQEGFhEBHhP801OxvKy1VBKIrS4jRGQcwB0r0bIhIBdDPG7DTGfBYowdoky5bBpEkQFEROzueAi9jYk5rn2PXNpv76a/s+cmTznEdRFKWRNCYG8Rbg8dmudNo6FhkZsHmzT4B6CTEx4wkO7tQ8x69PQSxcCLGxcMoRBsIVRVGaSGMURLCzIhwAzufQwInURlm+3L5PnozbXUhBwffN417y4k9BVFbCe+/B2Wf7L8GhKIoSQBqjIDJEZLp3Q0RmAJmBE6mNsmwZhIbChAnk5y/DGHfzBKi9+FMQ335rLZfp0/2PURRFCSCNiUHcBPxLRJ4DBLsU6DUBlaotsmwZjBsH4eHkpC1BJITY2MnNd/y4OFizpmbbwoXWcpg2rfnOoyiK0kgaVBDGmG3A8SIS7WwXBlyqtoYxsG4dXH01AHl5XxMTMwGXK6r5ztG5c10LYuFCOPVUG4NQFEVpYRpV7ltEzgFGAOHepTSNMQ8HUK62RUYG5OfDILsAXmnpNuLjz2nec8TF2XNUVoLLBZs22Yl5t9/evOdRFEVpJI2ZKPc3bD2m27AupkuAPgGWq22RmmrfBw2isrKU8vL9hIf3bd5z1C75vXChfdf4g6IorURjgtSTjDHXADnGmIeAE4DBgRWrjeFVEAMHUla2G4Dw8GbWkbVnUy9cCGPHQq9ezXseRVGURtIYBVHqvBeLSE/s+tEda1rv1q3W7dO3L6WlOwECZ0FcdJGdjPfttzBjRvOeQ1EUpQk0Jgbxroh0Bp4AVgIG+EcghWpzpKZCnz4QGkpp6S4gAAri+OPhggvs0qMA554L117bvOdQFEVpAodUEM5CQZ8ZY3KB/4rIe0C4MSavJYRrM2zdateAAEpLdyISTFhYz+Y9R9eu8M47zXtMRVGUI+CQLiZjjAd43me7rMMpB2OsBVGVwbSTsLBeiLhaWTBFUZTA0pgYxGcicpF481s7GllZkJfnY0Hsav4AtaIoShukMQriRmxxvjIRyReRAhHJD7BcbYetW+27jwXR7PEHRVGUNkhjZlJ3vKVFffFJcfV4yikv30dYmFoQiqK0fxpUECJysr/22gsItVu2boWgIOjXj7KyPYBRC0JRlA5BY9Jc7/X5HA5MBH4ATg+IRG0N3xTXHG+Kq1oQiqK0fxqMQRhjzvN5nQmMBPysbFMXEZkqIptFJFVEHvCz/24R2SAia0TkMxHp47OvUkRWOa9FTbmoZiU1tUaKKwRgDoSiKEobpDFB6trsBYY11ElsHujzwDRgODBTRIbX6vYjMN4YcyzwNvC4z74SY8xo59U6BYmMsS6mqgD1LiCIsLDkVhFHURSlJWlMDOJZ7OxpsAplNHZGdUNMBFKNMdud48wDZgAbvB2MMUt8+n8LXNUoqVuK7GxbPM/HgggLSyIoSFd3UxSl/dOYGESKz2c38B9jzLJGjEvCLi7kZS9w3CH6Xw984LMdLiIpzjkfNcYsqD1ARG4AbgDo3bt3I0RqIn5TXDX+oChKx6AxCuJtoNQYUwnWdSQikcaY4uYSQkSuAsYDp/g09zHGpIlIf+BzEVnrLF5UhTHmReBFgPHjxxuaG58UV7Aups6dT2r20yiKorRFGjWTGojw2Y4APm3EuDTAt1Z1stNWAxGZAvwKmG6MKfO2G2PSnPftwFJgTCPO2bykplaluHo8bsrK9uocCEVROgyNURDhvsuMOp8jGzFuBTBIRPqJSChwOVAjG0lExgB/xyqHgz7tcSIS5nxOACbjE7toMbZuhd69ISyM8vI0oFIzmBRF6TA0RkEUichY74aIjANKGhpkjHEDtwIfARuB+caY9SLysIh4s5KeAKKBt2qlsw4DUkRkNbAEG4NoeQXhN8VVLQhFUToGjYlB3Im9ge/DLjnaHbsEaYMYYxYDi2u1/dbn85R6xi0HjmnMOQJGRgasX1+1JkPA1oFQFEVpozSmFtMKERkKDHGaNhtjKgIrVhvgvvugrAxuuQXwtSACkC2lKIrSBmnQxSQis4EoY8w6Y8w6IFpEbgm8aK3IF1/Aq6/CvffCcDu3r7R0F6GhPQgKCmtd2RRFUVqIxsQgfu6sKAeAMSYH+HnAJGptysvh5puhb1/49a+rmrXMt6IoHY3GxCBcIiLGGANVJTRCAytWK/LnP8PGjfD++xBZnaxVWrqdTp1OaEXBFEVRWpbGWBAfAm+KyBkicgbwH2rOeG4/VFbCI4/A+efD2WdXNbvdBZSW7iQysnYpKUVRlPZLYyyI+7HlLG5yttdgM5naH9nZUFQEZ5xRo7moaD0A0dGtm1ilKIrSkjSm3LcH+A7YiS3Adzp2XkP7IzPTvnfpUqO5qGgtAFFRqiAURek41GtBiMhgYKbzygTeBDDGnNYyorUCWVn2PSGhRnNR0VqCgqI0SK0oSofiUC6mTcBXwLnGmFQAEbmrRaRqLbwWhB8FERU1EpHDWT5DURTl6ORQd7wLgXRgiYj8wwlQS8uI1Ur4URDGGAoL12r8QVGUDke9CsIYs8AYczkwFFsP6U4gUUT+KiJntZB8LYufGER5+X7c7iyNPyiK0uFoTJC6yBjzb2PMediS3T9iM5vaH1lZEBFRY/6DBqgVRemoNMmpbozJMca8aIw5o+HeRyGZmX7jD6AKQlGUjodGXX3xoyAKC9cSGtqd0NCEegYpiqK0T1RB+JKZ6XcOhFoPiqJ0RFRB+JKVVSuDqZLi4g2qIBRF6ZCogvCllouppCQVj6dUFYSiKB0SVRBe3G7IyanhYiostAFqnQOhKEpHRBWEl5wc++5jQdgMpiCt4qooSodEFYQXP7Ooi4rWEhExEJcropWEUhRFaT1UQXipR0Fo/EFRlI6KKggvtcpsGGMoLd1FRMTAVhRKURSl9VAF4aVWqW+3OxdjKggN7daKQimKorQeqiC81LIgyssPAKiCUBSlw6IKwktmpi3S5xTqq6g4CEBISGJrSqUoitJqqILwUqvMhloQiqJ0dFRBeKlVZsNrQYSGqgWhKErHJKAKQkSmishmEUkVkQf87L9bRDaIyBoR+UxE+vjsmyUiW53XrEDKCdQps2EtiCBCQrSKq6IoHZOAKQgRcQHPA9OA4cBMEak9JflHYLwx5ljgbeBxZ2w88DvgOGAi8DsRiQuUrIAfF9NBQkISsJehKIrS8QikBTERSDXGbDfGlAPzgBm+HYwxS4wxxc7mt9gV6wB+AnxijMk2xuQAnwBTAyhrHQuiouKAupcURenQBFJBJAF7fLb3Om31cT3wQVPGisgNIpIiIikZGRmHL6nbDbm5dVxMISEaoFYUpePSJoLUInIVMB54oinjnOVPxxtjxnft2vXwBcjOtu81FMRBtSAURenQBFJBpAG9fLaTnbYaiMgU4FfAdGNMWVPGNhu1JsmB18WkFoSiKB2XQCqIFcAgEeknIqHA5cAi3w4iMgb4O1Y5HPTZ9RFwlojEOcHps5y2wFCrUF9lZTGVlYU6SU5RlA5NcKAObIxxi8it2Bu7C3jZGLNeRB4GUowxi7AupWjgLREB2G2MmW6MyRaR32OVDMDDxpjsQMlauw5Tebl3DoRaEIqidFwCpiAAjDGLgcW12n7r83nKIca+DLwcOOl8qGVBaJkNRVGUNhKkbnW0UJ+iKEodVEGAdTFFRkKEXTlOy2woiqKogrD4LbOhLiZFUTo2qiDAb5kNlytG16JWFKVDowoC6imzofEHRVE6NqogoE6pb1uoTxWEoigdG1UQ4DcGoQFqRVE6OqogKipsob4aZTYOqotJUZQOjyqIWoX6PB43FRWZmsGkKEqHJ6AzqY8K4uLg++8h2S5FUVGRCRi1IBRF6fCogggNhQkTqja1zIaiKIpFXUy10DIbiqIoFlUQtdAyG4qiKBZVELWoLrOhFoSiKB0bVRC1KC8/iEgowcGxrS2KoihKq6IKoha2zEYizgJGiqIoHRZVELWwZTY0/qAoiqIKoha2zIbGHxRFUVRB1ELLbCiKolhUQfhgjFEXk6IoioMqCB/c7hyMKSM0tHtri6IoitLqqILwoaxsHwBhYUmtLImiKErro7WYfCgvtwoiNLRnK0uiKEcXFRUV7N27l9LS0tYWRamH8PBwkpOTCQkJafQYVRA+VFsQqiAUpSns3buXmJgY+vbtq3OI2iDGGLKysti7dy/9+vVr9Dh1MflQXp4GqAWhKE2ltLSULl26qHJoo4gIXbp0abKFpwrCh7KyfQQHx+Nyhbe2KIpy1KHKoW1zOL+PKggfysv3qXtJURTFIaAKQkSmishmEUkVkQf87D9ZRFaKiFtELq61r1JEVjmvRYGU00tZWZq6lxTlKCQrK4vRo0czevRounfvTlJSUtV2eXn5IcempKRw++23N3iOSZMmNZe4Rw0BC1KLiAt4HjgT2AusEJFFxpgNPt12A9cCv/BziBJjzOhAyeePsrJ9REWNbMlTKorSDHTp0oVVq1YBMGfOHKKjo/nFL6pvK263m+Bg/7e78ePHM378+AbPsXz58maR9WgikFlME4FUY8x2ABGZB8wAqhSEMWans88TQDkahTGVlJfvVwtCUY6QrVvvpLBwVbMeMzp6NIMGPdOkMddeey3h4eH8+OOPTJ48mcsvv5w77riD0tJSIiIieOWVVxgyZAhLly7lySef5L333mPOnDns3r2b7du3s3v3bu68884q6yI6OprCwkKWLl3KnDlzSEhIYN26dYwbN4433ngDEWHx4sXcfffdREVFMXnyZLZv3857771XQ66dO3dy9dVXU1RUBMBzzz1XZZ089thjvPHGGwQFBTFt2jQeffRRUlNTuemmm8jIyMDlcvHWW28xYMCAI/9SG0EgFUQSsMdney9wXBPGh4tICuAGHjXGLKjdQURuAG4A6N279+FLiq3iCpUag1CUdsTevXtZvnw5LpeL/Px8vvrqK4KDg/n000958MEH+e9//1tnzKZNm1iyZAkFBQUMGTKEm2++uc7cgR9//JH169fTs2dPJk+ezLJlyxg/fjw33ngjX375Jf369WPmzJl+ZUpMTOSTTz4hPDycrVu3MnPmTFJSUvjggw9YuHAh3333HZGRkWRnZwNw5ZVX8sADD3DBBRdQWlqKx9Nyz9NteR5EH2NMmoj0Bz4XkbXGmG2+HYwxLwIvAowfP94cycm8k+R0FrWiHBlNfdIPJJdccgkulwuAvLw8Zs2axdatWxERKioq/I4555xzCAsLIywsjMTERA4cOEBycnKNPhMnTqxqGz16NDt37iQ6Opr+/ftXzTOYOXMmL774Yp3jV1RUcOutt7Jq1SpcLhdbtmwB4NNPP+W6664jMjISgPj4eAoKCkhLS+OCCy4A7GS3liSQQeo0oJfPdrLT1iiMMWnO+3ZgKTCmOYWrjXeSnLqYFKX9EBUVVfX5N7/5Daeddhrr1q3j3XffrXdOQFhYWNVnl8uF2+0+rD718fTTT9OtWzdWr15NSkpKg0H01iSQCmIFMEhE+olIKHA50KhsJBGJE5Ew53MCMBmf2EUgqLYgVEEoSnskLy+PpCTrIXj11Veb/fhDhgxh+/bt7Ny5E4A333yzXjl69OhBUFAQr7/+OpWVlQCceeaZvPLKKxQXFwOQnZ1NTEwMycnJLFiwAICysrKq/S1BwBSEMcYN3Ap8BGwE5htj1ovIwyIyHUBEJojIXuAS4O8ist4ZPgxIEZHVwBJsDCKgCqKsLA0IIiRE14JQlPbIfffdxy9/+UvGjBnTpCf+xhIREcELL7zA1KlTGTduHDExMcTG1l3b/pZbbmHu3LmMGjWKTZs2VVk5U6dOZfr06YwfP57Ro0fz5JNPAvD666/zl7/8hWOPPZZJkyaxf//+Zpe9PsSYI3LdtxnGjx9vUlJSDnv8pk0/Izt7MZMm7WtGqRSlY7Bx40aGDRvW2mK0OoWFhURHR2OMYfbs2QwaNIi77rqrtcWqwt/vJCI/GGP85vnqTGqH8vJ9Gn9QFOWI+Mc//sHo0aMZMWIEeXl53Hjjja0t0hHRlrOYWpSysjTCw/u0thiKohzF3HXXXW3KYjhS1IJwsHWYNMVVURTFiyoIwOMpo6IiU11MiqIoPqiCAMrLbVaAprgqiqJUowoCb4orhIaqi0lRFMWLKgh0qVFFOdo57bTT+Oijj2q0PfPMM9x88831jjn11FPxpsafffbZ5Obm1ukzZ86cqvkI9bFgwQI2bKiepvXb3/6WTz/9tAnSt11UQVA9i1pjEIpydDJz5kzmzZtXo23evHn1FsyrzeLFi+ncufNhnbu2gnj44YeZMmXKYR2rraFprlgXk0goISFdWlsURTn6ufNOcNZmaDZGj4Znnql398UXX8yvf/1rysvLCQ0NZefOnezbt4+TTjqJm2++mRUrVlBSUsLFF1/MQw89VGd83759SUlJISEhgUceeYS5c+eSmJhIr169GDduHGDnOLz44ouUl5czcOBAXn/9dVatWsWiRYv44osv+MMf/sB///tffv/733Puuedy8cUX89lnn/GLX/wCt9vNhAkT+Otf/0pYWBh9+/Zl1qxZvPvuu1RUVPDWW28xdOjQGjK1hbLgakFQvdSorqmrKEcn8fHxTJw4kQ8++ACw1sOll16KiPDII4+QkpLCmjVr+OKLL1izZk29x/nhhx+YN28eq1atYvHixaxYsaJq34UXXsiKFStYvXo1w4YN46WXXmLSpElMnz6dJ554glWrVtW4IZeWlnLttdfy5ptvsnbtWtxuN3/961+r9ickJLBy5Upuvvlmv24sb1nwlStX8uabb1atS+FbFnz16tXcd999gC0LPnv2bFavXs3y5cvp0aPHkX2pqAUB2BiEupcUpZk4xJN+IPG6mWbMmMG8efN46aWXAJg/fz4vvvgibreb9PR0NmzYwLHHHuv3GF999RUXXHBBVcnt6dOnV+1bt24dv/71r8nNzaWwsJCf/OQnh5Rn8+bN9OvXj8GDBwMwa9Ysnn/+ee68807AKhyAcePG8c4779QZ3xbKgquCwLqYoqOPaW0xFEU5AmbMmMFdd93FypUrKS4uZty4cezYsYMnn3ySFStWEBcXx7XXXltvme+GuPbaa1mwYAGjRo3i1VdfZenSpUckr7dkeH3lwn3Lgns8nhZfCwLUxQR46zBpiquiHM1ER0dz2mmn8dOf/rQqOJ2fn09UVBSxsbEcOHCgygVVHyeffDILFiygpKSEgoIC3n333ap9BQUF9OjRg4qKCv71r39VtcfExFBQUFDnWEOGDGHnzp2kpqYCtirrKaec0ujraQtlwTu8gnC7C6isLNAUV0VpB8ycOZPVq1dXKYhRo0YxZswYhg4dyhVXXMHkyZMPOX7s2LFcdtlljBo1imnTpjFhwoSqfb///e857rjjmDx5co2A8uWXX84TTzzBmDFj2LatetHL8PBwXnnlFS655BKOOeYYgoKCuOmmmxp9LW2hLHiHL/ddXp5JauptdO/+U+LjzwyAZIrS/tFy30cHTS333eFjEKGhCQwf/p/WFkNRFKXN0eFdTIqiKIp/VEEoitIstBd3dXvlcH4fVRCKohwx4eHhZGVlqZJooxhjyMrKanKqbIePQSiKcuQkJyezd+9eMjIyWlsUpR7Cw8NJTk5u0hhVEIqiHDEhISH069evtcVQmhl1MSmKoih+UQWhKIqi+EUVhKIoiuKXdjOTWkQygF1NHJYAZAZAnLZMR7xm6JjX3RGvGTrmdR/JNfcxxnT1t6PdKIjDQURS6pti3l7piNcMHfO6O+I1Q8e87kBds7qYFEVRFL+oglAURVH80tEVxIutLUAr0BGvGTrmdXfEa4aOed0BueYOHYNQFEVR6qejWxCKoihKPaiCUBRFUfzSIRWEiEwVkc0ikioiD7S2PIFCRHqJyBIR2SAi60XkDqc9XkQ+EZGtzntca8va3IiIS0R+FJH3nO1+IvKd85u/KSKhrS1jcyIinUXkbRHZJCIbReSEDvI73+X8ba8Tkf+ISHh7/K1F5GUROSgi63za/P6+YvmLc/1rRGTs4Z63wykIEXEBzwPTgOHATBEZ3rpSBQw3cI8xZjhwPDDbudYHgM+MMYOAz5zt9sYdwEaf7ceAp40xA4Ec4PpWkSpw/B/woTFmKDAKe+3t+ncWkSTgdmC8MWYk4AIup33+1q8CU2u11ff7TgMGOa8bgL8e7kk7nIIAJgKpxpjtxphyYB4wo5VlCgjGmHRjzErncwH2ppGEvd65Tre5wPmtImCAEJFk4Bzgn862AKcDbztd2tU1i0gscDLwEoAxptwYk0s7/50dgoEIEQkGIoF02uFvbYz5Esiu1Vzf7zsDeM1YvgU6i0iPwzlvR1QQScAen+29Tlu7RkT6AmOA74Buxph0Z9d+oFtryRUgngHuAzzOdhcg1xjjdrbb22/eD8gAXnHcav8UkSja+e9sjEkDngR2YxVDHvAD7fu39qW+37fZ7nEdUUF0OEQkGvgvcKcxJt93n7F5zu0m11lEzgUOGmN+aG1ZWpBgYCzwV2PMGKCIWu6k9vY7Azg+9xlYBdkTiKKuG6ZDEKjftyMqiDSgl892stPWLhGREKxy+Jcx5h2n+YDX5HTeD7aWfAFgMjBdRHZi3YenY/3znR03BLS/33wvsNcY852z/TZWYbTn3xlgCrDDGJNhjKkA3sH+/u35t/alvt+32e5xHVFBrAAGOZkOodig1qJWlikgOL73l4CNxpinfHYtAmY5n2cBC1tatkBhjPmlMSbZGNMX+9t+boy5ElgCXOx0a2/XvB/YIyJDnKYzgA2049/ZYTdwvIhEOn/r3utut791Ler7fRcB1zjZTMcDeT6uqCbRIWdSi8jZWD+1C3jZGPNI60oUGETkROArYC3V/vgHsXGI+UBvbIn0S40xtQNgRz0icirwC2PMuSLSH2tRxAM/AlcZY8paUbxmRURGY4PyocB24DrsA2C7/p1F5CHgMmzG3o/Az7D+9nb1W4vIf4BTsWW9DwC/Axbg5/d1lOVzWHdbMXCdMSblsM7bERWEoiiK0jAd0cWkKIqiNAJVEIqiKIpfVEEoiqIoflEFoSiKovhFFYSiKIriF1UQitIAIlIpIqt8Xs1W9E5E+vpW6FSUtkRww10UpcNTYowZ3dpCKEpLoxaEohwmIrJTRB4XkbUi8r2IDHTa+4rI504t/s9EpLfT3k1E/iciq53XJOdQLhH5h7OuwcciEuH0v13sWh5rRGReK12m0oFRBaEoDRNRy8V0mc++PGPMMdiZq884bc8Cc40xxwL/Av7itP8F+MIYMwpbK2m90z4IeN4YMwLIBS5y2h8AxjjHuSkwl6Yo9aMzqRWlAUSk0BgT7ad9J3C6MWa7UxRxvzGmi4hkAj2MMRVOe7oxJkFEMoBk37IPThn2T5xFXxCR+4EQY8wfRORDoBBbUmGBMaYwwJeqKDVQC0JRjgxTz+em4FsnqJLq2OA52NUPxwIrfCqUKkqLoApCUY6My3zev3E+L8dWkgW4ElswEeyykDdD1ZrZsfUdVESCgF7GmCXA/UAsUMeKUZRAok8kitIwESKyymf7Q2OMN9U1TkTWYK2AmU7bbdjV3e7FrvR2ndN+B/CiiFyPtRRuxq6E5g8X8IajRAT4i7OMqKK0GBqDUJTDxIlBjDfGZLa2LIoSCNTFpCiKovhFLQhFURTFL2pBKIqiKH5RBaEoiqL4RRWEoiiK4hdVEIqiKIpfVEEoiqIofvl/Z8RPXb1FM5oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_graphs(history1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  0.764\n"
     ]
    }
   ],
   "source": [
    "tmp_pred_value1 = calc_train_acc(model1, 'expected_target1', Y_expected1, new_X_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for agent2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fearues_names2 = feature_selection(X_train2, y_train2, X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_2_feat_ScoredAv</th>\n",
       "      <th>agent_2_feat_XgAv</th>\n",
       "      <th>agent_2_feat_pl_mean</th>\n",
       "      <th>agent_1_feat_MissedAv</th>\n",
       "      <th>agent_2_feat_Rating</th>\n",
       "      <th>agent_1_feat_XgaAv</th>\n",
       "      <th>agent_2_feat_xg_mean_3</th>\n",
       "      <th>agent_2_feat_scored_mean_3</th>\n",
       "      <th>agent_2_feat_form_mean_3</th>\n",
       "      <th>agent_2_feat_scored_3</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_1_feat_scored_1.1</th>\n",
       "      <th>agent_1_feat_xg_2</th>\n",
       "      <th>agent_2_feat_xga_mean_3</th>\n",
       "      <th>agent_1_feat_form_mean_3</th>\n",
       "      <th>agent_1_feat_scored_mean_3.1</th>\n",
       "      <th>agent_1_feat_xg_mean_3</th>\n",
       "      <th>agent_1_feat_XgAv</th>\n",
       "      <th>agent_2_feat_MissedAv</th>\n",
       "      <th>agent_2_feat_XgaAv</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.631579</td>\n",
       "      <td>1.426579</td>\n",
       "      <td>80.272727</td>\n",
       "      <td>1.394737</td>\n",
       "      <td>6.99</td>\n",
       "      <td>1.523421</td>\n",
       "      <td>1.178134</td>\n",
       "      <td>1.210526</td>\n",
       "      <td>-0.228070</td>\n",
       "      <td>1.631579</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.371141</td>\n",
       "      <td>0.664349</td>\n",
       "      <td>-0.771930</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.750198</td>\n",
       "      <td>1.036053</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>1.048421</td>\n",
       "      <td>6.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.263158</td>\n",
       "      <td>1.217368</td>\n",
       "      <td>76.090909</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.87</td>\n",
       "      <td>1.065789</td>\n",
       "      <td>1.083304</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>-0.324561</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.480530</td>\n",
       "      <td>1.005866</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>2.543860</td>\n",
       "      <td>1.596474</td>\n",
       "      <td>1.995263</td>\n",
       "      <td>1.184211</td>\n",
       "      <td>1.237632</td>\n",
       "      <td>7.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.236842</td>\n",
       "      <td>1.177895</td>\n",
       "      <td>74.727273</td>\n",
       "      <td>1.657895</td>\n",
       "      <td>6.83</td>\n",
       "      <td>1.342368</td>\n",
       "      <td>0.651080</td>\n",
       "      <td>1.078947</td>\n",
       "      <td>-0.692982</td>\n",
       "      <td>1.236842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.965583</td>\n",
       "      <td>1.363546</td>\n",
       "      <td>-0.412281</td>\n",
       "      <td>0.912281</td>\n",
       "      <td>0.907779</td>\n",
       "      <td>1.058421</td>\n",
       "      <td>1.342105</td>\n",
       "      <td>1.196842</td>\n",
       "      <td>6.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.985526</td>\n",
       "      <td>75.090909</td>\n",
       "      <td>1.921053</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.717368</td>\n",
       "      <td>0.881272</td>\n",
       "      <td>1.271930</td>\n",
       "      <td>-0.061404</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.900670</td>\n",
       "      <td>1.326438</td>\n",
       "      <td>-0.807018</td>\n",
       "      <td>0.807018</td>\n",
       "      <td>1.393386</td>\n",
       "      <td>1.203421</td>\n",
       "      <td>1.394737</td>\n",
       "      <td>1.354211</td>\n",
       "      <td>6.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.016579</td>\n",
       "      <td>73.818182</td>\n",
       "      <td>1.289474</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.472105</td>\n",
       "      <td>1.121913</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.278076</td>\n",
       "      <td>1.024063</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.280702</td>\n",
       "      <td>0.756213</td>\n",
       "      <td>1.076316</td>\n",
       "      <td>1.342105</td>\n",
       "      <td>1.414474</td>\n",
       "      <td>6.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>1.078947</td>\n",
       "      <td>1.186579</td>\n",
       "      <td>75.727273</td>\n",
       "      <td>1.973684</td>\n",
       "      <td>6.62</td>\n",
       "      <td>1.884474</td>\n",
       "      <td>1.344915</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.801428</td>\n",
       "      <td>1.578853</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.084952</td>\n",
       "      <td>0.979737</td>\n",
       "      <td>1.763158</td>\n",
       "      <td>1.884211</td>\n",
       "      <td>6.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>1.026316</td>\n",
       "      <td>1.205526</td>\n",
       "      <td>72.818182</td>\n",
       "      <td>1.684211</td>\n",
       "      <td>6.72</td>\n",
       "      <td>1.566579</td>\n",
       "      <td>1.186190</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.344718</td>\n",
       "      <td>1.602597</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.414319</td>\n",
       "      <td>1.277895</td>\n",
       "      <td>1.026316</td>\n",
       "      <td>1.369474</td>\n",
       "      <td>6.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>0.815789</td>\n",
       "      <td>0.906579</td>\n",
       "      <td>76.272727</td>\n",
       "      <td>1.631579</td>\n",
       "      <td>6.68</td>\n",
       "      <td>1.797895</td>\n",
       "      <td>1.122357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.513760</td>\n",
       "      <td>1.172430</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.781500</td>\n",
       "      <td>1.291316</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>1.510263</td>\n",
       "      <td>6.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>1.815789</td>\n",
       "      <td>2.006053</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>1.578947</td>\n",
       "      <td>6.86</td>\n",
       "      <td>1.489211</td>\n",
       "      <td>2.855210</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.180210</td>\n",
       "      <td>1.046628</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.639787</td>\n",
       "      <td>1.486842</td>\n",
       "      <td>1.421053</td>\n",
       "      <td>1.081316</td>\n",
       "      <td>6.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>1.052632</td>\n",
       "      <td>1.175526</td>\n",
       "      <td>75.181818</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>6.60</td>\n",
       "      <td>1.506579</td>\n",
       "      <td>1.677573</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.532090</td>\n",
       "      <td>1.660693</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.747413</td>\n",
       "      <td>1.337368</td>\n",
       "      <td>1.710526</td>\n",
       "      <td>1.665526</td>\n",
       "      <td>6.71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1730 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      agent_2_feat_ScoredAv  agent_2_feat_XgAv  agent_2_feat_pl_mean  \\\n",
       "20                 1.631579           1.426579             80.272727   \n",
       "21                 1.263158           1.217368             76.090909   \n",
       "22                 1.236842           1.177895             74.727273   \n",
       "23                 0.815789           0.985526             75.090909   \n",
       "24                 1.000000           1.016579             73.818182   \n",
       "...                     ...                ...                   ...   \n",
       "1973               1.078947           1.186579             75.727273   \n",
       "1974               1.026316           1.205526             72.818182   \n",
       "1975               0.815789           0.906579             76.272727   \n",
       "1977               1.815789           2.006053             80.000000   \n",
       "1978               1.052632           1.175526             75.181818   \n",
       "\n",
       "      agent_1_feat_MissedAv  agent_2_feat_Rating  agent_1_feat_XgaAv  \\\n",
       "20                 1.394737                 6.99            1.523421   \n",
       "21                 1.000000                 6.87            1.065789   \n",
       "22                 1.657895                 6.83            1.342368   \n",
       "23                 1.921053                 6.75            1.717368   \n",
       "24                 1.289474                 6.77            1.472105   \n",
       "...                     ...                  ...                 ...   \n",
       "1973               1.973684                 6.62            1.884474   \n",
       "1974               1.684211                 6.72            1.566579   \n",
       "1975               1.631579                 6.68            1.797895   \n",
       "1977               1.578947                 6.86            1.489211   \n",
       "1978               1.263158                 6.60            1.506579   \n",
       "\n",
       "      agent_2_feat_xg_mean_3  agent_2_feat_scored_mean_3  \\\n",
       "20                  1.178134                    1.210526   \n",
       "21                  1.083304                    0.754386   \n",
       "22                  0.651080                    1.078947   \n",
       "23                  0.881272                    1.271930   \n",
       "24                  1.121913                    1.000000   \n",
       "...                      ...                         ...   \n",
       "1973                1.344915                    1.333333   \n",
       "1974                1.186190                    0.666667   \n",
       "1975                1.122357                    1.000000   \n",
       "1977                2.855210                    2.666667   \n",
       "1978                1.677573                    2.666667   \n",
       "\n",
       "      agent_2_feat_form_mean_3  agent_2_feat_scored_3  ...  \\\n",
       "20                   -0.228070               1.631579  ...   \n",
       "21                   -0.324561               1.263158  ...   \n",
       "22                   -0.692982               1.236842  ...   \n",
       "23                   -0.061404               0.815789  ...   \n",
       "24                   -0.043860               1.000000  ...   \n",
       "...                        ...                    ...  ...   \n",
       "1973                 -0.333333               0.000000  ...   \n",
       "1974                 -0.333333               0.000000  ...   \n",
       "1975                  0.000000               0.000000  ...   \n",
       "1977                  0.333333               5.000000  ...   \n",
       "1978                  0.666667               3.000000  ...   \n",
       "\n",
       "      agent_1_feat_scored_1.1  agent_1_feat_xg_2  agent_2_feat_xga_mean_3  \\\n",
       "20                        1.0           0.371141                 0.664349   \n",
       "21                        3.0           1.480530                 1.005866   \n",
       "22                        0.0           0.965583                 1.363546   \n",
       "23                        0.0           1.900670                 1.326438   \n",
       "24                        1.0           0.278076                 1.024063   \n",
       "...                       ...                ...                      ...   \n",
       "1973                      3.0           0.801428                 1.578853   \n",
       "1974                      2.0           0.344718                 1.602597   \n",
       "1975                      2.0           0.513760                 1.172430   \n",
       "1977                      1.0           2.180210                 1.046628   \n",
       "1978                      3.0           2.532090                 1.660693   \n",
       "\n",
       "      agent_1_feat_form_mean_3  agent_1_feat_scored_mean_3.1  \\\n",
       "20                   -0.771930                      0.824561   \n",
       "21                    0.815789                      2.543860   \n",
       "22                   -0.412281                      0.912281   \n",
       "23                   -0.807018                      0.807018   \n",
       "24                    0.684211                      1.280702   \n",
       "...                        ...                           ...   \n",
       "1973                 -0.333333                      2.666667   \n",
       "1974                 -0.666667                      1.000000   \n",
       "1975                  0.333333                      1.333333   \n",
       "1977                 -0.333333                      1.000000   \n",
       "1978                  0.333333                      2.333333   \n",
       "\n",
       "      agent_1_feat_xg_mean_3  agent_1_feat_XgAv  agent_2_feat_MissedAv  \\\n",
       "20                  0.750198           1.036053               0.973684   \n",
       "21                  1.596474           1.995263               1.184211   \n",
       "22                  0.907779           1.058421               1.342105   \n",
       "23                  1.393386           1.203421               1.394737   \n",
       "24                  0.756213           1.076316               1.342105   \n",
       "...                      ...                ...                    ...   \n",
       "1973                1.084952           0.979737               1.763158   \n",
       "1974                1.414319           1.277895               1.026316   \n",
       "1975                0.781500           1.291316               1.315789   \n",
       "1977                1.639787           1.486842               1.421053   \n",
       "1978                1.747413           1.337368               1.710526   \n",
       "\n",
       "      agent_2_feat_XgaAv  agent_1_feat_Rating  \n",
       "20              1.048421                 6.79  \n",
       "21              1.237632                 7.07  \n",
       "22              1.196842                 6.74  \n",
       "23              1.354211                 6.77  \n",
       "24              1.414474                 6.86  \n",
       "...                  ...                  ...  \n",
       "1973            1.884211                 6.52  \n",
       "1974            1.369474                 6.59  \n",
       "1975            1.510263                 6.72  \n",
       "1977            1.081316                 6.71  \n",
       "1978            1.665526                 6.71  \n",
       "\n",
       "[1730 rows x 33 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_train2 = X_train2[list(fearues_names2)]\n",
    "new_X_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_2_feat_ScoredAv</th>\n",
       "      <th>agent_2_feat_XgAv</th>\n",
       "      <th>agent_2_feat_pl_mean</th>\n",
       "      <th>agent_1_feat_MissedAv</th>\n",
       "      <th>agent_2_feat_Rating</th>\n",
       "      <th>agent_1_feat_XgaAv</th>\n",
       "      <th>agent_2_feat_xg_mean_3</th>\n",
       "      <th>agent_2_feat_scored_mean_3</th>\n",
       "      <th>agent_2_feat_form_mean_3</th>\n",
       "      <th>agent_2_feat_scored_3</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_1_feat_scored_1.1</th>\n",
       "      <th>agent_1_feat_xg_2</th>\n",
       "      <th>agent_2_feat_xga_mean_3</th>\n",
       "      <th>agent_1_feat_form_mean_3</th>\n",
       "      <th>agent_1_feat_scored_mean_3.1</th>\n",
       "      <th>agent_1_feat_xg_mean_3</th>\n",
       "      <th>agent_1_feat_XgAv</th>\n",
       "      <th>agent_2_feat_MissedAv</th>\n",
       "      <th>agent_2_feat_XgaAv</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>1.736842</td>\n",
       "      <td>1.741842</td>\n",
       "      <td>78.727273</td>\n",
       "      <td>1.526316</td>\n",
       "      <td>6.83</td>\n",
       "      <td>1.763947</td>\n",
       "      <td>1.170650</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.098070</td>\n",
       "      <td>0.892297</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.481092</td>\n",
       "      <td>0.960263</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.001579</td>\n",
       "      <td>6.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1.289474</td>\n",
       "      <td>1.291316</td>\n",
       "      <td>78.818182</td>\n",
       "      <td>1.473684</td>\n",
       "      <td>6.72</td>\n",
       "      <td>1.295000</td>\n",
       "      <td>1.014997</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.358540</td>\n",
       "      <td>1.888677</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.642060</td>\n",
       "      <td>1.413421</td>\n",
       "      <td>1.631579</td>\n",
       "      <td>1.797895</td>\n",
       "      <td>6.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>1.026316</td>\n",
       "      <td>1.247895</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>1.763158</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1.884211</td>\n",
       "      <td>1.477451</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.914266</td>\n",
       "      <td>1.947001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.102665</td>\n",
       "      <td>1.186579</td>\n",
       "      <td>1.421053</td>\n",
       "      <td>1.590000</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.979737</td>\n",
       "      <td>72.818182</td>\n",
       "      <td>1.710526</td>\n",
       "      <td>6.52</td>\n",
       "      <td>1.665526</td>\n",
       "      <td>1.120646</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.319570</td>\n",
       "      <td>2.625453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.433756</td>\n",
       "      <td>1.175526</td>\n",
       "      <td>1.973684</td>\n",
       "      <td>1.884474</td>\n",
       "      <td>6.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960263</td>\n",
       "      <td>76.727273</td>\n",
       "      <td>1.421053</td>\n",
       "      <td>6.65</td>\n",
       "      <td>1.081316</td>\n",
       "      <td>0.600833</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.264430</td>\n",
       "      <td>1.336501</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>2.549883</td>\n",
       "      <td>2.006053</td>\n",
       "      <td>1.526316</td>\n",
       "      <td>1.763947</td>\n",
       "      <td>6.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2465</th>\n",
       "      <td>1.631579</td>\n",
       "      <td>1.559474</td>\n",
       "      <td>74.727273</td>\n",
       "      <td>1.631579</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.579474</td>\n",
       "      <td>0.981234</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.168796</td>\n",
       "      <td>1.369450</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.538870</td>\n",
       "      <td>1.156842</td>\n",
       "      <td>1.421053</td>\n",
       "      <td>1.658421</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2466</th>\n",
       "      <td>1.631579</td>\n",
       "      <td>1.587895</td>\n",
       "      <td>77.909091</td>\n",
       "      <td>1.736842</td>\n",
       "      <td>6.77</td>\n",
       "      <td>1.628947</td>\n",
       "      <td>1.702073</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.709634</td>\n",
       "      <td>0.410268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.453369</td>\n",
       "      <td>0.928684</td>\n",
       "      <td>1.236842</td>\n",
       "      <td>1.312105</td>\n",
       "      <td>6.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2467</th>\n",
       "      <td>2.184211</td>\n",
       "      <td>2.045263</td>\n",
       "      <td>83.545455</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.01</td>\n",
       "      <td>1.948421</td>\n",
       "      <td>2.404496</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.463320</td>\n",
       "      <td>0.276812</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.265762</td>\n",
       "      <td>0.920263</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.805526</td>\n",
       "      <td>6.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2468</th>\n",
       "      <td>1.447368</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>80.090909</td>\n",
       "      <td>1.789474</td>\n",
       "      <td>6.69</td>\n",
       "      <td>1.540789</td>\n",
       "      <td>2.236761</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500436</td>\n",
       "      <td>0.582768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.516526</td>\n",
       "      <td>1.191579</td>\n",
       "      <td>1.026316</td>\n",
       "      <td>1.137632</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2469</th>\n",
       "      <td>1.447368</td>\n",
       "      <td>1.492632</td>\n",
       "      <td>76.090909</td>\n",
       "      <td>1.447368</td>\n",
       "      <td>6.84</td>\n",
       "      <td>1.554211</td>\n",
       "      <td>1.589569</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.395668</td>\n",
       "      <td>2.069648</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.708280</td>\n",
       "      <td>1.003421</td>\n",
       "      <td>1.210526</td>\n",
       "      <td>1.401316</td>\n",
       "      <td>6.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>433 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      agent_2_feat_ScoredAv  agent_2_feat_XgAv  agent_2_feat_pl_mean  \\\n",
       "1979               1.736842           1.741842             78.727273   \n",
       "1980               1.289474           1.291316             78.818182   \n",
       "1981               1.026316           1.247895             75.000000   \n",
       "1982               0.684211           0.979737             72.818182   \n",
       "1983               1.000000           0.960263             76.727273   \n",
       "...                     ...                ...                   ...   \n",
       "2465               1.631579           1.559474             74.727273   \n",
       "2466               1.631579           1.587895             77.909091   \n",
       "2467               2.184211           2.045263             83.545455   \n",
       "2468               1.447368           1.375000             80.090909   \n",
       "2469               1.447368           1.492632             76.090909   \n",
       "\n",
       "      agent_1_feat_MissedAv  agent_2_feat_Rating  agent_1_feat_XgaAv  \\\n",
       "1979               1.526316                 6.83            1.763947   \n",
       "1980               1.473684                 6.72            1.295000   \n",
       "1981               1.763158                 6.63            1.884211   \n",
       "1982               1.710526                 6.52            1.665526   \n",
       "1983               1.421053                 6.65            1.081316   \n",
       "...                     ...                  ...                 ...   \n",
       "2465               1.631579                 6.77            1.579474   \n",
       "2466               1.736842                 6.77            1.628947   \n",
       "2467               2.000000                 7.01            1.948421   \n",
       "2468               1.789474                 6.69            1.540789   \n",
       "2469               1.447368                 6.84            1.554211   \n",
       "\n",
       "      agent_2_feat_xg_mean_3  agent_2_feat_scored_mean_3  \\\n",
       "1979                1.170650                    0.666667   \n",
       "1980                1.014997                    1.666667   \n",
       "1981                1.477451                    1.000000   \n",
       "1982                1.120646                    0.333333   \n",
       "1983                0.600833                    0.333333   \n",
       "...                      ...                         ...   \n",
       "2465                0.981234                    1.666667   \n",
       "2466                1.702073                    1.333333   \n",
       "2467                2.404496                    2.333333   \n",
       "2468                2.236761                    2.333333   \n",
       "2469                1.589569                    1.000000   \n",
       "\n",
       "      agent_2_feat_form_mean_3  agent_2_feat_scored_3  ...  \\\n",
       "1979                  0.000000                    1.0  ...   \n",
       "1980                  0.000000                    2.0  ...   \n",
       "1981                  0.000000                    0.0  ...   \n",
       "1982                 -1.000000                    0.0  ...   \n",
       "1983                  0.000000                    0.0  ...   \n",
       "...                        ...                    ...  ...   \n",
       "2465                 -0.333333                    5.0  ...   \n",
       "2466                  1.000000                    1.0  ...   \n",
       "2467                  1.000000                    1.0  ...   \n",
       "2468                  0.666667                    4.0  ...   \n",
       "2469                 -0.333333                    1.0  ...   \n",
       "\n",
       "      agent_1_feat_scored_1.1  agent_1_feat_xg_2  agent_2_feat_xga_mean_3  \\\n",
       "1979                      0.0           1.098070                 0.892297   \n",
       "1980                      1.0           2.358540                 1.888677   \n",
       "1981                      2.0           0.914266                 1.947001   \n",
       "1982                      2.0           2.319570                 2.625453   \n",
       "1983                      2.0           4.264430                 1.336501   \n",
       "...                       ...                ...                      ...   \n",
       "2465                      1.0           0.168796                 1.369450   \n",
       "2466                      2.0           0.709634                 0.410268   \n",
       "2467                      0.0           2.463320                 0.276812   \n",
       "2468                      1.0           0.500436                 0.582768   \n",
       "2469                      0.0           0.395668                 2.069648   \n",
       "\n",
       "      agent_1_feat_form_mean_3  agent_1_feat_scored_mean_3.1  \\\n",
       "1979                 -0.666667                      0.333333   \n",
       "1980                 -1.000000                      1.333333   \n",
       "1981                  0.000000                      1.333333   \n",
       "1982                  0.000000                      2.000000   \n",
       "1983                  0.333333                      1.666667   \n",
       "...                        ...                           ...   \n",
       "2465                 -1.000000                      0.666667   \n",
       "2466                  0.000000                      1.000000   \n",
       "2467                 -0.333333                      0.000000   \n",
       "2468                  0.000000                      0.333333   \n",
       "2469                 -0.333333                      1.000000   \n",
       "\n",
       "      agent_1_feat_xg_mean_3  agent_1_feat_XgAv  agent_2_feat_MissedAv  \\\n",
       "1979                0.481092           0.960263               0.947368   \n",
       "1980                1.642060           1.413421               1.631579   \n",
       "1981                2.102665           1.186579               1.421053   \n",
       "1982                1.433756           1.175526               1.973684   \n",
       "1983                2.549883           2.006053               1.526316   \n",
       "...                      ...                ...                    ...   \n",
       "2465                0.538870           1.156842               1.421053   \n",
       "2466                0.453369           0.928684               1.236842   \n",
       "2467                1.265762           0.920263               0.842105   \n",
       "2468                0.516526           1.191579               1.026316   \n",
       "2469                0.708280           1.003421               1.210526   \n",
       "\n",
       "      agent_2_feat_XgaAv  agent_1_feat_Rating  \n",
       "1979            1.001579                 6.65  \n",
       "1980            1.797895                 6.68  \n",
       "1981            1.590000                 6.62  \n",
       "1982            1.884474                 6.60  \n",
       "1983            1.763947                 6.86  \n",
       "...                  ...                  ...  \n",
       "2465            1.658421                 6.62  \n",
       "2466            1.312105                 6.61  \n",
       "2467            0.805526                 6.51  \n",
       "2468            1.137632                 6.62  \n",
       "2469            1.401316                 6.64  \n",
       "\n",
       "[433 rows x 33 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X_test2 = X_test2[list(fearues_names2)]\n",
    "new_X_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agent_2_feat_ScoredAv</th>\n",
       "      <th>agent_2_feat_XgAv</th>\n",
       "      <th>agent_2_feat_pl_mean</th>\n",
       "      <th>agent_1_feat_MissedAv</th>\n",
       "      <th>agent_2_feat_Rating</th>\n",
       "      <th>agent_1_feat_XgaAv</th>\n",
       "      <th>agent_2_feat_xg_mean_3</th>\n",
       "      <th>agent_2_feat_scored_mean_3</th>\n",
       "      <th>agent_2_feat_form_mean_3</th>\n",
       "      <th>agent_2_feat_scored_3</th>\n",
       "      <th>...</th>\n",
       "      <th>agent_1_feat_scored_1.1</th>\n",
       "      <th>agent_1_feat_xg_2</th>\n",
       "      <th>agent_2_feat_xga_mean_3</th>\n",
       "      <th>agent_1_feat_form_mean_3</th>\n",
       "      <th>agent_1_feat_scored_mean_3.1</th>\n",
       "      <th>agent_1_feat_xg_mean_3</th>\n",
       "      <th>agent_1_feat_XgAv</th>\n",
       "      <th>agent_2_feat_MissedAv</th>\n",
       "      <th>agent_2_feat_XgaAv</th>\n",
       "      <th>agent_1_feat_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.016316</td>\n",
       "      <td>76.909091</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>6.67</td>\n",
       "      <td>0.813158</td>\n",
       "      <td>1.538212</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.379390</td>\n",
       "      <td>1.736522</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.895377</td>\n",
       "      <td>1.806842</td>\n",
       "      <td>1.368421</td>\n",
       "      <td>1.373421</td>\n",
       "      <td>6.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.710526</td>\n",
       "      <td>1.080526</td>\n",
       "      <td>75.181818</td>\n",
       "      <td>1.210526</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1.050263</td>\n",
       "      <td>0.756073</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.177084</td>\n",
       "      <td>2.069040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.248041</td>\n",
       "      <td>1.416316</td>\n",
       "      <td>1.394737</td>\n",
       "      <td>1.516842</td>\n",
       "      <td>6.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.789474</td>\n",
       "      <td>1.547368</td>\n",
       "      <td>77.727273</td>\n",
       "      <td>1.263158</td>\n",
       "      <td>6.80</td>\n",
       "      <td>1.320000</td>\n",
       "      <td>1.322390</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.371045</td>\n",
       "      <td>0.419486</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421098</td>\n",
       "      <td>1.295789</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>1.238684</td>\n",
       "      <td>6.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.872632</td>\n",
       "      <td>74.909091</td>\n",
       "      <td>1.157895</td>\n",
       "      <td>6.46</td>\n",
       "      <td>1.103158</td>\n",
       "      <td>1.104814</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.189290</td>\n",
       "      <td>0.976729</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.371087</td>\n",
       "      <td>1.662368</td>\n",
       "      <td>1.657895</td>\n",
       "      <td>1.739737</td>\n",
       "      <td>6.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.789474</td>\n",
       "      <td>1.900263</td>\n",
       "      <td>86.181818</td>\n",
       "      <td>1.184211</td>\n",
       "      <td>6.82</td>\n",
       "      <td>1.382895</td>\n",
       "      <td>1.407720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.143720</td>\n",
       "      <td>0.923439</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.318723</td>\n",
       "      <td>1.491579</td>\n",
       "      <td>1.105263</td>\n",
       "      <td>1.244737</td>\n",
       "      <td>6.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>2.750000</td>\n",
       "      <td>1.905000</td>\n",
       "      <td>81.090909</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>6.71</td>\n",
       "      <td>1.238000</td>\n",
       "      <td>1.130094</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.390090</td>\n",
       "      <td>1.992155</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.827937</td>\n",
       "      <td>1.228000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.892500</td>\n",
       "      <td>6.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.162000</td>\n",
       "      <td>76.181818</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>6.63</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>0.586779</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.272970</td>\n",
       "      <td>1.876742</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.628449</td>\n",
       "      <td>0.842500</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.558000</td>\n",
       "      <td>6.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>1.644000</td>\n",
       "      <td>78.727273</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.69</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>1.031708</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.754170</td>\n",
       "      <td>2.248713</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>1.331350</td>\n",
       "      <td>2.736000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.170000</td>\n",
       "      <td>7.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>75.363636</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>6.68</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>1.081035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.393040</td>\n",
       "      <td>1.591810</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.198800</td>\n",
       "      <td>2.598000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>1.018000</td>\n",
       "      <td>7.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>81.363636</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>6.88</td>\n",
       "      <td>2.122000</td>\n",
       "      <td>2.267448</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.219660</td>\n",
       "      <td>0.688478</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.176910</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.777500</td>\n",
       "      <td>6.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>570 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     agent_2_feat_ScoredAv  agent_2_feat_XgAv  agent_2_feat_pl_mean  \\\n",
       "0                 0.947368           1.016316             76.909091   \n",
       "1                 0.710526           1.080526             75.181818   \n",
       "2                 1.789474           1.547368             77.727273   \n",
       "3                 0.526316           0.872632             74.909091   \n",
       "4                 1.789474           1.900263             86.181818   \n",
       "..                     ...                ...                   ...   \n",
       "565               2.750000           1.905000             81.090909   \n",
       "566               0.800000           1.162000             76.181818   \n",
       "567               0.400000           1.644000             78.727273   \n",
       "568               1.600000           0.940000             75.363636   \n",
       "569               0.750000           0.947500             81.363636   \n",
       "\n",
       "     agent_1_feat_MissedAv  agent_2_feat_Rating  agent_1_feat_XgaAv  \\\n",
       "0                 0.947368                 6.67            0.813158   \n",
       "1                 1.210526                 6.63            1.050263   \n",
       "2                 1.263158                 6.80            1.320000   \n",
       "3                 1.157895                 6.46            1.103158   \n",
       "4                 1.184211                 6.82            1.382895   \n",
       "..                     ...                  ...                 ...   \n",
       "565               1.600000                 6.71            1.238000   \n",
       "566               1.500000                 6.63            1.950000   \n",
       "567               0.200000                 6.69            0.778000   \n",
       "568               0.200000                 6.68            0.498000   \n",
       "569               2.800000                 6.88            2.122000   \n",
       "\n",
       "     agent_2_feat_xg_mean_3  agent_2_feat_scored_mean_3  \\\n",
       "0                  1.538212                    2.000000   \n",
       "1                  0.756073                    0.666667   \n",
       "2                  1.322390                    2.000000   \n",
       "3                  1.104814                    0.666667   \n",
       "4                  1.407720                    0.000000   \n",
       "..                      ...                         ...   \n",
       "565                1.130094                    1.333333   \n",
       "566                0.586779                    0.666667   \n",
       "567                1.031708                    1.333333   \n",
       "568                1.081035                    1.000000   \n",
       "569                2.267448                    1.666667   \n",
       "\n",
       "     agent_2_feat_form_mean_3  agent_2_feat_scored_3  ...  \\\n",
       "0                   -0.666667                    3.0  ...   \n",
       "1                   -0.666667                    1.0  ...   \n",
       "2                    1.000000                    2.0  ...   \n",
       "3                   -0.333333                    0.0  ...   \n",
       "4                   -0.666667                    0.0  ...   \n",
       "..                        ...                    ...  ...   \n",
       "565                  0.000000                    1.0  ...   \n",
       "566                 -1.000000                    1.0  ...   \n",
       "567                 -0.333333                    2.0  ...   \n",
       "568                 -0.333333                    1.0  ...   \n",
       "569                  0.666667                    1.0  ...   \n",
       "\n",
       "     agent_1_feat_scored_1.1  agent_1_feat_xg_2  agent_2_feat_xga_mean_3  \\\n",
       "0                        1.0           1.379390                 1.736522   \n",
       "1                        3.0           0.177084                 2.069040   \n",
       "2                        0.0           0.371045                 0.419486   \n",
       "3                        2.0           1.189290                 0.976729   \n",
       "4                        1.0           3.143720                 0.923439   \n",
       "..                       ...                ...                      ...   \n",
       "565                      1.0           1.390090                 1.992155   \n",
       "566                      3.0           2.272970                 1.876742   \n",
       "567                      1.0           1.754170                 2.248713   \n",
       "568                      5.0           3.393040                 1.591810   \n",
       "569                      0.0           1.219660                 0.688478   \n",
       "\n",
       "     agent_1_feat_form_mean_3  agent_1_feat_scored_mean_3.1  \\\n",
       "0                   -0.333333                      1.666667   \n",
       "1                    0.000000                      1.333333   \n",
       "2                    0.333333                      1.000000   \n",
       "3                    0.666667                      3.000000   \n",
       "4                    0.666667                      1.333333   \n",
       "..                        ...                           ...   \n",
       "565                  0.000000                      1.333333   \n",
       "566                  0.666667                      1.333333   \n",
       "567                  0.666667                      2.333333   \n",
       "568                  0.666667                      4.333333   \n",
       "569                 -0.666667                      0.666667   \n",
       "\n",
       "     agent_1_feat_xg_mean_3  agent_1_feat_XgAv  agent_2_feat_MissedAv  \\\n",
       "0                  0.895377           1.806842               1.368421   \n",
       "1                  1.248041           1.416316               1.394737   \n",
       "2                  0.421098           1.295789               1.315789   \n",
       "3                  1.371087           1.662368               1.657895   \n",
       "4                  2.318723           1.491579               1.105263   \n",
       "..                      ...                ...                    ...   \n",
       "565                1.827937           1.228000               0.750000   \n",
       "566                1.628449           0.842500               1.200000   \n",
       "567                1.331350           2.736000               1.000000   \n",
       "568                3.198800           2.598000               1.400000   \n",
       "569                1.176910           0.832000               0.750000   \n",
       "\n",
       "     agent_2_feat_XgaAv  agent_1_feat_Rating  \n",
       "0              1.373421                 6.83  \n",
       "1              1.516842                 6.65  \n",
       "2              1.238684                 6.73  \n",
       "3              1.739737                 6.85  \n",
       "4              1.244737                 6.81  \n",
       "..                  ...                  ...  \n",
       "565            0.892500                 6.72  \n",
       "566            1.558000                 6.69  \n",
       "567            1.170000                 7.05  \n",
       "568            1.018000                 7.11  \n",
       "569            1.777500                 6.42  \n",
       "\n",
       "[570 rows x 33 columns]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_df2 = test_df2[list(fearues_names2)]\n",
    "new_test_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train2, new_X_test2, new_test_df2 = scale_data(new_X_train2, new_X_test2, new_test_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1730, 9) (433, 9)\n"
     ]
    }
   ],
   "source": [
    "y_train2 = pd.get_dummies(y_train2)\n",
    "y_test2 = pd.get_dummies(y_test2)\n",
    "y_train2, y_test2 = make_equal_dummies(y_train2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_results2 = grid_search(new_X_train2, y_train2, 'expected_target2')\n",
    "batch_size2 = grid_results2.best_params_['batch_size']\n",
    "epochs2 = grid_results2.best_params_['epochs']\n",
    "learning_rate2 = grid_results2.best_params_['learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size2 = 32\n",
    "# epochs2 = 100\n",
    "# learning_rate2 = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "14/14 [==============================] - 2s 23ms/step - loss: 2.9266 - accuracy: 0.1116 - val_loss: 2.1918 - val_accuracy: 0.1524\n",
      "Epoch 2/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.7972 - accuracy: 0.1347 - val_loss: 2.1496 - val_accuracy: 0.2702\n",
      "Epoch 3/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.6324 - accuracy: 0.1474 - val_loss: 2.1099 - val_accuracy: 0.3118\n",
      "Epoch 4/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.5117 - accuracy: 0.1642 - val_loss: 2.0722 - val_accuracy: 0.3187\n",
      "Epoch 5/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.4728 - accuracy: 0.1855 - val_loss: 2.0368 - val_accuracy: 0.3441\n",
      "Epoch 6/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.3881 - accuracy: 0.1908 - val_loss: 2.0027 - val_accuracy: 0.3510\n",
      "Epoch 7/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.2280 - accuracy: 0.2439 - val_loss: 1.9728 - val_accuracy: 0.3326\n",
      "Epoch 8/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2.1960 - accuracy: 0.2306 - val_loss: 1.9439 - val_accuracy: 0.3372\n",
      "Epoch 9/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.1237 - accuracy: 0.2630 - val_loss: 1.9179 - val_accuracy: 0.3372\n",
      "Epoch 10/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 2.0944 - accuracy: 0.2757 - val_loss: 1.8947 - val_accuracy: 0.3441\n",
      "Epoch 11/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.0214 - accuracy: 0.2977 - val_loss: 1.8711 - val_accuracy: 0.3487\n",
      "Epoch 12/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 2.0230 - accuracy: 0.2931 - val_loss: 1.8501 - val_accuracy: 0.3418\n",
      "Epoch 13/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.9384 - accuracy: 0.3191 - val_loss: 1.8318 - val_accuracy: 0.3557\n",
      "Epoch 14/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1.9092 - accuracy: 0.3237 - val_loss: 1.8171 - val_accuracy: 0.3510\n",
      "Epoch 15/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1.9148 - accuracy: 0.3254 - val_loss: 1.8017 - val_accuracy: 0.3649\n",
      "Epoch 16/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.8640 - accuracy: 0.3185 - val_loss: 1.7855 - val_accuracy: 0.3603\n",
      "Epoch 17/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.8118 - accuracy: 0.3497 - val_loss: 1.7729 - val_accuracy: 0.3626\n",
      "Epoch 18/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.7773 - accuracy: 0.3486 - val_loss: 1.7621 - val_accuracy: 0.3580\n",
      "Epoch 19/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.7727 - accuracy: 0.3676 - val_loss: 1.7505 - val_accuracy: 0.3510\n",
      "Epoch 20/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.7328 - accuracy: 0.3734 - val_loss: 1.7409 - val_accuracy: 0.3533\n",
      "Epoch 21/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.7196 - accuracy: 0.3688 - val_loss: 1.7276 - val_accuracy: 0.3580\n",
      "Epoch 22/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.7044 - accuracy: 0.3896 - val_loss: 1.7179 - val_accuracy: 0.3557\n",
      "Epoch 23/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.6998 - accuracy: 0.3803 - val_loss: 1.7071 - val_accuracy: 0.3626\n",
      "Epoch 24/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1.6590 - accuracy: 0.3988 - val_loss: 1.6994 - val_accuracy: 0.3510\n",
      "Epoch 25/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.6402 - accuracy: 0.3879 - val_loss: 1.6939 - val_accuracy: 0.3510\n",
      "Epoch 26/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.6188 - accuracy: 0.4133 - val_loss: 1.6877 - val_accuracy: 0.3441\n",
      "Epoch 27/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1.6101 - accuracy: 0.4104 - val_loss: 1.6792 - val_accuracy: 0.3441\n",
      "Epoch 28/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.6238 - accuracy: 0.3960 - val_loss: 1.6723 - val_accuracy: 0.3487\n",
      "Epoch 29/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.5780 - accuracy: 0.4214 - val_loss: 1.6667 - val_accuracy: 0.3464\n",
      "Epoch 30/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.5666 - accuracy: 0.4150 - val_loss: 1.6615 - val_accuracy: 0.3441\n",
      "Epoch 31/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1.5701 - accuracy: 0.4162 - val_loss: 1.6532 - val_accuracy: 0.3418\n",
      "Epoch 32/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.5239 - accuracy: 0.4220 - val_loss: 1.6451 - val_accuracy: 0.3441\n",
      "Epoch 33/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.5544 - accuracy: 0.4040 - val_loss: 1.6381 - val_accuracy: 0.3487\n",
      "Epoch 34/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.5313 - accuracy: 0.4185 - val_loss: 1.6325 - val_accuracy: 0.3441\n",
      "Epoch 35/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4833 - accuracy: 0.4480 - val_loss: 1.6284 - val_accuracy: 0.3464\n",
      "Epoch 36/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1.4995 - accuracy: 0.4370 - val_loss: 1.6264 - val_accuracy: 0.3372\n",
      "Epoch 37/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4938 - accuracy: 0.4179 - val_loss: 1.6232 - val_accuracy: 0.3372\n",
      "Epoch 38/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4793 - accuracy: 0.4353 - val_loss: 1.6196 - val_accuracy: 0.3418\n",
      "Epoch 39/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4810 - accuracy: 0.4289 - val_loss: 1.6146 - val_accuracy: 0.3372\n",
      "Epoch 40/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4460 - accuracy: 0.4636 - val_loss: 1.6116 - val_accuracy: 0.3349\n",
      "Epoch 41/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4539 - accuracy: 0.4410 - val_loss: 1.6088 - val_accuracy: 0.3326\n",
      "Epoch 42/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4571 - accuracy: 0.4376 - val_loss: 1.6036 - val_accuracy: 0.3326\n",
      "Epoch 43/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4436 - accuracy: 0.4555 - val_loss: 1.5970 - val_accuracy: 0.3418\n",
      "Epoch 44/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4260 - accuracy: 0.4272 - val_loss: 1.5911 - val_accuracy: 0.3441\n",
      "Epoch 45/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4087 - accuracy: 0.4457 - val_loss: 1.5864 - val_accuracy: 0.3441\n",
      "Epoch 46/75\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 1.4052 - accuracy: 0.4526 - val_loss: 1.5819 - val_accuracy: 0.3395\n",
      "Epoch 47/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4163 - accuracy: 0.4526 - val_loss: 1.5772 - val_accuracy: 0.3372\n",
      "Epoch 48/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3878 - accuracy: 0.4815 - val_loss: 1.5722 - val_accuracy: 0.3395\n",
      "Epoch 49/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4110 - accuracy: 0.4590 - val_loss: 1.5675 - val_accuracy: 0.3487\n",
      "Epoch 50/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3733 - accuracy: 0.4462 - val_loss: 1.5646 - val_accuracy: 0.3464\n",
      "Epoch 51/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4025 - accuracy: 0.4468 - val_loss: 1.5605 - val_accuracy: 0.3487\n",
      "Epoch 52/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.4032 - accuracy: 0.4474 - val_loss: 1.5562 - val_accuracy: 0.3464\n",
      "Epoch 53/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3887 - accuracy: 0.4590 - val_loss: 1.5549 - val_accuracy: 0.3464\n",
      "Epoch 54/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3608 - accuracy: 0.4566 - val_loss: 1.5543 - val_accuracy: 0.3418\n",
      "Epoch 55/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3712 - accuracy: 0.4682 - val_loss: 1.5522 - val_accuracy: 0.3487\n",
      "Epoch 56/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3599 - accuracy: 0.4653 - val_loss: 1.5515 - val_accuracy: 0.3464\n",
      "Epoch 57/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3436 - accuracy: 0.4636 - val_loss: 1.5481 - val_accuracy: 0.3418\n",
      "Epoch 58/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3482 - accuracy: 0.4613 - val_loss: 1.5476 - val_accuracy: 0.3464\n",
      "Epoch 59/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3473 - accuracy: 0.4607 - val_loss: 1.5453 - val_accuracy: 0.3510\n",
      "Epoch 60/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3453 - accuracy: 0.4671 - val_loss: 1.5432 - val_accuracy: 0.3603\n",
      "Epoch 61/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3213 - accuracy: 0.4902 - val_loss: 1.5428 - val_accuracy: 0.3603\n",
      "Epoch 62/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3216 - accuracy: 0.4671 - val_loss: 1.5424 - val_accuracy: 0.3418\n",
      "Epoch 63/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3158 - accuracy: 0.4613 - val_loss: 1.5435 - val_accuracy: 0.3510\n",
      "Epoch 64/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3379 - accuracy: 0.4584 - val_loss: 1.5462 - val_accuracy: 0.3441\n",
      "Epoch 65/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3429 - accuracy: 0.4595 - val_loss: 1.5483 - val_accuracy: 0.3395\n",
      "Epoch 66/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3094 - accuracy: 0.4792 - val_loss: 1.5455 - val_accuracy: 0.3395\n",
      "Epoch 67/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.2920 - accuracy: 0.4832 - val_loss: 1.5441 - val_accuracy: 0.3441\n",
      "Epoch 68/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3092 - accuracy: 0.4879 - val_loss: 1.5442 - val_accuracy: 0.3418\n",
      "Epoch 69/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3048 - accuracy: 0.4913 - val_loss: 1.5443 - val_accuracy: 0.3464\n",
      "Epoch 70/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3095 - accuracy: 0.4624 - val_loss: 1.5426 - val_accuracy: 0.3487\n",
      "Epoch 71/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.3137 - accuracy: 0.4855 - val_loss: 1.5401 - val_accuracy: 0.3580\n",
      "Epoch 72/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.2733 - accuracy: 0.4855 - val_loss: 1.5400 - val_accuracy: 0.3487\n",
      "Epoch 73/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.2916 - accuracy: 0.4815 - val_loss: 1.5373 - val_accuracy: 0.3557\n",
      "Epoch 74/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.2822 - accuracy: 0.4671 - val_loss: 1.5350 - val_accuracy: 0.3510\n",
      "Epoch 75/75\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 1.2921 - accuracy: 0.4792 - val_loss: 1.5323 - val_accuracy: 0.3557\n"
     ]
    }
   ],
   "source": [
    "num_classes = train_df['expected_target2'].nunique()\n",
    "model2 = create_model(batch_size2, epochs2, learning_rate2, num_classes, new_X_train2.shape[1])\n",
    "history2 = model2.fit(new_X_train2, y_train2, \n",
    "                      batch_size = batch_size2, \n",
    "                      epochs = epochs2,\n",
    "                      validation_data = (new_X_test2, y_test2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_graphs(history2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step\n",
      "ACC:  0.672\n"
     ]
    }
   ],
   "source": [
    "tmp_pred_value2 = calc_train_acc(model2, 'expected_target2', Y_expected2, new_X_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = np.logical_and(tmp_pred_value1, tmp_pred_value2)\n",
    "test_pred = [0 if x==False else x for x in test_pred]\n",
    "test_pred = [1 if x==True else x for x in test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1979    0\n",
       "1980    0\n",
       "1981    1\n",
       "1982    0\n",
       "1983    0\n",
       "       ..\n",
       "2465    1\n",
       "2466    1\n",
       "2467    0\n",
       "2468    1\n",
       "2469    1\n",
       "Name: category, Length: 433, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_real_value = train_df['category'][int(len(Y_expected1)*0.8):len(Y_expected1)]\n",
    "test_real_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:  0.524\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for elem1, elem2 in zip(test_pred, test_real_value):\n",
    "    if elem1 == elem2:\n",
    "        acc += 1\n",
    "print(\"ACC: \", np.round((acc / len(test_real_value)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKm-keClFa1Q"
   },
   "source": [
    "# Make a submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "fin_pred1 = model1.predict(new_test_df1)\n",
    "tmp1 = []\n",
    "for i in fin_pred1:\n",
    "    if i.argmax() != 0:\n",
    "        tmp1.append(1)\n",
    "    else:\n",
    "        tmp1.append(0)\n",
    "\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "fin_pred2 = model2.predict(new_test_df2)\n",
    "tmp2 = []\n",
    "for i in fin_pred2:\n",
    "    if i.argmax() != 0:\n",
    "        tmp2.append(1)\n",
    "    else:\n",
    "        tmp2.append(0)\n",
    "\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Answer = np.logical_and(tmp1, tmp2)\n",
    "Answer = [0 if x==False else x for x in Answer]\n",
    "Answer = [1 if x==True else x for x in Answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "_b27gGlzPE2w"
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample_submission['tmp'] = Answer\n",
    "sample_submission.drop(['category'], axis = 1, inplace= True)\n",
    "sample_submission = sample_submission.rename(columns={\"tmp\": \"category\"})\n",
    "sample_submission.to_csv('Answer.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>565</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>567</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>568</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>570 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  category\n",
       "0      0         0\n",
       "1      1         0\n",
       "2      2         0\n",
       "3      3         0\n",
       "4      4         0\n",
       "..   ...       ...\n",
       "565  565         0\n",
       "566  566         0\n",
       "567  567         0\n",
       "568  568         0\n",
       "569  569         0\n",
       "\n",
       "[570 rows x 2 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check 33% acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pd.read_csv('released_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5053191489361702\n"
     ]
    }
   ],
   "source": [
    "num = 0\n",
    "for i, j in zip(sample_submission.iloc[0:len(answers)]['category'], answers['category']):\n",
    "    if i == j:\n",
    "        num +=1\n",
    "print(num / len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
